{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96eec4c9",
   "metadata": {},
   "source": [
    "# Chapter 2: Your first Pyspark application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144d74b8",
   "metadata": {},
   "source": [
    "Most data-driven application functions in the Extract-Transform-Load (ETL) pipeline:\n",
    "\n",
    "1. Ingest or read the data we wish to work with.\n",
    "2. Transform the data via a few simple instructions or a very complex machine learning model\n",
    "3. Export the resulting data, either into a file to be fed into an app or by summarizing our findings into a visualization.\n",
    "\n",
    "### `SparkSession` entry point\n",
    "\n",
    "- `SparkSession` provides an entry point to Spark.\n",
    "  - Wraps `SparkContext` and provides functionality for interacting with the data.\n",
    "- Can be used as a normal object imported from a library in Python.\n",
    "- `SparkSession` builder: builder pattern with set of methods to create a configurable object.\n",
    "\n",
    "#### Creating a `SparkSession` entry point from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24cdb47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Analyzing the vocabulary of Pride and Prejudice.\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459b0cac",
   "metadata": {},
   "source": [
    "`sparkContext` can be invoked from the `SparkSession` object like below.  \n",
    "\n",
    "(Older code may present `sparkContext` as an `sc` variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36a80ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://taichis-imac.lan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Analyzing the vocabulary of Pride and Prejudice.</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=Analyzing the vocabulary of Pride and Prejudice.>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547cfb4e",
   "metadata": {},
   "source": [
    "### Setting the log level\n",
    "\n",
    "- Spark defaults to `WARN`.\n",
    "- Can change via `spark.sparkContext.setLogLevel(KEYWORD)`\n",
    "\n",
    "#### Log level keywords\n",
    "\n",
    "<table border=\"1\" class=\"contenttable\" summary=\"log level keywords\" width=\"100%\"> \n",
    "  <colgroup class=\"calibre26\" span=\"1\"> \n",
    "   <col class=\"col_\" span=\"1\" width=\"50%\"> \n",
    "   <col class=\"col_\" span=\"1\" width=\"50%\"> \n",
    "  </colgroup> \n",
    "  <tbody>\n",
    "   <tr class=\"calibre19\"> \n",
    "       <td class=\"contenttable1\" colspan=\"1\" rowspan=\"1\" align=\"left\"><p>Keyword</p></td> \n",
    "       <td class=\"contenttable1\" colspan=\"1\"  align=\"left\" rowspan=\"1\"><p>Description</p></td> \n",
    "   </tr> \n",
    "   <tr class=\"calibre19\"> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p><code class=\"code\">OFF</code></p> </td> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p>No logging at all (not recommended).</p> </td> \n",
    "   </tr> \n",
    "   <tr class=\"calibre19\"> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p><code class=\"code\">FATAL</code></p> </td> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p>Only fatal errors. A fatal error will crash your Spark cluster.</p> </td> \n",
    "   </tr> \n",
    "   <tr class=\"calibre19\"> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p><code class=\"code\">ERROR</code></p> </td> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p>My personal favorite, will show <code class=\"code\">FATAL</code> as well as other useful (but recoverable) errors.</p> </td> \n",
    "   </tr> \n",
    "   <tr class=\"calibre19\"> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p><code class=\"code\">WARN</code></p> </td> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p>Add warnings (and there is quite a lot of them).</p> </td> \n",
    "   </tr> \n",
    "   <tr class=\"calibre19\"> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p><code class=\"code\">INFO</code></p> </td> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p>Will give you runtime information, such as repartitioning and data recovery (see chapter 1).</p> </td> \n",
    "   </tr> \n",
    "   <tr class=\"calibre19\"> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p><code class=\"code\">DEBUG</code></p> </td> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p>Will provide debug information on your jobs.</p> </td> \n",
    "   </tr> \n",
    "   <tr class=\"calibre19\"> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p><code class=\"code\">TRACE</code></p> </td> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p>Will trace your jobs (more verbose debug logs). Can be quite pedagogic, but very annoying.</p> </td> \n",
    "   </tr> \n",
    "   <tr class=\"calibre19\"> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p><code class=\"code\">ALL</code></p> </td> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p>Everything that PySpark can spit, it will spit. As useful as <code class=\"code\">OFF</code>.</p> </td> \n",
    "   </tr> \n",
    "  </tbody>\n",
    " </table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfde62b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28789022",
   "metadata": {},
   "source": [
    "## Application Design\n",
    "\n",
    "__Goal: What are the most popular words in the Jane Austen's _Pride and Prejudice_?__\n",
    "\n",
    "Steps:\n",
    "1. Read: Read the input data (we’re assuming a plain text file)\n",
    "2. Tokenize: Tokenize each word\n",
    "3. Clean: Remove any punctuation and/or tokens that aren’t words.\n",
    "4. Count: Count the frequency of each word present in the text\n",
    "5. Answer: Return the top 10 (or 20, 50, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5e1b33",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "PySpark provide two main structures for storing data when performing manipulations:\n",
    "\n",
    "1. The Resilient Distributed Dataset (or RDD)\n",
    "2. The data frame; Stricter version of RDD. Makes heavy use of the concept of _columns_ where you perform ops on columns instead of on records (like in RDD).  \n",
    "  - More common than RDD.\n",
    "  - Syntax is similar to SQL\n",
    "\n",
    "#### RDD vs Dataframe\n",
    "<img src=\"notes/img/rdd_df.png\">\n",
    "\n",
    "#### Reading a dataframe with `spark.read`\n",
    "Reading data into a data frame is done through the DataFrameReader object, which we can access through `spark.read`. \n",
    "\n",
    "`value: string` is the column, with text within that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a08d0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book = spark.read.text(\"data/Ch02/1342-0.txt\")\n",
    "book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b5e651b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('value', 'string')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check schema\n",
    "display(book.printSchema())\n",
    "\n",
    "display(book.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c38f826",
   "metadata": {},
   "source": [
    "#### Showing a dataframe with `spark.show()`\n",
    "\n",
    "The show() method takes three optional parameters.\n",
    "\n",
    "1. `n` can be set to any positive integer, and will display that number of rows.\n",
    "2. `truncate`, if set to true, will truncate the columns to display only 20 characters. Set to False to display the whole length, or any positive integer to truncate to a specific number of characters.\n",
    "3. `vertical` takes a Boolean value and, when set to True, will display each record as a small table. If you need to check some records in detail, this is a very useful option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44c6ec09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------------------------------\n",
      " value | The Project Gutenberg EBook of Pride and Prejudice, by Jane Austen \n",
      "-RECORD 1-------------------------------------------------------------------\n",
      " value |                                                                    \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# play with params\n",
    "book.show(2, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af55a12",
   "metadata": {},
   "source": [
    "#### Lazy vs Eager Evaluation\n",
    "\n",
    "- Default, you need to pass `show()` to see dataframe content.  This follow's Spark's idea of lazy evaluation until some action is needed.\n",
    "- Since Spark 2.4.0, you can configure the SparkSession object to support printing to screen. This may be helpful when learning:\n",
    "\n",
    "```py\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "                     .config(\"spark.sql.repl.eagerEval.enabled\", \"True\")\n",
    "                     .getOrCreate())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a92883",
   "metadata": {},
   "source": [
    "### Tokenizing sentences with `select()` and `split()`\n",
    "\n",
    "`select()` selects the data. Similar to SQL. Syntax is similar to pandas:\n",
    "\n",
    "```py\n",
    "book.select(book.value)\n",
    "book.select(book[\"value\"])\n",
    "book.select(col(\"value\"))\n",
    "book.select(\"value\")\n",
    "```\n",
    "\n",
    "`split()` transforms string column into an array column, containing `n` string elements (i.e. tokens).  Note that it uses `JVM`-based regex instead of Python.\n",
    "\n",
    "`alias()` renames transformed columns for easier reference.  When applied to a column, it takes a single string as an argument.\n",
    "\n",
    "Another way to alias set an alias is calling `.withColumnRenamed()` on the data frame.  If you just want to rename a column without changing the rest of the data frame, use .withColumnRenamed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6024b8f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[line: array<string>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- line: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+--------------------+\n",
      "|                line|\n",
      "+--------------------+\n",
      "|[The, Project, Gu...|\n",
      "|                  []|\n",
      "|[This, eBook, is,...|\n",
      "|[almost, no, rest...|\n",
      "|[re-use, it, unde...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, split\n",
    "\n",
    "# Read, tokenize and alias the column\n",
    "lines = book.select(split(col('value'), \" \").alias(\"line\"))\n",
    "\n",
    "display(lines)\n",
    "\n",
    "lines.printSchema()\n",
    "\n",
    "lines.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "80e3f8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- here is an alternate alias: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Changing alias name using withColumnRenamed\n",
    "alternative = lines.withColumnRenamed(\"line\", \n",
    "                                      \"here is an alternate alias\")\n",
    "alternative.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc584ff9",
   "metadata": {},
   "source": [
    "### Reshaping data with `explode()`\n",
    "\n",
    "When applied to a column containing a container-like data structure (such as an array), `explode()` will take each element and give it its own row.\n",
    "\n",
    "![img](notes/img/explode.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "13709e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      word|\n",
      "+----------+\n",
      "|       The|\n",
      "|   Project|\n",
      "| Gutenberg|\n",
      "|     EBook|\n",
      "|        of|\n",
      "|     Pride|\n",
      "|       and|\n",
      "|Prejudice,|\n",
      "|        by|\n",
      "|      Jane|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explode column of arrays into rows of elements\n",
    "\n",
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    "words.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c6964a",
   "metadata": {},
   "source": [
    "### String normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "71474300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|word_lower|\n",
      "+----------+\n",
      "|       the|\n",
      "|   project|\n",
      "| gutenberg|\n",
      "|     ebook|\n",
      "|        of|\n",
      "|     pride|\n",
      "|       and|\n",
      "|prejudice,|\n",
      "|        by|\n",
      "|      jane|\n",
      "|    austen|\n",
      "|          |\n",
      "|      this|\n",
      "|     ebook|\n",
      "|        is|\n",
      "|       for|\n",
      "|       the|\n",
      "|       use|\n",
      "|        of|\n",
      "|    anyone|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------------+\n",
      "|word_normalized|\n",
      "+---------------+\n",
      "|            the|\n",
      "|        project|\n",
      "|      gutenberg|\n",
      "|          ebook|\n",
      "|             of|\n",
      "|          pride|\n",
      "|            and|\n",
      "|      prejudice|\n",
      "|             by|\n",
      "|           jane|\n",
      "|         austen|\n",
      "|               |\n",
      "|           this|\n",
      "|          ebook|\n",
      "|             is|\n",
      "|            for|\n",
      "|            the|\n",
      "|            use|\n",
      "|             of|\n",
      "|         anyone|\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower, regexp_extract\n",
    "\n",
    "# Lowercase\n",
    "words_lower = words.select(lower(\"word\").alias(\"word_lower\"))\n",
    "words_lower.show()\n",
    "\n",
    "# Naive punctuation normalization using regex\n",
    "word_norm = words_lower.select(regexp_extract(col(\"word_lower\"), \"[a-z]*\", 0).alias(\"word_normalized\"))\n",
    "word_norm.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ecc248",
   "metadata": {},
   "source": [
    "### Filtering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f0794bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|word_nonull|\n",
      "+-----------+\n",
      "|        the|\n",
      "|    project|\n",
      "|  gutenberg|\n",
      "|      ebook|\n",
      "|         of|\n",
      "|      pride|\n",
      "|        and|\n",
      "|  prejudice|\n",
      "|         by|\n",
      "|       jane|\n",
      "|     austen|\n",
      "|       this|\n",
      "|      ebook|\n",
      "|         is|\n",
      "|        for|\n",
      "|        the|\n",
      "|        use|\n",
      "|         of|\n",
      "|     anyone|\n",
      "|   anywhere|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove empty records\n",
    "\n",
    "word_nonull = word_norm.filter(col(\"word_normalized\") != \"\") \\\n",
    "                       .withColumnRenamed('word_normalized', 'word_nonull')\n",
    "word_nonull.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325a37b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
