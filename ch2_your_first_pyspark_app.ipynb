{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "753c0e28",
   "metadata": {},
   "source": [
    "# Chapter 2: Your first Pyspark application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89be918",
   "metadata": {
    "tags": []
   },
   "source": [
    "Most data-driven application functions in the Extract-Transform-Load (ETL) pipeline:\n",
    "\n",
    "1. Ingest or read the data we wish to work with.\n",
    "2. Transform the data via a few simple instructions or a very complex machine learning model\n",
    "3. Export the resulting data, either into a file to be fed into an app or by summarizing our findings into a visualization.\n",
    "\n",
    "### `SparkSession` entry point\n",
    "\n",
    "- `SparkSession` provides an entry point to Spark.\n",
    "  - Wraps `SparkContext` and provides functionality for interacting with the data.\n",
    "- Can be used as a normal object imported from a library in Python.\n",
    "- `SparkSession` builder: builder pattern with set of methods to create a configurable object.\n",
    "\n",
    "#### Creating a `SparkSession` entry point from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f072c846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Analyzing the vocabulary of Pride and Prejudice.\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ea2a2e",
   "metadata": {},
   "source": [
    "`sparkContext` can be invoked from the `SparkSession` object like below.  \n",
    "\n",
    "(Older code may present `sparkContext` as an `sc` variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43506485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://taichis-imac.lan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Analyzing the vocabulary of Pride and Prejudice.</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=Analyzing the vocabulary of Pride and Prejudice.>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ef28a2",
   "metadata": {},
   "source": [
    "### Setting the log level\n",
    "\n",
    "- Spark defaults to `WARN`.\n",
    "- Can change via `spark.sparkContext.setLogLevel(KEYWORD)`\n",
    "\n",
    "#### Log level keywords\n",
    "\n",
    "<table border=\"1\" class=\"contenttable\" summary=\"log level keywords\" width=\"100%\"> \n",
    "  <colgroup class=\"calibre26\" span=\"1\"> \n",
    "   <col class=\"col_\" span=\"1\" width=\"50%\"> \n",
    "   <col class=\"col_\" span=\"1\" width=\"50%\"> \n",
    "  </colgroup> \n",
    "  <tbody>\n",
    "   <tr class=\"calibre19\"> \n",
    "       <td class=\"contenttable1\" colspan=\"1\" rowspan=\"1\" align=\"left\"><p>Keyword</p></td> \n",
    "       <td class=\"contenttable1\" colspan=\"1\"  align=\"left\" rowspan=\"1\"><p>Description</p></td> \n",
    "   </tr> \n",
    "   <tr class=\"calibre19\"> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p><code class=\"code\">OFF</code></p> </td> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p>No logging at all (not recommended).</p> </td> \n",
    "   </tr> \n",
    "   <tr class=\"calibre19\"> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p><code class=\"code\">FATAL</code></p> </td> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p>Only fatal errors. A fatal error will crash your Spark cluster.</p> </td> \n",
    "   </tr> \n",
    "   <tr class=\"calibre19\"> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p><code class=\"code\">ERROR</code></p> </td> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p>My personal favorite, will show <code class=\"code\">FATAL</code> as well as other useful (but recoverable) errors.</p> </td> \n",
    "   </tr> \n",
    "   <tr class=\"calibre19\"> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p><code class=\"code\">WARN</code></p> </td> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p>Add warnings (and there is quite a lot of them).</p> </td> \n",
    "   </tr> \n",
    "   <tr class=\"calibre19\"> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p><code class=\"code\">INFO</code></p> </td> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p>Will give you runtime information, such as repartitioning and data recovery (see chapter 1).</p> </td> \n",
    "   </tr> \n",
    "   <tr class=\"calibre19\"> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p><code class=\"code\">DEBUG</code></p> </td> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p>Will provide debug information on your jobs.</p> </td> \n",
    "   </tr> \n",
    "   <tr class=\"calibre19\"> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p><code class=\"code\">TRACE</code></p> </td> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p>Will trace your jobs (more verbose debug logs). Can be quite pedagogic, but very annoying.</p> </td> \n",
    "   </tr> \n",
    "   <tr class=\"calibre19\"> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p><code class=\"code\">ALL</code></p> </td> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p>Everything that PySpark can spit, it will spit. As useful as <code class=\"code\">OFF</code>.</p> </td> \n",
    "   </tr> \n",
    "  </tbody>\n",
    " </table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3155d7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382b2a52",
   "metadata": {},
   "source": [
    "## Application Design\n",
    "\n",
    "__Goal: What are the most popular words in the Jane Austen's _Pride and Prejudice_?__\n",
    "\n",
    "Steps:\n",
    "1. Read: Read the input data (we’re assuming a plain text file)\n",
    "2. Tokenize: Tokenize each word\n",
    "3. Clean: Remove any punctuation and/or tokens that aren’t words.\n",
    "4. Count: Count the frequency of each word present in the text\n",
    "5. Answer: Return the top 10 (or 20, 50, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f3dba5",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "PySpark provide two main structures for storing data when performing manipulations:\n",
    "\n",
    "1. The Resilient Distributed Dataset (or RDD)\n",
    "2. The data frame; Stricter version of RDD. Makes heavy use of the concept of _columns_ where you perform ops on columns instead of on records (like in RDD).  \n",
    "  - More common than RDD.\n",
    "  - Syntax is similar to SQL\n",
    "\n",
    "#### RDD vs Dataframe\n",
    "<img src=\"notes/img/rdd_df.png\">\n",
    "\n",
    "#### Reading a dataframe with `spark.read`\n",
    "Reading data into a data frame is done through the DataFrameReader object, which we can access through `spark.read`. \n",
    "\n",
    "`value: string` is the column, with text within that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfdb7487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book = spark.read.text(\"data/Ch02/1342-0.txt\")\n",
    "book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39186739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('value', 'string')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check schema\n",
    "display(book.printSchema())\n",
    "\n",
    "display(book.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c3d5b0",
   "metadata": {},
   "source": [
    "#### Showing a dataframe with `spark.show()`\n",
    "\n",
    "The show() method takes three optional parameters.\n",
    "\n",
    "1. `n` can be set to any positive integer, and will display that number of rows.\n",
    "2. `truncate`, if set to true, will truncate the columns to display only 20 characters. Set to False to display the whole length, or any positive integer to truncate to a specific number of characters.\n",
    "3. `vertical` takes a Boolean value and, when set to True, will display each record as a small table. If you need to check some records in detail, this is a very useful option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46ee4223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------------------------------\n",
      " value | The Project Gutenberg EBook of Pride and Prejudice, by Jane Austen \n",
      "-RECORD 1-------------------------------------------------------------------\n",
      " value |                                                                    \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# play with params\n",
    "book.show(2, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4b2a41",
   "metadata": {},
   "source": [
    "#### Lazy vs Eager Evaluation\n",
    "\n",
    "- Default, you need to pass `show()` to see dataframe content.  This follow's Spark's idea of lazy evaluation until some action is needed.\n",
    "- Since Spark 2.4.0, you can configure the SparkSession object to support printing to screen. This may be helpful when learning:\n",
    "\n",
    "```py\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "                     .config(\"spark.sql.repl.eagerEval.enabled\", \"True\")\n",
    "                     .getOrCreate())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdde1d2",
   "metadata": {},
   "source": [
    "### Tokenizing sentences with `select()` and `split()`\n",
    "\n",
    "`select()` selects the data. Similar to SQL. Syntax is similar to pandas:\n",
    "\n",
    "```py\n",
    "book.select(book.value)\n",
    "book.select(book[\"value\"])\n",
    "book.select(col(\"value\"))\n",
    "book.select(\"value\")\n",
    "```\n",
    "\n",
    "`split()` transforms string column into an array column, containing `n` string elements (i.e. tokens).  Note that it uses `JVM`-based regex instead of Python.\n",
    "\n",
    "`alias()` renames transformed columns for easier reference.  When applied to a column, it takes a single string as an argument.\n",
    "\n",
    "Another way to alias set an alias is calling `.withColumnRenamed()` on the data frame.  If you just want to rename a column without changing the rest of the data frame, use .withColumnRenamed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bebc893d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[line: array<string>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- line: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+--------------------+\n",
      "|                line|\n",
      "+--------------------+\n",
      "|[The, Project, Gu...|\n",
      "|                  []|\n",
      "|[This, eBook, is,...|\n",
      "|[almost, no, rest...|\n",
      "|[re-use, it, unde...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, split\n",
    "\n",
    "# Read, tokenize and alias the column\n",
    "lines = book.select(split(col('value'), \" \").alias(\"line\"))\n",
    "\n",
    "display(lines)\n",
    "\n",
    "lines.printSchema()\n",
    "\n",
    "lines.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "16ed69b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- here is an alternate alias: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Changing alias name using withColumnRenamed\n",
    "alternative = lines.withColumnRenamed(\"line\", \n",
    "                                      \"here is an alternate alias\")\n",
    "alternative.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d675d0c",
   "metadata": {},
   "source": [
    "### Reshaping data with `explode()`\n",
    "\n",
    "When applied to a column containing a container-like data structure (such as an array), `explode()` will take each element and give it its own row.\n",
    "\n",
    "![img](notes/img/explode.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b34cbb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      word|\n",
      "+----------+\n",
      "|       The|\n",
      "|   Project|\n",
      "| Gutenberg|\n",
      "|     EBook|\n",
      "|        of|\n",
      "|     Pride|\n",
      "|       and|\n",
      "|Prejudice,|\n",
      "|        by|\n",
      "|      Jane|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explode column of arrays into rows of elements\n",
    "\n",
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    "words.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459b67f2",
   "metadata": {},
   "source": [
    "### String normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6c28d2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|word_lower|\n",
      "+----------+\n",
      "|       the|\n",
      "|   project|\n",
      "| gutenberg|\n",
      "|     ebook|\n",
      "|        of|\n",
      "|     pride|\n",
      "|       and|\n",
      "|prejudice,|\n",
      "|        by|\n",
      "|      jane|\n",
      "|    austen|\n",
      "|          |\n",
      "|      this|\n",
      "|     ebook|\n",
      "|        is|\n",
      "|       for|\n",
      "|       the|\n",
      "|       use|\n",
      "|        of|\n",
      "|    anyone|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------------+\n",
      "|word_normalized|\n",
      "+---------------+\n",
      "|            the|\n",
      "|        project|\n",
      "|      gutenberg|\n",
      "|          ebook|\n",
      "|             of|\n",
      "|          pride|\n",
      "|            and|\n",
      "|      prejudice|\n",
      "|             by|\n",
      "|           jane|\n",
      "|         austen|\n",
      "|               |\n",
      "|           this|\n",
      "|          ebook|\n",
      "|             is|\n",
      "|            for|\n",
      "|            the|\n",
      "|            use|\n",
      "|             of|\n",
      "|         anyone|\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower, regexp_extract\n",
    "\n",
    "# Lowercase\n",
    "words_lower = words.select(lower(\"word\").alias(\"word_lower\"))\n",
    "words_lower.show()\n",
    "\n",
    "# Naive punctuation normalization using regex\n",
    "word_norm = words_lower.select(regexp_extract(col(\"word_lower\"), \"[a-z]*\", 0).alias(\"word_normalized\"))\n",
    "word_norm.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29c4340",
   "metadata": {},
   "source": [
    "### Filtering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "79ac3f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|word_nonull|\n",
      "+-----------+\n",
      "|        the|\n",
      "|    project|\n",
      "|  gutenberg|\n",
      "|      ebook|\n",
      "|         of|\n",
      "|      pride|\n",
      "|        and|\n",
      "|  prejudice|\n",
      "|         by|\n",
      "|       jane|\n",
      "|     austen|\n",
      "|       this|\n",
      "|      ebook|\n",
      "|         is|\n",
      "|        for|\n",
      "|        the|\n",
      "|        use|\n",
      "|         of|\n",
      "|     anyone|\n",
      "|   anywhere|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove empty records\n",
    "\n",
    "word_nonull = word_norm.filter(col(\"word_normalized\") != \"\") \\\n",
    "                       .withColumnRenamed('word_normalized', 'word_nonull')\n",
    "word_nonull.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7be6b8",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### 2.1\n",
    "Rewrite the following code snippet, removing the withColumnRenamed method. Which version is clearer and easier to read?\n",
    "\n",
    "```py\n",
    "from pyspark.sql.functions import col, length\n",
    "\n",
    "# The `length` function returns the number of characters in a string column.\n",
    "\n",
    "ex21 = (\n",
    "    spark.read.text(\"./data/Ch02/1342-0.txt\")\n",
    "    .select(length(col(\"value\")))\n",
    "    .withColumnRenamed(\"length(value)\", \"number_of_char\")\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "76cd3269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|values|\n",
      "+------+\n",
      "|    66|\n",
      "|     0|\n",
      "|    64|\n",
      "|    68|\n",
      "|    67|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, length\n",
    "ex21 = (\n",
    "    spark.read.text(\"./data/Ch02/1342-0.txt\")\n",
    "    .select(length(col(\"value\")).alias('values'))\n",
    ")\n",
    "ex21.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78591714",
   "metadata": {},
   "source": [
    "### 2.2\n",
    "The following code blocks gives an error. What is the problem and how can you solve it?\n",
    "\n",
    "```py\n",
    "from pyspark.sql.functions import col, greatest\n",
    "\n",
    "ex22.printSchema()\n",
    "# root\n",
    "#  |-- key: string (containsNull = true)\n",
    "#  |-- value1: long (containsNull = true)\n",
    "#  |-- value2: long (containsNull = true)\n",
    "\n",
    "# `greatest` will return the greatest value of the list of column names,\n",
    "# skipping null value\n",
    "\n",
    "# The following statement will return an error\n",
    "ex22.select(\n",
    "    greatest(col(\"value1\"), col(\"value2\")).alias(\"maximum_value\")\n",
    ").select(\n",
    "    \"key\", \"max_value\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Answer\n",
    "\n",
    "The columns given are not in a list?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70501743",
   "metadata": {},
   "source": [
    "### 2.3\n",
    "\n",
    "Let’s take our words_nonull data frame, available in listing 2.19. You can use the code in the repository (code/Ch02/end_of_chapter.py) into your REPL to get the data frame loaded.\n",
    "\n",
    "a) Remove all of the occurrences of the word \"is\"\n",
    "\n",
    "b) (Challenge) Using the length function explained in exercise 2.1, keep only the words with more than 3 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4cc748cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|words_greater_than_3|\n",
      "+--------------------+\n",
      "|             project|\n",
      "|           gutenberg|\n",
      "|               ebook|\n",
      "|               pride|\n",
      "|           prejudice|\n",
      "|                jane|\n",
      "|              austen|\n",
      "|                this|\n",
      "|               ebook|\n",
      "|              anyone|\n",
      "|            anywhere|\n",
      "|                cost|\n",
      "|                with|\n",
      "|              almost|\n",
      "|        restrictions|\n",
      "|          whatsoever|\n",
      "|                copy|\n",
      "|                give|\n",
      "|                away|\n",
      "|               under|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Remove all of the occurences of the word \"is\",\n",
    "# 2. Using the length function explained in exercise 2.1, keep only the words with more than 3 characters.\n",
    "word_nonull.filter(col(\"word_nonull\") != \"is\") \\\n",
    "           .filter(length(col(\"word_nonull\")) > 3) \\\n",
    "           .withColumnRenamed('word_nonull', 'words_greater_than_3') \\\n",
    "           .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfe2600",
   "metadata": {},
   "source": [
    "### 2.4\n",
    "\n",
    "Remove the words is, not, the and if from your list of words, using a single `where()` method on the words_nonull data frame (see exercise 2.3). Write the code to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5887d9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|word_nonull|\n",
      "+-----------+\n",
      "|    project|\n",
      "|  gutenberg|\n",
      "|      ebook|\n",
      "|         of|\n",
      "|      pride|\n",
      "|        and|\n",
      "|  prejudice|\n",
      "|         by|\n",
      "|       jane|\n",
      "|     austen|\n",
      "|       this|\n",
      "|      ebook|\n",
      "|        for|\n",
      "|        use|\n",
      "|         of|\n",
      "|     anyone|\n",
      "|   anywhere|\n",
      "|         at|\n",
      "|         no|\n",
      "|       cost|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_nonull.where(~col(\"word_nonull\").isin(['is', 'not', 'the', 'if'])) \\\n",
    "           .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66607b8",
   "metadata": {},
   "source": [
    "### 2.5\n",
    "\n",
    "One of your friends come to you with the following code. They have no idea why it doesn’t work. Can you diagnose the problem, explain why it is an error and provide a fix?\n",
    "\n",
    "```py\n",
    "from pyspark.sql.functions import col, split\n",
    "\n",
    "book = spark.read.text(\"./data/ch02/1342-0.txt\")\n",
    "\n",
    "book = book.printSchema()\n",
    "\n",
    "lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
    "\n",
    "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    "```\n",
    "\n",
    "#### Answer\n",
    "\n",
    "They're assigning the output of `book.printSchema()` to `book`, hence writing over the spark data frame.\n",
    "\n",
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "aacc64b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+----------+\n",
      "|      word|\n",
      "+----------+\n",
      "|       The|\n",
      "|   Project|\n",
      "| Gutenberg|\n",
      "|     EBook|\n",
      "|        of|\n",
      "|     Pride|\n",
      "|       and|\n",
      "|Prejudice,|\n",
      "|        by|\n",
      "|      Jane|\n",
      "|    Austen|\n",
      "|          |\n",
      "|      This|\n",
      "|     eBook|\n",
      "|        is|\n",
      "|       for|\n",
      "|       the|\n",
      "|       use|\n",
      "|        of|\n",
      "|    anyone|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, split\n",
    "\n",
    "book = spark.read.text(\"./data/ch02/1342-0.txt\")\n",
    "\n",
    "# Don't assign it back to `book`\n",
    "book.printSchema()\n",
    "\n",
    "lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
    "\n",
    "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    "\n",
    "words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edf3a56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
