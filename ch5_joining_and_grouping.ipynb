{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Joining-Data\" data-toc-modified-id=\"Joining-Data-1\">Joining Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Understanding-the-join-recipe\" data-toc-modified-id=\"Understanding-the-join-recipe-1.1\">Understanding the <code>join</code> recipe</a></span><ul class=\"toc-item\"><li><span><a href=\"#Important-points\" data-toc-modified-id=\"Important-points-1.1.1\">Important points</a></span></li><li><span><a href=\"#Pyspark-helpers-in-join-logic\" data-toc-modified-id=\"Pyspark-helpers-in-join-logic-1.1.2\">Pyspark helpers in join logic</a></span></li><li><span><a href=\"#Setting-up-join-logic-with-how\" data-toc-modified-id=\"Setting-up-join-logic-with-how-1.1.3\">Setting up join logic with <code>how</code></a></span></li></ul></li><li><span><a href=\"#Warning:-What-happens-when-joining-columns-in-a-distributed-environment\" data-toc-modified-id=\"Warning:-What-happens-when-joining-columns-in-a-distributed-environment-1.2\">Warning: What happens when joining columns in a distributed environment</a></span></li><li><span><a href=\"#Warning:-Joining-tables-with-identically-named-columns-leads-to-errors-downstream\" data-toc-modified-id=\"Warning:-Joining-tables-with-identically-named-columns-leads-to-errors-downstream-1.3\">Warning: Joining tables with identically named columns leads to errors downstream</a></span></li><li><span><a href=\"#Solutions-for-preventing-ambiguous-column-references\" data-toc-modified-id=\"Solutions-for-preventing-ambiguous-column-references-1.4\">Solutions for preventing ambiguous column references</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LogIdentifierID: string (nullable = true)\n",
      " |-- LogServiceID: integer (nullable = true)\n",
      " |-- PrimaryFG: integer (nullable = true)\n",
      "\n",
      "+---------------+------------+---------+\n",
      "|LogIdentifierID|LogServiceID|PrimaryFG|\n",
      "+---------------+------------+---------+\n",
      "|           13ST|        3157|        1|\n",
      "|         2000SM|        3466|        1|\n",
      "|           70SM|        3883|        1|\n",
      "|           80SM|        3590|        1|\n",
      "|           90SM|        3470|        1|\n",
      "+---------------+------------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Unique primary channels:  758\n"
     ]
    }
   ],
   "source": [
    "# Set up\n",
    "import os\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Read the data\n",
    "DIRECTORY = \"./data/Ch04\"\n",
    "logs = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"BroadcastLogs_2018_Q3_M8_sample.CSV\"),\n",
    "    sep=\"|\",  # default is \",\"\n",
    "    quote='\"',  # default is double quote.\n",
    "    header=True,  # set first row as column names\n",
    "    inferSchema=True,  # infer schema from column names default False\n",
    ")\n",
    "\n",
    "\n",
    "# Read link table and filter to only primary channels (ie. PrimaryFG == 1)\n",
    "log_identifier = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"ReferenceTables\", \"LogIdentifier.csv\"),\n",
    "    sep=\"|\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ")\n",
    "log_identifier = log_identifier.where(F.col(\"PrimaryFG\") == 1)\n",
    "\n",
    "\n",
    "# Show results\n",
    "log_identifier.printSchema()\n",
    "log_identifier.show(5)\n",
    "print(\"Unique primary channels: \", log_identifier.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the `join` recipe\n",
    "\n",
    "```py\n",
    "[LEFT].join(\n",
    "    [RIGHT],\n",
    "    on=[PREDICTES],\n",
    "    how=[METHOD]\n",
    ")\n",
    "```\n",
    "\n",
    "#### Important points\n",
    "\n",
    "1. If one record in the left table resolves the predicate with more than one record in the right table (or vice versa), __this record will be duplicated in the joined table__.\n",
    "2. If one record in the left or in the right table does not resolve the predicate with any record in the other table, __it will not be present in the resulting table, unless the join method specifies a protocol for failed predicates__.\n",
    "\n",
    "#### Pyspark helpers in join logic\n",
    "\n",
    "- You can put multiple `and` predicates into a list, like:\n",
    "    ```py\n",
    "    [\n",
    "        left[\"col1\"] == right[\"colA\"], \n",
    "        left[\"col2\"] > right[\"colB\"],  # value on left table is greater than the right\n",
    "        left[\"col3\"] != right[\"colC\"]\n",
    "    ]\n",
    "    ```\n",
    "- You can test equality just by specifying the column name, or list of column names\n",
    "\n",
    "#### Setting up join logic with `how`\n",
    "\n",
    "1. `cross` - returns a record for every record pair. not common.\n",
    "2. `inner` = returns record if predicate is true, otherwise drops it. most common, pyspark `join` default. \n",
    "3. `left` & `right` - similar to `inner`, except on what to do with false predicates:\n",
    "    - `left` join adds unmatched records from the left table in the joined table, and fills in columns from right able with `None`\n",
    "    - `right` join adds unmatched records nad fills in column vice versa.\n",
    "4. `outer` - adds unmatched records from the left and right able, padding with `None`.\n",
    "5. `left_semi` - same as inner join but only keeps columns in left table. \n",
    "6. `left_anti` - returns only records that don't match the predicate with any record in the right table.  opposite of `left` join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join `logs` with `log_identifier` using the 'LogServiceID' column\n",
    "joined = logs.join(log_identifier, on=\"LogServiceID\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ProgramClassID: integer (nullable = true)\n",
      " |-- CategoryID: integer (nullable = true)\n",
      " |-- LogServiceID: integer (nullable = true)\n",
      " |-- BroadcastLogID: integer (nullable = true)\n",
      " |-- LogDate: string (nullable = true)\n",
      " |-- SequenceNO: integer (nullable = true)\n",
      " |-- AudienceTargetAgeID: integer (nullable = true)\n",
      " |-- AudienceTargetEthnicID: integer (nullable = true)\n",
      " |-- ClosedCaptionID: integer (nullable = true)\n",
      " |-- CountryOfOriginID: integer (nullable = true)\n",
      " |-- DubDramaCreditID: integer (nullable = true)\n",
      " |-- EthnicProgramID: integer (nullable = true)\n",
      " |-- ProductionSourceID: integer (nullable = true)\n",
      " |-- FilmClassificationID: integer (nullable = true)\n",
      " |-- ExhibitionID: integer (nullable = true)\n",
      " |-- Duration: string (nullable = true)\n",
      " |-- EndTime: string (nullable = true)\n",
      " |-- LogEntryDate: string (nullable = true)\n",
      " |-- ProductionNO: string (nullable = true)\n",
      " |-- ProgramTitle: string (nullable = true)\n",
      " |-- StartTime: string (nullable = true)\n",
      " |-- Subtitle: string (nullable = true)\n",
      " |-- NetworkAffiliationID: integer (nullable = true)\n",
      " |-- SpecialAttentionID: integer (nullable = true)\n",
      " |-- BroadcastOriginPointID: integer (nullable = true)\n",
      " |-- CompositionID: integer (nullable = true)\n",
      " |-- Producer1: string (nullable = true)\n",
      " |-- Producer2: string (nullable = true)\n",
      " |-- Language1: integer (nullable = true)\n",
      " |-- Language2: integer (nullable = true)\n",
      " |-- LogIdentifierID: string (nullable = true)\n",
      " |-- PrimaryFG: integer (nullable = true)\n",
      " |-- CategoryCD: string (nullable = true)\n",
      " |-- Category_Description: string (nullable = true)\n",
      " |-- ProgramClassCD: string (nullable = true)\n",
      " |-- ProgramClass_Description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Additionally join CategoryID and ProgramClassID table\n",
    "# Use left joins since keys may not be available in the link table.\n",
    "\n",
    "# CategoryID\n",
    "cd_category = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"ReferenceTables\", \"CD_Category.csv\"),\n",
    "    sep=\"|\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ").select(\n",
    "    \"CategoryID\",\n",
    "    \"CategoryCD\",\n",
    "    F.col(\"EnglishDescription\").alias(\"Category_Description\"),\n",
    ")\n",
    "\n",
    "# ProgramClass\n",
    "cd_program_class = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"ReferenceTables\", \"CD_ProgramClass.csv\"),\n",
    "    sep=\"|\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ").select(\n",
    "    \"ProgramClassID\",\n",
    "    \"ProgramClassCD\",\n",
    "    F.col(\"EnglishDescription\").alias(\"ProgramClass_Description\"),\n",
    ")\n",
    "\n",
    "\n",
    "# Join all to joined table\n",
    "full_log = joined.join(cd_category, \"CategoryID\", how=\"left\",).join(\n",
    "    cd_program_class, \"ProgramClassID\", how=\"left\",\n",
    ")\n",
    "\n",
    "\n",
    "# Check if additional columns were joined to original log data frame\n",
    "full_log.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warning: What happens when joining columns in a distributed environment\n",
    "\n",
    ">  To be able to process a comparison between records, the data needs to be on the same machine. If not, PySpark will move the data in an operation called a _shuffle_, which is slow and expensive.  More on join strategies in later chapters.\n",
    "\n",
    "### Warning: Joining tables with identically named columns leads to errors downstream\n",
    "\n",
    "PySpark happily joins the two data frames together but fails when we try to work with the ambiguous column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined table now has two \"LogServiceID\" columns:  ['LogServiceID', 'LogServiceID'] \n",
      "\n",
      "Selecting \"LogServiceID\" will now throw an error\n",
      "+------------+\n",
      "|LogServiceID|\n",
      "+------------+\n",
      "|        3157|\n",
      "|        3157|\n",
      "|        3157|\n",
      "|        3157|\n",
      "|        3157|\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "AnalysisException:  Reference 'LogServiceID' is ambiguous, could be: LogServiceID, LogServiceID.;\n"
     ]
    }
   ],
   "source": [
    "# Joining two tables with the same LogServiceID column\n",
    "logs_and_channels_verbose = logs.join(\n",
    "    log_identifier, logs[\"LogServiceID\"] == log_identifier[\"LogServiceID\"]\n",
    ")\n",
    "\n",
    "\n",
    "print(\n",
    "    'Joined table now has two \"LogServiceID\" columns: ',\n",
    "    [col for col in logs_and_channels_verbose.columns if col == \"LogServiceID\"],\n",
    "    \"\\n\",\n",
    ")\n",
    "print('Selecting \"LogServiceID\" will now throw an error')\n",
    "\n",
    "\n",
    "# Selecting \"LogServiceID\" will throw an error\n",
    "try:\n",
    "    logs_and_channels_verbose.select(\"LogServiceID\")\n",
    "except AnalysisException as err:\n",
    "    print(\"AnalysisException: \", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions for preventing ambiguous column references\n",
    "\n",
    "1. Use simplified syntax (ie. passing string of column you want). Auto-removes second instance of predicate column.  Can only use on equi-joins.\n",
    "    ```logs_and_channels = logs.join(log_identifier, \"LogServiceID\")```\n",
    "2. Refer to the pre-existing table name.\n",
    "    ```\n",
    "    logs_and_channels_verbose.select(log_identifier[\"LogServiceID\"])\n",
    "    ```\n",
    "3. Use the `Column` object directly\n",
    "    ```\n",
    "    logs_and_channels_verbose = logs.alias(\"left\").join(\n",
    "    log_identifier.alias(\"right\"),\n",
    "    logs[\"LogServiceID\"] == log_identifier[\"LogServiceID\"],\n",
    "    )\n",
    "\n",
    "    logs_and_channels_verbose.drop(F.col(\"right.LogServiceID\")).select(\n",
    "        \"LogServiceID\"\n",
    "    )\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark]",
   "language": "python",
   "name": "conda-env-pyspark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
