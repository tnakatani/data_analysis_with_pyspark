{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "957edeee",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Aggregation:-groupBy-and-count\" data-toc-modified-id=\"Aggregation:-groupBy-and-count-1\">Aggregation: <code>groupBy</code> and <code>count</code></a></span></li><li><span><a href=\"#Writing-to-file:-csv\" data-toc-modified-id=\"Writing-to-file:-csv-2\">Writing to file: <code>csv</code></a></span></li><li><span><a href=\"#Streamlining-the-code-by-chaining\" data-toc-modified-id=\"Streamlining-the-code-by-chaining-3\">Streamlining the code by chaining</a></span><ul class=\"toc-item\"><li><span><a href=\"#Method-chaining\" data-toc-modified-id=\"Method-chaining-3.1\">Method chaining</a></span></li></ul></li><li><span><a href=\"#Submitting-code-in-batch-mode-using-spark-submit\" data-toc-modified-id=\"Submitting-code-in-batch-mode-using-spark-submit-4\">Submitting code in batch mode using <code>spark-submit</code></a></span></li><li><span><a href=\"#Exercises\" data-toc-modified-id=\"Exercises-5\">Exercises</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95db7068",
   "metadata": {},
   "source": [
    "# Chapter 3: Submitting and scaling your first PySpark program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5a56523",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T02:17:07.776223Z",
     "start_time": "2021-05-13T02:16:59.806539Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"Analyzing the vocabulary of Pride and Prejudice.\"\n",
    ").getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08c5b660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Frame Setup\n",
    "# Set up\n",
    "from pyspark.sql.functions import col, split, lower, explode, regexp_extract\n",
    "\n",
    "book = spark.read.text(\"data/Ch02/1342-0.txt\")\n",
    "lines = book.select(split(col(\"value\"), \" \").alias(\"line\"))\n",
    "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    "words_lower = words.select(lower(\"word\").alias(\"word_lower\"))\n",
    "word_norm = words_lower.select(\n",
    "    regexp_extract(col(\"word_lower\"), \"[a-z]*\", 0).alias(\"word_normalized\")\n",
    ")\n",
    "word_nonull = word_norm.filter(col(\"word_normalized\") != \"\").withColumnRenamed(\n",
    "    \"word_normalized\", \"word_nonull\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efc2a5a",
   "metadata": {},
   "source": [
    "## Aggregation: `groupBy` and `count`\n",
    "\n",
    "- `GroupedData` allows you to perform an aggregate function on each group. \n",
    "- Use `groupby` to count record occurrence, passing columns we want to group.  Returned value is a `GroupedData` object, not a `DataFrame`.  Once you apply a function to it like `count()`, it returns a  `DataFrame`.\n",
    "    - Note that `groupby` and `groupBy` are the same thing.\n",
    "- You can sort the output by `orderBy`\n",
    "    - Note that `orderBy` only exists as camel case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bb3da6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T02:22:17.223409Z",
     "start_time": "2021-05-13T02:22:16.540819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x11b77b910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|word_nonull|count|\n",
      "+-----------+-----+\n",
      "|        the| 4480|\n",
      "|         to| 4218|\n",
      "|         of| 3711|\n",
      "|        and| 3504|\n",
      "|        her| 2199|\n",
      "|          a| 1982|\n",
      "|         in| 1909|\n",
      "|        was| 1838|\n",
      "|          i| 1750|\n",
      "|        she| 1668|\n",
      "|       that| 1487|\n",
      "|         it| 1482|\n",
      "|        not| 1427|\n",
      "|        you| 1301|\n",
      "|         he| 1296|\n",
      "|         be| 1257|\n",
      "|        his| 1247|\n",
      "|         as| 1174|\n",
      "|        had| 1170|\n",
      "|       with| 1092|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "groups = word_nonull.groupBy(col(\"word_nonull\"))\n",
    "display(groups)\n",
    "\n",
    "results = groups.count().orderBy(\"count\", ascending=False)\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728a775f",
   "metadata": {},
   "source": [
    "## Writing to file: `csv`\n",
    "\n",
    "- data frame has `write` method, which can be chained with `csv`\n",
    "- default writes a bunch of separate files (1 file per partition) + `_SUCCESS` file.\n",
    "- use `coalesce` to concat to 1 file\n",
    "- use `.mode('overwrite')` to force write\n",
    "\n",
    "> __TIP:__\n",
    "Never assume that your data frame will keep the same ordering of records unless you explicitly ask via orderBy()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c52925f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write multiple partitions + success file\n",
    "results.write.mode(\"overwrite\").csv(\"./output/results\")\n",
    "\n",
    "# Concatenate into 1 file, then write to disk\n",
    "results.coalesce(1).write.mode(\"overwrite\").csv(\"./output/result_single_partition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238657bc",
   "metadata": {},
   "source": [
    "## Streamlining the code by chaining\n",
    "\n",
    "### Method chaining\n",
    "\n",
    "In PySpark, every transformation returns an object, which is why we need to assign a variable to the result.  This means that PySpark doesnâ€™t perform modifications in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "268dc246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qualified import; import the whole module\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# chain methods together instead of multiple variables\n",
    "results = (\n",
    "    spark.read.text(\"./data/ch02/1342-0.txt\")\n",
    "    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
    "    .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
    "    .select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
    "    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\n",
    "    .where(F.col(\"word\") != \"\")\n",
    "    .groupby(\"word\")\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58c6c2d",
   "metadata": {},
   "source": [
    "## Submitting code in batch mode using `spark-submit`\n",
    "\n",
    "When wrapping a script to be executed with `spark-submit` ratherh than with the `pyspark` command, you'll need to define your `SparkSession` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd9fa7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the|38895|\n",
      "| and|23919|\n",
      "|  of|21199|\n",
      "|  to|20526|\n",
      "|   a|14464|\n",
      "|   i|13973|\n",
      "|  in|12777|\n",
      "|that| 9623|\n",
      "|  it| 9099|\n",
      "| was| 8920|\n",
      "| her| 7923|\n",
      "|  my| 7385|\n",
      "| his| 6642|\n",
      "|with| 6575|\n",
      "|  he| 6444|\n",
      "|  as| 6439|\n",
      "| you| 6295|\n",
      "| had| 5718|\n",
      "| she| 5617|\n",
      "| for| 5425|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This can be wrapped into a `word_counter.py` file and be executed\n",
    "# using `spark-submit`\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"Analyzing the vocabulary of Pride and Prejudice.\"\n",
    ").getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "results = (\n",
    "    spark.read.text(\"./data/ch02/*.txt\")\n",
    "    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
    "    .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
    "    .select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
    "    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\n",
    "    .where(F.col(\"word\") != \"\")\n",
    "    .groupby(\"word\")\n",
    "    .count()\n",
    "    .orderBy(\"count\", ascending=False)\n",
    ")\n",
    "\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac462b0",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "See [chapter 3 code](./code/Ch03/word_count_submit.py)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
