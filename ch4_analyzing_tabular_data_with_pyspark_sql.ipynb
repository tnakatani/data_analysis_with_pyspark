{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing tabular data with pyspark.sql\n",
    "\n",
    "> PySpark operates either on the whole __data frame__ objects (via methods such as `select()` and `groupby()`) or on __Column__ objects (for instance when using a function like `split()`). \n",
    ">\n",
    "> - The data frame is __column-major__, so its API focuses on manipulating the columns to transform the data. \n",
    "> - Hence with data transformations, think about what operations to do and which columns will be impacted.\n",
    "\n",
    "- RDDs on the other hand are _row-major_.  Hence you're thinking about items with attributes in which you apply functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source Info\n",
    "\n",
    "> For this exercise, we’ll use some open data from the Government of Canada, more specifically the CRTC (Canadian Radio-television and Telecommunications Commission). Every broadcaster is mandated to provide a complete log of the programs, commercials and all, showcased to the Canadian public. \n",
    ">\n",
    "> This gives us a lot of potential questions to answer, but we’ll select one specific one: __what are the channels with the most and least proportion of commercials?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a data frame\n",
    "\n",
    "`spark.createDataFrame`\n",
    "- 1st param: data (list of lists, pandas dataframe, RDD)\n",
    "- 2nd param: schema (ie. think column headers in SQL)\n",
    "- Master node knows the structure of the dataframe, but actual data is on worker nodes (ie. cluster memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Item: string (nullable = true)\n",
      " |-- Quantity: long (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example creating a data frame with toy data\n",
    "my_grocery_list = [\n",
    "    [\"Banana\", 2, 1.74],\n",
    "    [\"Apple\", 4, 2.04],\n",
    "    [\"Carrot\", 1, 1.09],\n",
    "    [\"Cake\", 1, 10.99],\n",
    "]\n",
    "\n",
    "df_grocery_list = spark.createDataFrame(my_grocery_list, [\"Item\", \"Quantity\", \"Price\"])\n",
    "\n",
    "df_grocery_list.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data frame structure\n",
    "\n",
    "Composed of _row delimiter_ (e.g. newline `\\n`) and _column delimiter_ (e.g. tabs `\\t` for TSVs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = \"./data/Ch04\"\n",
    "logs = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"BroadcastLogs_2018_Q3_M8_sample.CSV\"),\n",
    "    sep=\"|\",  # default is \",\"\n",
    "    quote=\"\\\"\", # default is double quote.\n",
    "    header=True, # set first row as column names\n",
    "    inferSchema=True, # infer schema from column names default False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- BroadcastLogID: integer (nullable = true)\n",
      " |-- LogServiceID: integer (nullable = true)\n",
      " |-- LogDate: string (nullable = true)\n",
      " |-- SequenceNO: integer (nullable = true)\n",
      " |-- AudienceTargetAgeID: integer (nullable = true)\n",
      " |-- AudienceTargetEthnicID: integer (nullable = true)\n",
      " |-- CategoryID: integer (nullable = true)\n",
      " |-- ClosedCaptionID: integer (nullable = true)\n",
      " |-- CountryOfOriginID: integer (nullable = true)\n",
      " |-- DubDramaCreditID: integer (nullable = true)\n",
      " |-- EthnicProgramID: integer (nullable = true)\n",
      " |-- ProductionSourceID: integer (nullable = true)\n",
      " |-- ProgramClassID: integer (nullable = true)\n",
      " |-- FilmClassificationID: integer (nullable = true)\n",
      " |-- ExhibitionID: integer (nullable = true)\n",
      " |-- Duration: string (nullable = true)\n",
      " |-- EndTime: string (nullable = true)\n",
      " |-- LogEntryDate: string (nullable = true)\n",
      " |-- ProductionNO: string (nullable = true)\n",
      " |-- ProgramTitle: string (nullable = true)\n",
      " |-- StartTime: string (nullable = true)\n",
      " |-- Subtitle: string (nullable = true)\n",
      " |-- NetworkAffiliationID: integer (nullable = true)\n",
      " |-- SpecialAttentionID: integer (nullable = true)\n",
      " |-- BroadcastOriginPointID: integer (nullable = true)\n",
      " |-- CompositionID: integer (nullable = true)\n",
      " |-- Producer1: string (nullable = true)\n",
      " |-- Producer2: string (nullable = true)\n",
      " |-- Language1: integer (nullable = true)\n",
      " |-- Language2: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "##  Exercises\n",
    "\n",
    "### 4.1\n",
    "Take the following file, called sample.csv, and read it into a dataframe.\n",
    "\n",
    "```\n",
    "Item,Quantity,Price\n",
    "$Banana, organic$,1,0.99\n",
    "Pear,7,1.24\n",
    "$Cake, chocolate$,1,14.50\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+-----+\n",
      "|           Item|Quantity|Price|\n",
      "+---------------+--------+-----+\n",
      "|Banana, organic|       1| 0.99|\n",
      "|           Pear|       7| 1.24|\n",
      "|Cake, chocolate|       1| 14.5|\n",
      "+---------------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample =  spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"ch4_exercise.csv\"),\n",
    "    sep=\",\",\n",
    "    header=True,\n",
    "    quote=\"$\",\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "sample.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2\n",
    "\n",
    "Re-read the data in a `logs_raw` data frame, taking inspiration from the code in listing 4.3, this time without passing any optional parameters. Print the first 5 rows of data, as well as the schema. What are the differences in terms of data and schema between logs and logs_raw?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|_c0                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|BroadcastLogID|LogServiceID|LogDate|SequenceNO|AudienceTargetAgeID|AudienceTargetEthnicID|CategoryID|ClosedCaptionID|CountryOfOriginID|DubDramaCreditID|EthnicProgramID|ProductionSourceID|ProgramClassID|FilmClassificationID|ExhibitionID|Duration|EndTime|LogEntryDate|ProductionNO|ProgramTitle|StartTime|Subtitle|NetworkAffiliationID|SpecialAttentionID|BroadcastOriginPointID|CompositionID|Producer1|Producer2|Language1|Language2|\n",
      "|1196192316|3157|2018-08-01|1|4||13|3|3|||10|19||2|02:00:00.0000000|08:00:00.0000000|2018-08-01|A39082|Newlywed and Dead|06:00:00.0000000||||||||94|                                                                                                                                                                                                                                                                                        |\n",
      "|1196192317|3157|2018-08-01|2||||1|||||20|||00:00:30.0000000|06:13:45.0000000|2018-08-01||15-SPECIALTY CHANNELS-Canadian Generic|06:13:15.0000000|||||||||                                                                                                                                                                                                                                                                                  |\n",
      "|1196192318|3157|2018-08-01|3||||1|||||3|||00:00:15.0000000|06:14:00.0000000|2018-08-01||3-PROCTER & GAMBLE INC-Anti-Perspirant 3rd|06:13:45.0000000|||||||||                                                                                                                                                                                                                                                                               |\n",
      "|1196192319|3157|2018-08-01|4||||1|||||3|||00:00:15.0000000|06:14:15.0000000|2018-08-01||12-CREDIT KARMA-Bank/Credit Union/Trust 3rd|06:14:00.0000000|||||||||                                                                                                                                                                                                                                                                              |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DIRECTORY = \"./data/Ch04\"\n",
    "raw_logs = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"BroadcastLogs_2018_Q3_M8_sample.CSV\"),\n",
    ")\n",
    "raw_logs.show(5, False) # False = show entire contents\n",
    "raw_logs.printSchema()\n",
    "\n",
    "# Result shows entire row concatenated into one column (_c0). Not what we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exploring the shape of our data universe\n",
    "\n",
    "### About Star Schema\n",
    "\n",
    "\n",
    "Wiki:\n",
    "> In computing, the __star schema__ is the simplest style of data mart schema and is the approach most widely used to develop data warehouses and dimensional data marts. The star schema consists of one or more fact tables referencing any number of dimension tables.\n",
    "\n",
    "Star schemas are common in the relational database world because of __normalization__, a process used to avoid duplicating pieces of data and improve data integrity.\n",
    "\n",
    "Spark uses __denormalized__ tables (ie __fat__ tables). Why? Mainly because it is easier to run analyses on a single table.  \n",
    "  - If you do need to analyze complex star schema, best bet is to work with a database manger to get a denormalized table.\n",
    "  \n",
    "### `select`-ing what we want to see\n",
    "\n",
    "Four ways to `select` colums in PySpark, all equivalent in term of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[BroadCastLogID: int, LogServiceID: int, LogDate: string]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the string to column conversion\n",
    "logs.select(\"BroadCastLogID\", \"LogServiceID\", \"LogDate\")\n",
    "logs.select(*[\"BroadCastLogID\", \"LogServiceID\", \"LogDate\"]) # Unpack list with star prefix\n",
    "\n",
    "# Passing the column object explicitly\n",
    "logs.select(F.col(\"BroadCastLogID\"), F.col(\"LogServiceID\"), F.col(\"LogDate\"))\n",
    "logs.select(*[F.col(\"BroadCastLogID\"), F.col(\"LogServiceID\"), F.col(\"LogDate\")]) # Unpack list with star prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the width of our data frame, we could split our columns into manageable sets of three to keep the output tidy on the screen. This gives a high-level view of what the data frame contains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Columns in groups of three'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[array(['BroadcastLogID', 'LogServiceID', 'LogDate'], dtype='<U22'),\n",
       " array(['SequenceNO', 'AudienceTargetAgeID', 'AudienceTargetEthnicID'],\n",
       "       dtype='<U22'),\n",
       " array(['CategoryID', 'ClosedCaptionID', 'CountryOfOriginID'], dtype='<U22'),\n",
       " array(['DubDramaCreditID', 'EthnicProgramID', 'ProductionSourceID'],\n",
       "       dtype='<U22'),\n",
       " array(['ProgramClassID', 'FilmClassificationID', 'ExhibitionID'],\n",
       "       dtype='<U22'),\n",
       " array(['Duration', 'EndTime', 'LogEntryDate'], dtype='<U22'),\n",
       " array(['ProductionNO', 'ProgramTitle', 'StartTime'], dtype='<U22'),\n",
       " array(['Subtitle', 'NetworkAffiliationID', 'SpecialAttentionID'],\n",
       "       dtype='<U22'),\n",
       " array(['BroadcastOriginPointID', 'CompositionID', 'Producer1'],\n",
       "       dtype='<U22'),\n",
       " array(['Producer2', 'Language1', 'Language2'], dtype='<U22')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Table display in column groups of three'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+----------+\n",
      "|BroadcastLogID|LogServiceID|LogDate   |\n",
      "+--------------+------------+----------+\n",
      "|1196192316    |3157        |2018-08-01|\n",
      "|1196192317    |3157        |2018-08-01|\n",
      "|1196192318    |3157        |2018-08-01|\n",
      "|1196192319    |3157        |2018-08-01|\n",
      "|1196192320    |3157        |2018-08-01|\n",
      "+--------------+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+-------------------+----------------------+\n",
      "|SequenceNO|AudienceTargetAgeID|AudienceTargetEthnicID|\n",
      "+----------+-------------------+----------------------+\n",
      "|1         |4                  |null                  |\n",
      "|2         |null               |null                  |\n",
      "|3         |null               |null                  |\n",
      "|4         |null               |null                  |\n",
      "|5         |null               |null                  |\n",
      "+----------+-------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+---------------+-----------------+\n",
      "|CategoryID|ClosedCaptionID|CountryOfOriginID|\n",
      "+----------+---------------+-----------------+\n",
      "|13        |3              |3                |\n",
      "|null      |1              |null             |\n",
      "|null      |1              |null             |\n",
      "|null      |1              |null             |\n",
      "|null      |1              |null             |\n",
      "+----------+---------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------------+---------------+------------------+\n",
      "|DubDramaCreditID|EthnicProgramID|ProductionSourceID|\n",
      "+----------------+---------------+------------------+\n",
      "|null            |null           |10                |\n",
      "|null            |null           |null              |\n",
      "|null            |null           |null              |\n",
      "|null            |null           |null              |\n",
      "|null            |null           |null              |\n",
      "+----------------+---------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------+--------------------+------------+\n",
      "|ProgramClassID|FilmClassificationID|ExhibitionID|\n",
      "+--------------+--------------------+------------+\n",
      "|19            |null                |2           |\n",
      "|20            |null                |null        |\n",
      "|3             |null                |null        |\n",
      "|3             |null                |null        |\n",
      "|3             |null                |null        |\n",
      "+--------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------------+----------------+------------+\n",
      "|Duration        |EndTime         |LogEntryDate|\n",
      "+----------------+----------------+------------+\n",
      "|02:00:00.0000000|08:00:00.0000000|2018-08-01  |\n",
      "|00:00:30.0000000|06:13:45.0000000|2018-08-01  |\n",
      "|00:00:15.0000000|06:14:00.0000000|2018-08-01  |\n",
      "|00:00:15.0000000|06:14:15.0000000|2018-08-01  |\n",
      "|00:00:15.0000000|06:14:30.0000000|2018-08-01  |\n",
      "+----------------+----------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------+-------------------------------------------+----------------+\n",
      "|ProductionNO|ProgramTitle                               |StartTime       |\n",
      "+------------+-------------------------------------------+----------------+\n",
      "|A39082      |Newlywed and Dead                          |06:00:00.0000000|\n",
      "|null        |15-SPECIALTY CHANNELS-Canadian Generic     |06:13:15.0000000|\n",
      "|null        |3-PROCTER & GAMBLE INC-Anti-Perspirant 3rd |06:13:45.0000000|\n",
      "|null        |12-CREDIT KARMA-Bank/Credit Union/Trust 3rd|06:14:00.0000000|\n",
      "|null        |3-L'OREAL CANADA-Hair Products 3rd         |06:14:15.0000000|\n",
      "+------------+-------------------------------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+--------------------+------------------+\n",
      "|Subtitle|NetworkAffiliationID|SpecialAttentionID|\n",
      "+--------+--------------------+------------------+\n",
      "|null    |null                |null              |\n",
      "|null    |null                |null              |\n",
      "|null    |null                |null              |\n",
      "|null    |null                |null              |\n",
      "|null    |null                |null              |\n",
      "+--------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------------------+-------------+---------+\n",
      "|BroadcastOriginPointID|CompositionID|Producer1|\n",
      "+----------------------+-------------+---------+\n",
      "|null                  |null         |null     |\n",
      "|null                  |null         |null     |\n",
      "|null                  |null         |null     |\n",
      "|null                  |null         |null     |\n",
      "|null                  |null         |null     |\n",
      "+----------------------+-------------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+---------+---------+\n",
      "|Producer2|Language1|Language2|\n",
      "+---------+---------+---------+\n",
      "|null     |94       |null     |\n",
      "|null     |null     |null     |\n",
      "|null     |null     |null     |\n",
      "|null     |null     |null     |\n",
      "|null     |null     |null     |\n",
      "+---------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting columns in groups of three using numpy\n",
    "display(\"Columns in groups of three\")\n",
    "column_split = np.array_split(np.array(logs.columns), len(logs.columns) // 3)\n",
    "display(column_split)\n",
    "\n",
    "# Show columns in groups of three\n",
    "display(\"Table display in column groups of three\")\n",
    "for x in column_split:\n",
    "    logs.select(*x).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `drop`-ing columns we don't need\n",
    "\n",
    "Remove `BroadCastLogID` (primary key not needed in single table) and `SequenceNo`.  `drop()` returns a new data frame.\n",
    "\n",
    "#### Warning with `drop`\n",
    "Unlike `select()`, where selecting a column that doesn’t exist will return a runtime error, dropping a non-existent column is a no-op. PySpark will __just ignore the columns it doesn’t find__. Careful with the spelling of your column names!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = logs.drop(\"BroadCastLogID\", \"SequenceNo\")\n",
    "\n",
    "assert all(col not in logs.columns for col in [\"BroadCastLogID\", \"SequenceNo\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternate method of above just using `select` using list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = logs.select(\n",
    "    *[col for col in logs.columns if col not in [\"BroadCastLogID\", \"SequenceNo\"]]\n",
    ")\n",
    "\n",
    "assert all(col not in logs.columns for col in [\"BroadCastLogID\", \"SequenceNo\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### 4.3\n",
    "\n",
    "Create a new data frame logs_clean that contains only the columns that do not end with ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LogDate', 'SequenceNO', 'Duration', 'EndTime', 'LogEntryDate', 'ProductionNO', 'ProgramTitle', 'StartTime', 'Subtitle', 'Producer1', 'Producer2', 'Language1', 'Language2']\n"
     ]
    }
   ],
   "source": [
    "print([col for col in logs.columns if col[-2:] != \"ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered results (not end with 'ID')\n",
      "root\n",
      " |-- LogDate: string (nullable = true)\n",
      " |-- SequenceNO: integer (nullable = true)\n",
      " |-- Duration: string (nullable = true)\n",
      " |-- EndTime: string (nullable = true)\n",
      " |-- LogEntryDate: string (nullable = true)\n",
      " |-- ProductionNO: string (nullable = true)\n",
      " |-- ProgramTitle: string (nullable = true)\n",
      " |-- StartTime: string (nullable = true)\n",
      " |-- Subtitle: string (nullable = true)\n",
      " |-- Producer1: string (nullable = true)\n",
      " |-- Producer2: string (nullable = true)\n",
      " |-- Language1: integer (nullable = true)\n",
      " |-- Language2: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load original CSV again\n",
    "DIRECTORY = \"./data/Ch04\"\n",
    "logs = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"BroadcastLogs_2018_Q3_M8_sample.CSV\"),\n",
    "    sep=\"|\",  # default is \",\"\n",
    "    quote=\"\\\"\", # default is double quote.\n",
    "    header=True, # set first row as column names\n",
    "    inferSchema=True, # infer schema from column names default False\n",
    ")\n",
    "\n",
    "# Filter to columns that don't end with \"ID\"\n",
    "logs_no_id = logs.select(\n",
    "    *[col for col in logs.columns if col[-2:].lower() != \"id\"]\n",
    ")\n",
    "print(\"Filtered results (not end with 'ID')\")\n",
    "logs_no_id.printSchema()\n",
    "\n",
    "assert all(\"id\" not in col[-2:] for col in logs_no_id.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating new columns with `withColumn`\n",
    "\n",
    "### 1. Check the data type of 'Duration' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|        Duration|\n",
      "+----------------+\n",
      "|02:00:00.0000000|\n",
      "|00:00:30.0000000|\n",
      "|00:00:15.0000000|\n",
      "|00:00:15.0000000|\n",
      "|00:00:15.0000000|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "dtype of 'Duration' column is 'string'. Best to convert to timestamp:\n",
      " [('Duration', 'string')]\n"
     ]
    }
   ],
   "source": [
    "logs.select(F.col(\"Duration\")).show(5)\n",
    "\n",
    "print(\"dtype of 'Duration' column is 'string'. Best to convert to timestamp:\\n\", logs.select(F.col(\"Duration\")).dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extract time features from Duration column only show distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+-------+-------+----------------+\n",
      "|        Duration|hours|minutes|seconds|duration_seconds|\n",
      "+----------------+-----+-------+-------+----------------+\n",
      "|00:00:19.0000000|    0|      0|     19|              19|\n",
      "|00:07:09.0000000|    0|      7|      9|             429|\n",
      "|00:53:26.0000000|    0|     53|     26|            3206|\n",
      "|00:30:43.0000000|    0|     30|     43|            1843|\n",
      "|00:02:41.0000000|    0|      2|     41|             161|\n",
      "+----------------+-----+-------+-------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs.select(\n",
    "    F.col(\"Duration\"),\n",
    "    F.col(\"Duration\").substr(1,2).cast('int').alias('hours'),\n",
    "    F.col(\"Duration\").substr(4,2).cast('int').alias('minutes'),\n",
    "    F.col(\"Duration\").substr(7,2).cast('int').alias('seconds'),\n",
    "    # Add final column converting duration into total seconds\n",
    "    (\n",
    "        F.col(\"Duration\").substr(1,2).cast('int') * 60 * 60\n",
    "        + F.col(\"Duration\").substr(4,2).cast('int') * 60\n",
    "        + F.col(\"Duration\").substr(7,2).cast('int')\n",
    "    ).alias('duration_seconds')\n",
    ").distinct().show(5) # only show distinct entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Use `withColumn()` to add 'duration_seconds' to original data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = logs.withColumn(\n",
    "    \"duration_seconds\",\n",
    "    F.col(\"Duration\").substr(1,2).cast('int') * 60 * 60\n",
    "    + F.col(\"Duration\").substr(4,2).cast('int') * 60\n",
    "    + F.col(\"Duration\").substr(7,2).cast('int')\n",
    ")\n",
    "\n",
    "assert \"duration_seconds\" in logs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark]",
   "language": "python",
   "name": "conda-env-pyspark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
