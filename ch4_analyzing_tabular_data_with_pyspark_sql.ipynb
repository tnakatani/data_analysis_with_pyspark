{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-1\">Summary</a></span></li><li><span><a href=\"#On-dataframes\" data-toc-modified-id=\"On-dataframes-2\">On dataframes</a></span></li><li><span><a href=\"#Data-Source-Info\" data-toc-modified-id=\"Data-Source-Info-3\">Data Source Info</a></span></li><li><span><a href=\"#Creating-a-data-frame\" data-toc-modified-id=\"Creating-a-data-frame-4\">Creating a data frame</a></span></li><li><span><a href=\"#Reading-a-data-frame\" data-toc-modified-id=\"Reading-a-data-frame-5\">Reading a data frame</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-frame-structure\" data-toc-modified-id=\"Data-frame-structure-5.1\">Data frame structure</a></span></li></ul></li><li><span><a href=\"#Exercises\" data-toc-modified-id=\"Exercises-6\">Exercises</a></span><ul class=\"toc-item\"><li><span><a href=\"#4.1\" data-toc-modified-id=\"4.1-6.1\">4.1</a></span></li><li><span><a href=\"#4.2\" data-toc-modified-id=\"4.2-6.2\">4.2</a></span></li></ul></li><li><span><a href=\"#Exploring-the-shape-of-our-data-universe\" data-toc-modified-id=\"Exploring-the-shape-of-our-data-universe-7\">Exploring the shape of our data universe</a></span><ul class=\"toc-item\"><li><span><a href=\"#About-Star-Schema\" data-toc-modified-id=\"About-Star-Schema-7.1\">About Star Schema</a></span></li><li><span><a href=\"#select-ing-what-we-want-to-see\" data-toc-modified-id=\"select-ing-what-we-want-to-see-7.2\"><code>select</code>-ing what we want to see</a></span></li><li><span><a href=\"#drop-ing-columns-we-don't-need\" data-toc-modified-id=\"drop-ing-columns-we-don't-need-7.3\"><code>drop</code>-ing columns we don't need</a></span></li></ul></li><li><span><a href=\"#Exercises\" data-toc-modified-id=\"Exercises-8\">Exercises</a></span><ul class=\"toc-item\"><li><span><a href=\"#4.3\" data-toc-modified-id=\"4.3-8.1\">4.3</a></span></li></ul></li><li><span><a href=\"#Creating-new-columns-with-withColumn\" data-toc-modified-id=\"Creating-new-columns-with-withColumn-9\">Creating new columns with <code>withColumn</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#1.-Check-the-data-type-of-'Duration'-column\" data-toc-modified-id=\"1.-Check-the-data-type-of-'Duration'-column-9.1\">1. Check the data type of 'Duration' column</a></span></li><li><span><a href=\"#2.-Extract-time-features-from-Duration-column-only-show-distinct\" data-toc-modified-id=\"2.-Extract-time-features-from-Duration-column-only-show-distinct-9.2\">2. Extract time features from Duration column only show distinct</a></span></li><li><span><a href=\"#3.-Use-withColumn()-to-add-'duration_seconds'-to-original-data-frame\" data-toc-modified-id=\"3.-Use-withColumn()-to-add-'duration_seconds'-to-original-data-frame-9.3\">3. Use <code>withColumn()</code> to add 'duration_seconds' to original data frame</a></span></li></ul></li><li><span><a href=\"#Batch-renaming-with-toDF()\" data-toc-modified-id=\"Batch-renaming-with-toDF()-10\">Batch renaming with <code>toDF()</code></a></span></li><li><span><a href=\"#Sorting-column-order-with-sort\" data-toc-modified-id=\"Sorting-column-order-with-sort-11\">Sorting column order with <code>sort</code></a></span></li><li><span><a href=\"#Getting-high-level-summary-of-your-dataframe-with-describe-and-summary\" data-toc-modified-id=\"Getting-high-level-summary-of-your-dataframe-with-describe-and-summary-12\">Getting high level summary of your dataframe with <code>describe</code> and <code>summary</code></a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing tabular data with pyspark.sql\n",
    "\n",
    "## Summary\n",
    "\n",
    "- PySpark uses the `SparkReader` object to read any kind of data directly in a data frame. The specialized `CSV SparkReader` is used to ingest comma-separated value (CSV) files. Just like when reading text, the only mandatory parameter is the source location.\n",
    "- The CSV format is very versatile, so PySpark provides many optional parameters to account for this flexibility. The most important ones are the `field` delimiter, the `record` delimiter, and the `quotation` character. All of those parameters have sensible defaults.\n",
    "- PySpark can infer the Schema of a CSV file by setting the `inferSchema` optional parameter to True. PySpark accomplishes this by reading the data twice: once for setting the appropriate types for each columns, and another time to ingest the data in the inferred format.\n",
    "- Tabular data is represented into a data frame in a series of Columns, each having a name and a type. Since the data frame is a column-major data structure, the concept of row is less relevant.\n",
    "- You can use Python code to explore the data efficiently, using the column list as any Python list to expose the elements of the data frame of interest.\n",
    "- The most common operations on a data frame are the selection, deletion, and creation or columns. In PySpark, the methods used are `select()`, `drop()` and `withColumn()`, respectively.\n",
    "- select can be used for column re-ordering by passing a re-ordered list of columns.\n",
    "- You can rename columns one by one with the `withColumnRenamed()` method, or all at once by using the `toDF()` method.\n",
    "- You can display a summary of the columns with the `describe()` or `summary()` method. `describe()` has a fixed set of metrics, while `summary()` will take functions as parameters and apply them to all columns.\n",
    "\n",
    "## On dataframes\n",
    "> PySpark operates either on the whole __data frame__ objects (via methods such as `select()` and `groupby()`) or on __Column__ objects (for instance when using a function like `split()`). \n",
    ">\n",
    "> - The data frame is __column-major__, so its API focuses on manipulating the columns to transform the data. \n",
    "> - Hence with data transformations, think about what operations to do and which columns will be impacted.\n",
    "\n",
    "- RDDs on the other hand are _row-major_.  Hence you're thinking about items with attributes in which you apply functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source Info\n",
    "\n",
    "> For this exercise, we’ll use some open data from the Government of Canada, more specifically the CRTC (Canadian Radio-television and Telecommunications Commission). Every broadcaster is mandated to provide a complete log of the programs, commercials and all, showcased to the Canadian public. \n",
    ">\n",
    "> This gives us a lot of potential questions to answer, but we’ll select one specific one: __what are the channels with the most and least proportion of commercials?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a data frame\n",
    "\n",
    "`spark.createDataFrame`\n",
    "- 1st param: data (list of lists, pandas dataframe, RDD)\n",
    "- 2nd param: schema (ie. think column headers in SQL)\n",
    "- Master node knows the structure of the dataframe, but actual data is on worker nodes (ie. cluster memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Item: string (nullable = true)\n",
      " |-- Quantity: long (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example creating a data frame with toy data\n",
    "my_grocery_list = [\n",
    "    [\"Banana\", 2, 1.74],\n",
    "    [\"Apple\", 4, 2.04],\n",
    "    [\"Carrot\", 1, 1.09],\n",
    "    [\"Cake\", 1, 10.99],\n",
    "]\n",
    "\n",
    "df_grocery_list = spark.createDataFrame(my_grocery_list, [\"Item\", \"Quantity\", \"Price\"])\n",
    "\n",
    "df_grocery_list.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data frame structure\n",
    "\n",
    "Composed of _row delimiter_ (e.g. newline `\\n`) and _column delimiter_ (e.g. tabs `\\t` for TSVs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = \"./data/Ch04\"\n",
    "logs = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"BroadcastLogs_2018_Q3_M8_sample.CSV\"),\n",
    "    sep=\"|\",  # default is \",\"\n",
    "    quote='\"',  # default is double quote.\n",
    "    header=True,  # set first row as column names\n",
    "    inferSchema=True,  # infer schema from column names default False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- BroadcastLogID: integer (nullable = true)\n",
      " |-- LogServiceID: integer (nullable = true)\n",
      " |-- LogDate: string (nullable = true)\n",
      " |-- SequenceNO: integer (nullable = true)\n",
      " |-- AudienceTargetAgeID: integer (nullable = true)\n",
      " |-- AudienceTargetEthnicID: integer (nullable = true)\n",
      " |-- CategoryID: integer (nullable = true)\n",
      " |-- ClosedCaptionID: integer (nullable = true)\n",
      " |-- CountryOfOriginID: integer (nullable = true)\n",
      " |-- DubDramaCreditID: integer (nullable = true)\n",
      " |-- EthnicProgramID: integer (nullable = true)\n",
      " |-- ProductionSourceID: integer (nullable = true)\n",
      " |-- ProgramClassID: integer (nullable = true)\n",
      " |-- FilmClassificationID: integer (nullable = true)\n",
      " |-- ExhibitionID: integer (nullable = true)\n",
      " |-- Duration: string (nullable = true)\n",
      " |-- EndTime: string (nullable = true)\n",
      " |-- LogEntryDate: string (nullable = true)\n",
      " |-- ProductionNO: string (nullable = true)\n",
      " |-- ProgramTitle: string (nullable = true)\n",
      " |-- StartTime: string (nullable = true)\n",
      " |-- Subtitle: string (nullable = true)\n",
      " |-- NetworkAffiliationID: integer (nullable = true)\n",
      " |-- SpecialAttentionID: integer (nullable = true)\n",
      " |-- BroadcastOriginPointID: integer (nullable = true)\n",
      " |-- CompositionID: integer (nullable = true)\n",
      " |-- Producer1: string (nullable = true)\n",
      " |-- Producer2: string (nullable = true)\n",
      " |-- Language1: integer (nullable = true)\n",
      " |-- Language2: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "##  Exercises\n",
    "\n",
    "### 4.1\n",
    "Take the following file, called sample.csv, and read it into a dataframe.\n",
    "\n",
    "```\n",
    "Item,Quantity,Price\n",
    "$Banana, organic$,1,0.99\n",
    "Pear,7,1.24\n",
    "$Cake, chocolate$,1,14.50\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+-----+\n",
      "|           Item|Quantity|Price|\n",
      "+---------------+--------+-----+\n",
      "|Banana, organic|       1| 0.99|\n",
      "|           Pear|       7| 1.24|\n",
      "|Cake, chocolate|       1| 14.5|\n",
      "+---------------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"ch4_exercise.csv\"),\n",
    "    sep=\",\",\n",
    "    header=True,\n",
    "    quote=\"$\",\n",
    "    inferSchema=True,\n",
    ")\n",
    "\n",
    "sample.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2\n",
    "\n",
    "Re-read the data in a `logs_raw` data frame, taking inspiration from the code in listing 4.3, this time without passing any optional parameters. Print the first 5 rows of data, as well as the schema. What are the differences in terms of data and schema between logs and logs_raw?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|_c0                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|BroadcastLogID|LogServiceID|LogDate|SequenceNO|AudienceTargetAgeID|AudienceTargetEthnicID|CategoryID|ClosedCaptionID|CountryOfOriginID|DubDramaCreditID|EthnicProgramID|ProductionSourceID|ProgramClassID|FilmClassificationID|ExhibitionID|Duration|EndTime|LogEntryDate|ProductionNO|ProgramTitle|StartTime|Subtitle|NetworkAffiliationID|SpecialAttentionID|BroadcastOriginPointID|CompositionID|Producer1|Producer2|Language1|Language2|\n",
      "|1196192316|3157|2018-08-01|1|4||13|3|3|||10|19||2|02:00:00.0000000|08:00:00.0000000|2018-08-01|A39082|Newlywed and Dead|06:00:00.0000000||||||||94|                                                                                                                                                                                                                                                                                        |\n",
      "|1196192317|3157|2018-08-01|2||||1|||||20|||00:00:30.0000000|06:13:45.0000000|2018-08-01||15-SPECIALTY CHANNELS-Canadian Generic|06:13:15.0000000|||||||||                                                                                                                                                                                                                                                                                  |\n",
      "|1196192318|3157|2018-08-01|3||||1|||||3|||00:00:15.0000000|06:14:00.0000000|2018-08-01||3-PROCTER & GAMBLE INC-Anti-Perspirant 3rd|06:13:45.0000000|||||||||                                                                                                                                                                                                                                                                               |\n",
      "|1196192319|3157|2018-08-01|4||||1|||||3|||00:00:15.0000000|06:14:15.0000000|2018-08-01||12-CREDIT KARMA-Bank/Credit Union/Trust 3rd|06:14:00.0000000|||||||||                                                                                                                                                                                                                                                                              |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DIRECTORY = \"./data/Ch04\"\n",
    "raw_logs = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"BroadcastLogs_2018_Q3_M8_sample.CSV\"),\n",
    ")\n",
    "raw_logs.show(5, False)  # False = show entire contents\n",
    "raw_logs.printSchema()\n",
    "\n",
    "# Result shows entire row concatenated into one column (_c0). Not what we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exploring the shape of our data universe\n",
    "\n",
    "### About Star Schema\n",
    "\n",
    "\n",
    "Wiki:\n",
    "> In computing, the __star schema__ is the simplest style of data mart schema and is the approach most widely used to develop data warehouses and dimensional data marts. The star schema consists of one or more fact tables referencing any number of dimension tables.\n",
    "\n",
    "Star schemas are common in the relational database world because of __normalization__, a process used to avoid duplicating pieces of data and improve data integrity.\n",
    "\n",
    "Spark uses __denormalized__ tables (ie __fat__ tables). Why? Mainly because it is easier to run analyses on a single table.  \n",
    "  - If you do need to analyze complex star schema, best bet is to work with a database manger to get a denormalized table.\n",
    "  \n",
    "### `select`-ing what we want to see\n",
    "\n",
    "Four ways to `select` colums in PySpark, all equivalent in term of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[BroadCastLogID: int, LogServiceID: int, LogDate: string]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the string to column conversion\n",
    "logs.select(\"BroadCastLogID\", \"LogServiceID\", \"LogDate\")\n",
    "logs.select(\n",
    "    *[\"BroadCastLogID\", \"LogServiceID\", \"LogDate\"]\n",
    ")  # Unpack list with star prefix\n",
    "\n",
    "# Passing the column object explicitly\n",
    "logs.select(F.col(\"BroadCastLogID\"), F.col(\"LogServiceID\"), F.col(\"LogDate\"))\n",
    "logs.select(\n",
    "    *[F.col(\"BroadCastLogID\"), F.col(\"LogServiceID\"), F.col(\"LogDate\")]\n",
    ")  # Unpack list with star prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the width of our data frame, we could split our columns into manageable sets of three to keep the output tidy on the screen. This gives a high-level view of what the data frame contains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Columns in groups of three'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[array(['BroadcastLogID', 'LogServiceID', 'LogDate'], dtype='<U22'),\n",
       " array(['SequenceNO', 'AudienceTargetAgeID', 'AudienceTargetEthnicID'],\n",
       "       dtype='<U22'),\n",
       " array(['CategoryID', 'ClosedCaptionID', 'CountryOfOriginID'], dtype='<U22'),\n",
       " array(['DubDramaCreditID', 'EthnicProgramID', 'ProductionSourceID'],\n",
       "       dtype='<U22'),\n",
       " array(['ProgramClassID', 'FilmClassificationID', 'ExhibitionID'],\n",
       "       dtype='<U22'),\n",
       " array(['Duration', 'EndTime', 'LogEntryDate'], dtype='<U22'),\n",
       " array(['ProductionNO', 'ProgramTitle', 'StartTime'], dtype='<U22'),\n",
       " array(['Subtitle', 'NetworkAffiliationID', 'SpecialAttentionID'],\n",
       "       dtype='<U22'),\n",
       " array(['BroadcastOriginPointID', 'CompositionID', 'Producer1'],\n",
       "       dtype='<U22'),\n",
       " array(['Producer2', 'Language1', 'Language2'], dtype='<U22')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Table display in column groups of three'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+----------+\n",
      "|BroadcastLogID|LogServiceID|LogDate   |\n",
      "+--------------+------------+----------+\n",
      "|1196192316    |3157        |2018-08-01|\n",
      "|1196192317    |3157        |2018-08-01|\n",
      "|1196192318    |3157        |2018-08-01|\n",
      "|1196192319    |3157        |2018-08-01|\n",
      "|1196192320    |3157        |2018-08-01|\n",
      "+--------------+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+-------------------+----------------------+\n",
      "|SequenceNO|AudienceTargetAgeID|AudienceTargetEthnicID|\n",
      "+----------+-------------------+----------------------+\n",
      "|1         |4                  |null                  |\n",
      "|2         |null               |null                  |\n",
      "|3         |null               |null                  |\n",
      "|4         |null               |null                  |\n",
      "|5         |null               |null                  |\n",
      "+----------+-------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+---------------+-----------------+\n",
      "|CategoryID|ClosedCaptionID|CountryOfOriginID|\n",
      "+----------+---------------+-----------------+\n",
      "|13        |3              |3                |\n",
      "|null      |1              |null             |\n",
      "|null      |1              |null             |\n",
      "|null      |1              |null             |\n",
      "|null      |1              |null             |\n",
      "+----------+---------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------------+---------------+------------------+\n",
      "|DubDramaCreditID|EthnicProgramID|ProductionSourceID|\n",
      "+----------------+---------------+------------------+\n",
      "|null            |null           |10                |\n",
      "|null            |null           |null              |\n",
      "|null            |null           |null              |\n",
      "|null            |null           |null              |\n",
      "|null            |null           |null              |\n",
      "+----------------+---------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------+--------------------+------------+\n",
      "|ProgramClassID|FilmClassificationID|ExhibitionID|\n",
      "+--------------+--------------------+------------+\n",
      "|19            |null                |2           |\n",
      "|20            |null                |null        |\n",
      "|3             |null                |null        |\n",
      "|3             |null                |null        |\n",
      "|3             |null                |null        |\n",
      "+--------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------------+----------------+------------+\n",
      "|Duration        |EndTime         |LogEntryDate|\n",
      "+----------------+----------------+------------+\n",
      "|02:00:00.0000000|08:00:00.0000000|2018-08-01  |\n",
      "|00:00:30.0000000|06:13:45.0000000|2018-08-01  |\n",
      "|00:00:15.0000000|06:14:00.0000000|2018-08-01  |\n",
      "|00:00:15.0000000|06:14:15.0000000|2018-08-01  |\n",
      "|00:00:15.0000000|06:14:30.0000000|2018-08-01  |\n",
      "+----------------+----------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------+-------------------------------------------+----------------+\n",
      "|ProductionNO|ProgramTitle                               |StartTime       |\n",
      "+------------+-------------------------------------------+----------------+\n",
      "|A39082      |Newlywed and Dead                          |06:00:00.0000000|\n",
      "|null        |15-SPECIALTY CHANNELS-Canadian Generic     |06:13:15.0000000|\n",
      "|null        |3-PROCTER & GAMBLE INC-Anti-Perspirant 3rd |06:13:45.0000000|\n",
      "|null        |12-CREDIT KARMA-Bank/Credit Union/Trust 3rd|06:14:00.0000000|\n",
      "|null        |3-L'OREAL CANADA-Hair Products 3rd         |06:14:15.0000000|\n",
      "+------------+-------------------------------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+--------------------+------------------+\n",
      "|Subtitle|NetworkAffiliationID|SpecialAttentionID|\n",
      "+--------+--------------------+------------------+\n",
      "|null    |null                |null              |\n",
      "|null    |null                |null              |\n",
      "|null    |null                |null              |\n",
      "|null    |null                |null              |\n",
      "|null    |null                |null              |\n",
      "+--------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------------------+-------------+---------+\n",
      "|BroadcastOriginPointID|CompositionID|Producer1|\n",
      "+----------------------+-------------+---------+\n",
      "|null                  |null         |null     |\n",
      "|null                  |null         |null     |\n",
      "|null                  |null         |null     |\n",
      "|null                  |null         |null     |\n",
      "|null                  |null         |null     |\n",
      "+----------------------+-------------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+---------+---------+\n",
      "|Producer2|Language1|Language2|\n",
      "+---------+---------+---------+\n",
      "|null     |94       |null     |\n",
      "|null     |null     |null     |\n",
      "|null     |null     |null     |\n",
      "|null     |null     |null     |\n",
      "|null     |null     |null     |\n",
      "+---------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting columns in groups of three using numpy\n",
    "display(\"Columns in groups of three\")\n",
    "column_split = np.array_split(np.array(logs.columns), len(logs.columns) // 3)\n",
    "display(column_split)\n",
    "\n",
    "# Show columns in groups of three\n",
    "display(\"Table display in column groups of three\")\n",
    "for x in column_split:\n",
    "    logs.select(*x).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `drop`-ing columns we don't need\n",
    "\n",
    "Remove `BroadCastLogID` (primary key not needed in single table) and `SequenceNo`.  `drop()` returns a new data frame.\n",
    "\n",
    "> __Warning with `drop`__: Unlike `select()`, where selecting a column that doesn’t exist will return a runtime error, dropping a non-existent column is a no-op. PySpark will __just ignore the columns it doesn’t find__. Careful with the spelling of your column names!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = logs.drop(\"BroadCastLogID\", \"SequenceNo\")\n",
    "\n",
    "assert all(col not in logs.columns for col in [\"BroadCastLogID\", \"SequenceNo\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternate method of above just using `select` using list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = logs.select(\n",
    "    *[col for col in logs.columns if col not in [\"BroadCastLogID\", \"SequenceNo\"]]\n",
    ")\n",
    "\n",
    "assert all(col not in logs.columns for col in [\"BroadCastLogID\", \"SequenceNo\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### 4.3\n",
    "\n",
    "Create a new data frame logs_clean that contains only the columns that do not end with ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LogDate', 'SequenceNO', 'Duration', 'EndTime', 'LogEntryDate', 'ProductionNO', 'ProgramTitle', 'StartTime', 'Subtitle', 'Producer1', 'Producer2', 'Language1', 'Language2']\n"
     ]
    }
   ],
   "source": [
    "print([col for col in logs.columns if col[-2:] != \"ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered results (not end with 'ID')\n",
      "root\n",
      " |-- LogDate: string (nullable = true)\n",
      " |-- SequenceNO: integer (nullable = true)\n",
      " |-- Duration: string (nullable = true)\n",
      " |-- EndTime: string (nullable = true)\n",
      " |-- LogEntryDate: string (nullable = true)\n",
      " |-- ProductionNO: string (nullable = true)\n",
      " |-- ProgramTitle: string (nullable = true)\n",
      " |-- StartTime: string (nullable = true)\n",
      " |-- Subtitle: string (nullable = true)\n",
      " |-- Producer1: string (nullable = true)\n",
      " |-- Producer2: string (nullable = true)\n",
      " |-- Language1: integer (nullable = true)\n",
      " |-- Language2: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load original CSV again\n",
    "DIRECTORY = \"./data/Ch04\"\n",
    "logs = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"BroadcastLogs_2018_Q3_M8_sample.CSV\"),\n",
    "    sep=\"|\",  # default is \",\"\n",
    "    quote='\"',  # default is double quote.\n",
    "    header=True,  # set first row as column names\n",
    "    inferSchema=True,  # infer schema from column names default False\n",
    ")\n",
    "\n",
    "# Filter to columns that don't end with \"ID\"\n",
    "logs_no_id = logs.select(*[col for col in logs.columns if col[-2:].lower() != \"id\"])\n",
    "print(\"Filtered results (not end with 'ID')\")\n",
    "logs_no_id.printSchema()\n",
    "\n",
    "assert all(\"id\" not in col[-2:] for col in logs_no_id.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating new columns with `withColumn`\n",
    "\n",
    "### 1. Check the data type of 'Duration' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|        Duration|\n",
      "+----------------+\n",
      "|02:00:00.0000000|\n",
      "|00:00:30.0000000|\n",
      "|00:00:15.0000000|\n",
      "|00:00:15.0000000|\n",
      "|00:00:15.0000000|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "dtype of 'Duration' column is 'string'. Best to convert to timestamp:\n",
      " [('Duration', 'string')]\n"
     ]
    }
   ],
   "source": [
    "logs.select(F.col(\"Duration\")).show(5)\n",
    "\n",
    "print(\n",
    "    \"dtype of 'Duration' column is 'string'. Best to convert to timestamp:\\n\",\n",
    "    logs.select(F.col(\"Duration\")).dtypes,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extract time features from Duration column only show distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+-------+-------+----------------+\n",
      "|        Duration|hours|minutes|seconds|duration_seconds|\n",
      "+----------------+-----+-------+-------+----------------+\n",
      "|00:00:19.0000000|    0|      0|     19|              19|\n",
      "|00:07:09.0000000|    0|      7|      9|             429|\n",
      "|00:53:26.0000000|    0|     53|     26|            3206|\n",
      "|00:30:43.0000000|    0|     30|     43|            1843|\n",
      "|00:02:41.0000000|    0|      2|     41|             161|\n",
      "+----------------+-----+-------+-------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs.select(\n",
    "    F.col(\"Duration\"),\n",
    "    F.col(\"Duration\").substr(1, 2).cast(\"int\").alias(\"hours\"),\n",
    "    F.col(\"Duration\").substr(4, 2).cast(\"int\").alias(\"minutes\"),\n",
    "    F.col(\"Duration\").substr(7, 2).cast(\"int\").alias(\"seconds\"),\n",
    "    # Add final column converting duration into total seconds\n",
    "    (\n",
    "        F.col(\"Duration\").substr(1, 2).cast(\"int\") * 60 * 60\n",
    "        + F.col(\"Duration\").substr(4, 2).cast(\"int\") * 60\n",
    "        + F.col(\"Duration\").substr(7, 2).cast(\"int\")\n",
    "    ).alias(\"duration_seconds\"),\n",
    ").distinct().show(\n",
    "    5\n",
    ")  # only show distinct entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Use `withColumn()` to add 'duration_seconds' to original data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = logs.withColumn(\n",
    "    \"duration_seconds\",\n",
    "    F.col(\"Duration\").substr(1, 2).cast(\"int\") * 60 * 60\n",
    "    + F.col(\"Duration\").substr(4, 2).cast(\"int\") * 60\n",
    "    + F.col(\"Duration\").substr(7, 2).cast(\"int\"),\n",
    ")\n",
    "\n",
    "assert \"duration_seconds\" in logs.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Warning__: If you’re creating a column withColumn() and give it a name that already exists in your data frame, PySpark will happily overwrite the column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch renaming with `toDF()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- broadcastlogid: integer (nullable = true)\n",
      " |-- logserviceid: integer (nullable = true)\n",
      " |-- logdate: string (nullable = true)\n",
      " |-- sequenceno: integer (nullable = true)\n",
      " |-- audiencetargetageid: integer (nullable = true)\n",
      " |-- audiencetargetethnicid: integer (nullable = true)\n",
      " |-- categoryid: integer (nullable = true)\n",
      " |-- closedcaptionid: integer (nullable = true)\n",
      " |-- countryoforiginid: integer (nullable = true)\n",
      " |-- dubdramacreditid: integer (nullable = true)\n",
      " |-- ethnicprogramid: integer (nullable = true)\n",
      " |-- productionsourceid: integer (nullable = true)\n",
      " |-- programclassid: integer (nullable = true)\n",
      " |-- filmclassificationid: integer (nullable = true)\n",
      " |-- exhibitionid: integer (nullable = true)\n",
      " |-- duration: string (nullable = true)\n",
      " |-- endtime: string (nullable = true)\n",
      " |-- logentrydate: string (nullable = true)\n",
      " |-- productionno: string (nullable = true)\n",
      " |-- programtitle: string (nullable = true)\n",
      " |-- starttime: string (nullable = true)\n",
      " |-- subtitle: string (nullable = true)\n",
      " |-- networkaffiliationid: integer (nullable = true)\n",
      " |-- specialattentionid: integer (nullable = true)\n",
      " |-- broadcastoriginpointid: integer (nullable = true)\n",
      " |-- compositionid: integer (nullable = true)\n",
      " |-- producer1: string (nullable = true)\n",
      " |-- producer2: string (nullable = true)\n",
      " |-- language1: integer (nullable = true)\n",
      " |-- language2: integer (nullable = true)\n",
      " |-- duration_seconds: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs.toDF(*[x.lower() for x in logs.columns]).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting column order with `sort`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- AudienceTargetAgeID: integer (nullable = true)\n",
      " |-- AudienceTargetEthnicID: integer (nullable = true)\n",
      " |-- BroadcastLogID: integer (nullable = true)\n",
      " |-- BroadcastOriginPointID: integer (nullable = true)\n",
      " |-- CategoryID: integer (nullable = true)\n",
      " |-- ClosedCaptionID: integer (nullable = true)\n",
      " |-- CompositionID: integer (nullable = true)\n",
      " |-- CountryOfOriginID: integer (nullable = true)\n",
      " |-- DubDramaCreditID: integer (nullable = true)\n",
      " |-- Duration: string (nullable = true)\n",
      " |-- EndTime: string (nullable = true)\n",
      " |-- EthnicProgramID: integer (nullable = true)\n",
      " |-- ExhibitionID: integer (nullable = true)\n",
      " |-- FilmClassificationID: integer (nullable = true)\n",
      " |-- Language1: integer (nullable = true)\n",
      " |-- Language2: integer (nullable = true)\n",
      " |-- LogDate: string (nullable = true)\n",
      " |-- LogEntryDate: string (nullable = true)\n",
      " |-- LogServiceID: integer (nullable = true)\n",
      " |-- NetworkAffiliationID: integer (nullable = true)\n",
      " |-- Producer1: string (nullable = true)\n",
      " |-- Producer2: string (nullable = true)\n",
      " |-- ProductionNO: string (nullable = true)\n",
      " |-- ProductionSourceID: integer (nullable = true)\n",
      " |-- ProgramClassID: integer (nullable = true)\n",
      " |-- ProgramTitle: string (nullable = true)\n",
      " |-- SequenceNO: integer (nullable = true)\n",
      " |-- SpecialAttentionID: integer (nullable = true)\n",
      " |-- StartTime: string (nullable = true)\n",
      " |-- Subtitle: string (nullable = true)\n",
      " |-- duration_seconds: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs.select(sorted(logs.columns)).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting high level summary of your dataframe with `describe` and `summary`\n",
    "- `describe` only works for numerical and string columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|      BroadcastLogID|\n",
      "+-------+--------------------+\n",
      "|  count|              238945|\n",
      "|   mean|1.2168651122760174E9|\n",
      "| stddev| 1.496913424143109E7|\n",
      "|    min|          1195788151|\n",
      "|    max|          1249431576|\n",
      "+-------+--------------------+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|      LogServiceID|\n",
      "+-------+------------------+\n",
      "|  count|            238945|\n",
      "|   mean| 3450.890284375065|\n",
      "| stddev|199.50673962555592|\n",
      "|    min|              3157|\n",
      "|    max|              3925|\n",
      "+-------+------------------+\n",
      "\n",
      "+-------+----------+\n",
      "|summary|   LogDate|\n",
      "+-------+----------+\n",
      "|  count|    238945|\n",
      "|   mean|      null|\n",
      "| stddev|      null|\n",
      "|    min|2018-08-01|\n",
      "|    max|2018-08-01|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show stats for the first three columns\n",
    "for i in logs.columns[:3]:\n",
    "    logs.describe(i).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `summary` shows extra stats like 25-50% and 75% percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|      BroadcastLogID|\n",
      "+-------+--------------------+\n",
      "|  count|              238945|\n",
      "|   mean|1.2168651122760174E9|\n",
      "| stddev| 1.496913424143109E7|\n",
      "|    min|          1195788151|\n",
      "|    25%|          1249431576|\n",
      "|    50%|          1213242718|\n",
      "|    75%|          1226220081|\n",
      "|    max|          1249431576|\n",
      "+-------+--------------------+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|      LogServiceID|\n",
      "+-------+------------------+\n",
      "|  count|            238945|\n",
      "|   mean| 3450.890284375065|\n",
      "| stddev|199.50673962555592|\n",
      "|    min|              3157|\n",
      "|    25%|              3287|\n",
      "|    50%|              3379|\n",
      "|    75%|              3627|\n",
      "|    max|              3925|\n",
      "+-------+------------------+\n",
      "\n",
      "+-------+----------+\n",
      "|summary|   LogDate|\n",
      "+-------+----------+\n",
      "|  count|    238945|\n",
      "|   mean|      null|\n",
      "| stddev|      null|\n",
      "|    min|2018-08-01|\n",
      "|    25%|      null|\n",
      "|    50%|      null|\n",
      "|    75%|      null|\n",
      "|    max|2018-08-01|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show stats for the first three columns\n",
    "for i in logs.columns[:3]:\n",
    "    logs.select(i).summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark]",
   "language": "python",
   "name": "conda-env-pyspark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
