{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-1\">Summary</a></span></li><li><span><a href=\"#Requirements\" data-toc-modified-id=\"Requirements-2\">Requirements</a></span></li><li><span><a href=\"#Column-transformation-using-Series-UDF\" data-toc-modified-id=\"Column-transformation-using-Series-UDF-3\">Column transformation using Series UDF</a></span><ul class=\"toc-item\"><li><span><a href=\"#Types-of-Series-UDF\" data-toc-modified-id=\"Types-of-Series-UDF-3.1\">Types of Series UDF</a></span></li><li><span><a href=\"#Dataset---Google-BigQuery\" data-toc-modified-id=\"Dataset---Google-BigQuery-3.2\">Dataset - Google BigQuery</a></span><ul class=\"toc-item\"><li><span><a href=\"#Steps-to-data-connection\" data-toc-modified-id=\"Steps-to-data-connection-3.2.1\">Steps to data connection</a></span></li><li><span><a href=\"#Installation-+-Configuration\" data-toc-modified-id=\"Installation-+-Configuration-3.2.2\">Installation + Configuration</a></span></li></ul></li><li><span><a href=\"#Errata\" data-toc-modified-id=\"Errata-3.3\">Errata</a></span></li><li><span><a href=\"#Read-data-locally\" data-toc-modified-id=\"Read-data-locally-3.4\">Read data locally</a></span></li></ul></li><li><span><a href=\"#Series-to-Series-UDF\" data-toc-modified-id=\"Series-to-Series-UDF-4\">Series to Series UDF</a></span><ul class=\"toc-item\"><li><span><a href=\"#Converting-Fahrenheit-to-Celsius-with-a-S-to-S-UDF\" data-toc-modified-id=\"Converting-Fahrenheit-to-Celsius-with-a-S-to-S-UDF-4.1\">Converting Fahrenheit to Celsius with a S-to-S UDF</a></span><ul class=\"toc-item\"><li><span><a href=\"#Errata\" data-toc-modified-id=\"Errata-4.1.1\">Errata</a></span></li></ul></li></ul></li><li><span><a href=\"#Iterator-of-Series-UDF\" data-toc-modified-id=\"Iterator-of-Series-UDF-5\">Iterator of Series UDF</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pandas UDF\n",
    "\n",
    "## Summary\n",
    "- Pandas UDFs allow you to take code that works on Pandas data frames and scale it to the Spark Data Frame structure. Efficient serialization between the two data structures is ensured by PyArrow.\n",
    "- We can group Pandas UDF into two main families, depending on the level of control we need over the batches. Series and Iterator of Series (and Iterator of data frame/mapInPandas) will batch efficiently with the user having no control over the batch composition.\n",
    "- If you need control over the content of each batch, you can use grouped data UDF with the split-apply-combing programming pattern. PySpark provides access to the values inside each batch of a GroupedData object either as Series (group aggregate UDF) of as data frame (group map UDF).\n",
    "\n",
    "## Requirements\n",
    "\n",
    "This chapter will use:\n",
    "1. pandas\n",
    "2. scikit-learn\n",
    "3. PyArrow\n",
    "\n",
    "The chapter assumes you are using PySpark 3.0 and above.\n",
    "\n",
    "## Column transformation using Series UDF\n",
    "\n",
    "### Types of Series UDF\n",
    "\n",
    "__Series to Series__ \n",
    "\n",
    "- Takes `Columns` objects, converts to Pandas `Series` and return `Series` object that gets promoted back to `Column` object.\n",
    "\n",
    "__Iterator of Series to Iterator of Series__ \n",
    "\n",
    "- `Column` is batched, then fed as a Iterator object.  \n",
    "- Takes single `Column`, returns single `Column`\n",
    "- Good when you need to initialize an expensive state\n",
    "\n",
    "__Iterator of multiples Series to Iterator of Series__\n",
    "\n",
    "- Takes multiple `Columns` as input but preserves iterator pattern.\n",
    "\n",
    "### Dataset - Google BigQuery\n",
    "\n",
    "We will use the National Oceanic and Atmospheric Administration (NOAA) Global Surface Summary of the Day (GSOD) dataset.\n",
    "\n",
    "#### Steps to data connection\n",
    "\n",
    "1. Install and configure the connector (if necessary), following the vendor’s documentation.\n",
    "2. Customize the SparkReader object to account for the new data source type.\n",
    "3. Read the data, authenticating as needed.\n",
    "\n",
    "#### Installation + Configuration\n",
    "\n",
    "After setting up Google Cloud Platform account, intiialize PySpark with the BigQuery connector enabled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errata \n",
    "The code below doesn't work due to a lot of issues with PyArrow compatability with Java 11.  I've skipped this part and just downloaded the dataset from the author's github.\n",
    "\n",
    "Reference:\n",
    "- https://stackoverflow.com/questions/62109276/errorjava-lang-unsupportedoperationexception-for-pyspark-pandas-udf-documenta\n",
    "- https://github.com/GoogleCloudDataproc/spark-bigquery-connector/issues/200\n",
    "- https://stackoverflow.com/questions/64960642/rewrite-udf-to-pandas-udf-with-arraytype-column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\n",
    "    \"spark.executor.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\"\n",
    ")\n",
    "conf.set(\"spark.driver.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\")\n",
    "conf.set(\n",
    "    \"spark.jars.packages\",\n",
    "    \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.19.1\",\n",
    ")\n",
    "\n",
    "# spark = (\n",
    "#     SparkSession.builder\n",
    "#     .config(\n",
    "#         \"spark.jars.packages\",\n",
    "#         \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.19.1\",\n",
    "#     )\n",
    "#     .config(\n",
    "#         \"spark.driver.extraJavaOptions\",\n",
    "#         \"-Dio.netty.tryReflectionSetAccessible=true\"\n",
    "#     )\n",
    "#     .config(\n",
    "#         \"spark.executor.extraJavaOptions\",\n",
    "#         \"-Dio.netty.tryReflectionSetAccessible=true\"\n",
    "#     )\n",
    "#     .getOrCreate()\n",
    "# )\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initializing, read the `stations` and `gsod` tables for 2010 to 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "def read_df_from_bq(year):\n",
    "    return (\n",
    "        spark.read.format(\"bigquery\").option(\n",
    "            \"table\", f\"bigquery-public-data.noaa_gsod.gsod{year}\"\n",
    "        )\n",
    "        .option(\"credentialsFile\", \"/Users/taichinakatani/dotfiles/keys/bq-key.json\")\n",
    "        .option(\"parentProject\", \"still-vim-244001\")\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "\n",
    "# Because gsod2020 has an additional date column that the previous years do not have,\n",
    "# unionByName will fill the values with null\n",
    "gsod = (\n",
    "    reduce(\n",
    "        lambda x, y: x.unionByName(y, allowMissingColumns=True),\n",
    "        [read_df_from_bq(year) for year in range(2020, 2021)],\n",
    "    )\n",
    "    .dropna(subset=[\"year\", \"mo\", \"da\", \"temp\"])\n",
    "    .where(F.col(\"temp\") != 9999.9)\n",
    "    .drop(\"date\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsod.select(F.col('year')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# Read from local parquet instead\n",
    "gsod = spark.read.load(\"data/gsod_noaa/gsod2018.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Series to Series UDF\n",
    "\n",
    "- Python UDFs work on one record at a time, while Scalar UDF work on one _Series_ at a time and is written through Pandas code.\n",
    "- Pandas has simpler data types than PySpark, so need to be careful to align the types.  `pandas_udf` helps with this.\n",
    "\n",
    "![](notes/img/pd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Fahrenheit to Celsius with a S-to-S UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Errata\n",
    "\n",
    "Using the `pandas_udf` decorator is killing the kernel for some reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# note the syntax \"pandas_udf\" and how it returns a pd.Series\n",
    "# @F.pandas_udf(T.DoubleType())\n",
    "def f_to_c(degrees: pd.Series) -> pd.Series:\n",
    "    \"\"\"Transforms Farhenheit to Celcius.\"\"\"\n",
    "    return (degrees - 32) * 5 / 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|temp|            temp_c|\n",
      "+----+------------------+\n",
      "|37.2|2.8888888888888906|\n",
      "|71.6|21.999999999999996|\n",
      "|53.5|11.944444444444445|\n",
      "|24.7|-4.055555555555555|\n",
      "|70.4|21.333333333333336|\n",
      "+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gsod = gsod.withColumn(\"temp_c\", f_to_c(F.col(\"temp\")))\n",
    "gsod.select(\"temp\", \"temp_c\").distinct().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterator of Series UDF\n",
    "\n",
    "- signature goes from `(pd.Series) → pd.Series` to `(Iterator[pd.Series]) → Iterator[pd.Series]`\n",
    "- Since we are working with an Iterator of Series, we are explicitly iterating over each batch one by one. PySpark will take care of distributing the work for us.\n",
    "- Uses `yield` than `return` so function returns an iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from typing import Iterator\n",
    "\n",
    "\n",
    "@F.pandas_udf(T.DoubleType())\n",
    "def f_to_c2(degrees: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    \"\"\"Transforms Farhenheit to Celcius.\"\"\"\n",
    "    sleep(5)\n",
    "    for batch in degrees:\n",
    "        yield (batch - 32) * 5 / 9\n",
    "\n",
    "\n",
    "gsod.select(\n",
    "    \"temp\", f_to_c2(F.col(\"temp\")).alias(\"temp_c\")\n",
    ").distinct().show(5)\n",
    "\n",
    "# +-----+-------------------+\n",
    "# | temp|             temp_c|\n",
    "# +-----+-------------------+\n",
    "# | 37.2| 2.8888888888888906|\n",
    "# | 85.9| 29.944444444444443|\n",
    "# | 53.5| 11.944444444444445|\n",
    "# | 71.6| 21.999999999999996|\n",
    "# |-27.6|-33.111111111111114|\n",
    "# +-----+-------------------+\n",
    "# only showing top 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
