{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1fda36b-1aad-4a4e-b9e8-f78aefca0be5",
   "metadata": {},
   "source": [
    "# Data Analysis with Python and PySpark\n",
    "\n",
    "Notes and code written while reading `Data Analysis with Python and PySpark` by Jonathan Rioux.\n",
    "\n",
    "[Link to book](https://www.manning.com/books/data-analysis-with-python-and-pyspark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c59dee5-2912-43da-a1c5-04afedc28bf7",
   "metadata": {},
   "source": [
    "# Data Analysis with Python and PySpark\n",
    "\n",
    "## Environment setup\n",
    "\n",
    "1. Install homebrew\n",
    "2. Install Java and Spark via  `brew install apache-spark`\n",
    "    - Save yourself a headache by running `sudo ln -sfn $(brew --prefix)/opt/openjdk@11/libexec/openjdk.jdk /Library/Java/JavaVirtualMachines/openjdk-11.jdk` after installation.  This makes sure system Java wrappers find the Java development kit (JDK) associated with this package.\n",
    "3. Create conda environment via `conda create -n pyspark python=3.8 pandas pyspark=3.0.0`\n",
    "4. Activate conda environment by `conda activate spark`\n",
    "5. Install jupyter notebook to your environment via `conda install -c conda-forge notebook`\n",
    "    - Add the environment to your notebook:\n",
    "    ```\n",
    "    conda install -c anaconda ipykernel\n",
    "    python -m ipykernel install --user --name=pyspark\n",
    "    ```\n",
    "\n",
    "## Chapter 1: Basics\n",
    "\n",
    "### Structure\n",
    "\n",
    "- Pypark is slower than native Scala code.  At its core, Pyspark code needs to be translated to Scala via JVM (Java Virtual Machine).  This causes a speed bottleneck. \n",
    "\n",
    "### Terminology\n",
    "\n",
    "- \"Workers\" are called _executors_ in Spark lingo.  They perform the actual work on the machines.  They perform the actual work on the machines.\n",
    "- The _master_ node manages these workers.\n",
    "- The _driver program_ is the task that the workers are going to run.\n",
    "- _Cluster managers_ is a program that plans the capacity it will allocate to the driver program.  Spark provides its own manager, but can use others like YARN, Mesos, Kubernetes, etc.\n",
    "- Any directions about capacity (machines and executors) are encoded in a _SparkContext_ object which represents the connection to our Spark cluster.\n",
    "- Master allocates data to _executors_, which are the processes that run computations and store data for the application.\n",
    "- Executors sit on _worker nodes_ which is the actual computer.\n",
    "- Executor = actual worker, worker node = workbench that the executor performs work on.\n",
    "\n",
    "Example of 4 workers working together to calculate and average of one column:\n",
    "\n",
    "![workers](notes/img/worker_example.png)\n",
    "\n",
    "#### Lazy vs Eager Evaluation\n",
    "\n",
    "Python R Java are eagerly evaluated.  Spark is lazily evaluated.\n",
    "\n",
    "Spark distinguishes between __transformations__ and __actions__.\n",
    "\n",
    "Transformations are:\n",
    "- Adding a column to a table\n",
    "- Performing aggregations\n",
    "- Computing stats\n",
    "- Training a ML model on data\n",
    "- Reading data\n",
    "\n",
    "Actions are:\n",
    "- Printing information to the screen (i.e. `show`)\n",
    "- Writing data to a hard drive or a cloud bucket (i.e. `write`).\n",
    "\n",
    "A Spark program will avoid performing any data work an action triggers the computation chain.  Before that the master will _cache_ your instructions.  Benefits are:\n",
    "1. Storing instructions in memory takes less space than storing intermediate data frames.\n",
    "2. Caching the tasks allow the master to optimize the work between the executors more efficiently.\n",
    "3. If one node fails during processing, Spark can recreate missing chunks of the data by referring to the cached instructions.  Simply put, it handles the data recovery part.\n",
    "\n",
    "### Pyspark program in a nutshell\n",
    "\n",
    "1. We first encode our instructions in Python code, forming a driver program.\n",
    "2. When submitting our program (or launching a PySpark shell), the cluster manager allocates resources for us to use. Those will stay constant for the duration of the program.\n",
    "3. The master ingests your code and translate it into Spark instructions. Those instructions are either transformations or actions.\n",
    "4. Once the master reaches an action, it optimizes the whole computation chain and splits the work between executors. Executors are processes performing the actual data work and they reside on machines labeled worked nodes.\n",
    "\n",
    "\n",
    "## Chapter 2: First Data Program in Pyspark\n",
    "\n",
    "Most data-driven application functions in the Extract-Transform-Load (ETL) pipeline:\n",
    "\n",
    "1. Ingest or read the data we wish to work with.\n",
    "2. Transform the data via a few simple instructions or a very complex machine learning model\n",
    "3. Export the resulting data, either into a file to be fed into an app or by summarizing our findings into a visualization.\n",
    "\n",
    "### `SparkSession` entry point\n",
    "\n",
    "- `SparkSession` provides an entry point to Spark.\n",
    "  - Wraps `SparkContext` and provides functionality for interacting with the data.\n",
    "- Can be used as a normal object imported from a library in Python.\n",
    "- `SparkSession` builder: builder pattern with set of methods to create a configurable object.\n",
    "\n",
    "Creating a `SparkSession` entry point from scratch\n",
    "```py\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Analyzing the vocabulary of Pride and Prejudice.\")\n",
    "         .getOrCreate())\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f6236c-14f8-416b-9ff1-9c2665f3bf38",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Chapter 2: Your first Pyspark application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9af448-390f-41f2-92c5-c37f54db94ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "Most data-driven application functions in the Extract-Transform-Load (ETL) pipeline:\n",
    "\n",
    "1. Ingest or read the data we wish to work with.\n",
    "2. Transform the data via a few simple instructions or a very complex machine learning model\n",
    "3. Export the resulting data, either into a file to be fed into an app or by summarizing our findings into a visualization.\n",
    "\n",
    "### `SparkSession` entry point\n",
    "\n",
    "- `SparkSession` provides an entry point to Spark.\n",
    "  - Wraps `SparkContext` and provides functionality for interacting with the data.\n",
    "- Can be used as a normal object imported from a library in Python.\n",
    "- `SparkSession` builder: builder pattern with set of methods to create a configurable object.\n",
    "\n",
    "#### Creating a `SparkSession` entry point from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f072c846",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      3\u001b[0m spark \u001b[38;5;241m=\u001b[39m (SparkSession\n\u001b[1;32m      4\u001b[0m          \u001b[38;5;241m.\u001b[39mbuilder\n\u001b[1;32m      5\u001b[0m          \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalyzing the vocabulary of Pride and Prejudice.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m          \u001b[38;5;241m.\u001b[39mgetOrCreate())\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Analyzing the vocabulary of Pride and Prejudice.\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a5b1be-f218-40eb-bcc0-11bc69d1e271",
   "metadata": {},
   "source": [
    "`sparkContext` can be invoked from the `SparkSession` object like below.  \n",
    "\n",
    "(Older code may present `sparkContext` as an `sc` variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43506485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://taichis-imac.lan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Analyzing the vocabulary of Pride and Prejudice.</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=Analyzing the vocabulary of Pride and Prejudice.>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf3d9bd-9225-441f-8855-e4a32f01032b",
   "metadata": {},
   "source": [
    "### Setting the log level\n",
    "\n",
    "- Spark defaults to `WARN`.\n",
    "- Can change via `spark.sparkContext.setLogLevel(KEYWORD)`\n",
    "\n",
    "#### Log level keywords\n",
    "\n",
    "<table border=\"1\" class=\"contenttable\" summary=\"log level keywords\" width=\"100%\"> \n",
    "  <colgroup class=\"calibre26\" span=\"1\"> \n",
    "   <col class=\"col_\" span=\"1\" width=\"50%\"> \n",
    "   <col class=\"col_\" span=\"1\" width=\"50%\"> \n",
    "  </colgroup> \n",
    "  <tbody>\n",
    "   <tr class=\"calibre19\"> \n",
    "       <td class=\"contenttable1\" colspan=\"1\" rowspan=\"1\" align=\"left\"><p>Keyword</p></td> \n",
    "       <td class=\"contenttable1\" colspan=\"1\"  align=\"left\" rowspan=\"1\"><p>Description</p></td> \n",
    "   </tr> \n",
    "   <tr class=\"calibre19\"> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p><code class=\"code\">OFF</code></p> </td> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p>No logging at all (not recommended).</p> </td> \n",
    "   </tr> \n",
    "   <tr class=\"calibre19\"> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p><code class=\"code\">FATAL</code></p> </td> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p>Only fatal errors. A fatal error will crash your Spark cluster.</p> </td> \n",
    "   </tr> \n",
    "   <tr class=\"calibre19\"> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p><code class=\"code\">ERROR</code></p> </td> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p>My personal favorite, will show <code class=\"code\">FATAL</code> as well as other useful (but recoverable) errors.</p> </td> \n",
    "   </tr> \n",
    "   <tr class=\"calibre19\"> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p><code class=\"code\">WARN</code></p> </td> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p>Add warnings (and there is quite a lot of them).</p> </td> \n",
    "   </tr> \n",
    "   <tr class=\"calibre19\"> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p><code class=\"code\">INFO</code></p> </td> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p>Will give you runtime information, such as repartitioning and data recovery (see chapter 1).</p> </td> \n",
    "   </tr> \n",
    "   <tr class=\"calibre19\"> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p><code class=\"code\">DEBUG</code></p> </td> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p>Will provide debug information on your jobs.</p> </td> \n",
    "   </tr> \n",
    "   <tr class=\"calibre19\"> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p><code class=\"code\">TRACE</code></p> </td> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p>Will trace your jobs (more verbose debug logs). Can be quite pedagogic, but very annoying.</p> </td> \n",
    "   </tr> \n",
    "   <tr class=\"calibre19\"> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p><code class=\"code\">ALL</code></p> </td> \n",
    "    <td class=\"contenttable2\" colspan=\"1\" rowspan=\"1\"> <p>Everything that PySpark can spit, it will spit. As useful as <code class=\"code\">OFF</code>.</p> </td> \n",
    "   </tr> \n",
    "  </tbody>\n",
    " </table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3155d7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723f4754-dcb2-44ee-93a7-85afee37af03",
   "metadata": {},
   "source": [
    "## Application Design\n",
    "\n",
    "__Goal: What are the most popular words in the Jane Austen's _Pride and Prejudice_?__\n",
    "\n",
    "Steps:\n",
    "1. Read: Read the input data (we’re assuming a plain text file)\n",
    "2. Tokenize: Tokenize each word\n",
    "3. Clean: Remove any punctuation and/or tokens that aren’t words.\n",
    "4. Count: Count the frequency of each word present in the text\n",
    "5. Answer: Return the top 10 (or 20, 50, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4858f87-a784-4ed1-918f-d8237ceacf25",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "PySpark provide two main structures for storing data when performing manipulations:\n",
    "\n",
    "1. The Resilient Distributed Dataset (or RDD)\n",
    "2. The data frame; Stricter version of RDD. Makes heavy use of the concept of _columns_ where you perform ops on columns instead of on records (like in RDD).  \n",
    "  - More common than RDD.\n",
    "  - Syntax is similar to SQL\n",
    "\n",
    "#### RDD vs Dataframe\n",
    "<img src=\"notes/img/rdd_df.png\">\n",
    "\n",
    "#### Reading a dataframe with `spark.read`\n",
    "Reading data into a data frame is done through the DataFrameReader object, which we can access through `spark.read`. \n",
    "\n",
    "`value: string` is the column, with text within that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfdb7487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book = spark.read.text(\"data/Ch02/1342-0.txt\")\n",
    "book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39186739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('value', 'string')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check schema\n",
    "display(book.printSchema())\n",
    "\n",
    "display(book.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5378b6-c646-4c7c-9414-48e7468de72e",
   "metadata": {},
   "source": [
    "#### Showing a dataframe with `spark.show()`\n",
    "\n",
    "The show() method takes three optional parameters.\n",
    "\n",
    "1. `n` can be set to any positive integer, and will display that number of rows.\n",
    "2. `truncate`, if set to true, will truncate the columns to display only 20 characters. Set to False to display the whole length, or any positive integer to truncate to a specific number of characters.\n",
    "3. `vertical` takes a Boolean value and, when set to True, will display each record as a small table. If you need to check some records in detail, this is a very useful option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46ee4223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------------------------------\n",
      " value | The Project Gutenberg EBook of Pride and Prejudice, by Jane Austen \n",
      "-RECORD 1-------------------------------------------------------------------\n",
      " value |                                                                    \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# play with params\n",
    "book.show(2, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1674f90a-1a35-4847-a528-e0907dcbbbf4",
   "metadata": {},
   "source": [
    "#### Lazy vs Eager Evaluation\n",
    "\n",
    "- Default, you need to pass `show()` to see dataframe content.  This follow's Spark's idea of lazy evaluation until some action is needed.\n",
    "- Since Spark 2.4.0, you can configure the SparkSession object to support printing to screen. This may be helpful when learning:\n",
    "\n",
    "```py\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "                     .config(\"spark.sql.repl.eagerEval.enabled\", \"True\")\n",
    "                     .getOrCreate())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4087ece9-1df2-4784-bb3e-88722455681d",
   "metadata": {},
   "source": [
    "### Tokenizing sentences with `select()` and `split()`\n",
    "\n",
    "`select()` selects the data. Similar to SQL. Syntax is similar to pandas:\n",
    "\n",
    "```py\n",
    "book.select(book.value)\n",
    "book.select(book[\"value\"])\n",
    "book.select(col(\"value\"))\n",
    "book.select(\"value\")\n",
    "```\n",
    "\n",
    "`split()` transforms string column into an array column, containing `n` string elements (i.e. tokens).  Note that it uses `JVM`-based regex instead of Python.\n",
    "\n",
    "`alias()` renames transformed columns for easier reference.  When applied to a column, it takes a single string as an argument.\n",
    "\n",
    "Another way to alias set an alias is calling `.withColumnRenamed()` on the data frame.  If you just want to rename a column without changing the rest of the data frame, use .withColumnRenamed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bebc893d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[line: array<string>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- line: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+--------------------+\n",
      "|                line|\n",
      "+--------------------+\n",
      "|[The, Project, Gu...|\n",
      "|                  []|\n",
      "|[This, eBook, is,...|\n",
      "|[almost, no, rest...|\n",
      "|[re-use, it, unde...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, split\n",
    "\n",
    "# Read, tokenize and alias the column\n",
    "lines = book.select(split(col('value'), \" \").alias(\"line\"))\n",
    "\n",
    "display(lines)\n",
    "\n",
    "lines.printSchema()\n",
    "\n",
    "lines.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "16ed69b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- here is an alternate alias: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Changing alias name using withColumnRenamed\n",
    "alternative = lines.withColumnRenamed(\"line\", \n",
    "                                      \"here is an alternate alias\")\n",
    "alternative.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390d5c23-709e-47f3-9e68-eac2a178c07c",
   "metadata": {},
   "source": [
    "### Reshaping data with `explode()`\n",
    "\n",
    "When applied to a column containing a container-like data structure (such as an array), `explode()` will take each element and give it its own row.\n",
    "\n",
    "![img](notes/img/explode.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b34cbb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      word|\n",
      "+----------+\n",
      "|       The|\n",
      "|   Project|\n",
      "| Gutenberg|\n",
      "|     EBook|\n",
      "|        of|\n",
      "|     Pride|\n",
      "|       and|\n",
      "|Prejudice,|\n",
      "|        by|\n",
      "|      Jane|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explode column of arrays into rows of elements\n",
    "\n",
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    "words.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797d4835-a8cc-478e-9b16-fe32d089f9df",
   "metadata": {},
   "source": [
    "### String normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6c28d2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|word_lower|\n",
      "+----------+\n",
      "|       the|\n",
      "|   project|\n",
      "| gutenberg|\n",
      "|     ebook|\n",
      "|        of|\n",
      "|     pride|\n",
      "|       and|\n",
      "|prejudice,|\n",
      "|        by|\n",
      "|      jane|\n",
      "|    austen|\n",
      "|          |\n",
      "|      this|\n",
      "|     ebook|\n",
      "|        is|\n",
      "|       for|\n",
      "|       the|\n",
      "|       use|\n",
      "|        of|\n",
      "|    anyone|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------------+\n",
      "|word_normalized|\n",
      "+---------------+\n",
      "|            the|\n",
      "|        project|\n",
      "|      gutenberg|\n",
      "|          ebook|\n",
      "|             of|\n",
      "|          pride|\n",
      "|            and|\n",
      "|      prejudice|\n",
      "|             by|\n",
      "|           jane|\n",
      "|         austen|\n",
      "|               |\n",
      "|           this|\n",
      "|          ebook|\n",
      "|             is|\n",
      "|            for|\n",
      "|            the|\n",
      "|            use|\n",
      "|             of|\n",
      "|         anyone|\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower, regexp_extract\n",
    "\n",
    "# Lowercase\n",
    "words_lower = words.select(lower(\"word\").alias(\"word_lower\"))\n",
    "words_lower.show()\n",
    "\n",
    "# Naive punctuation normalization using regex\n",
    "word_norm = words_lower.select(regexp_extract(col(\"word_lower\"), \"[a-z]*\", 0).alias(\"word_normalized\"))\n",
    "word_norm.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f331e2-9896-4292-9a96-00d4fcd22267",
   "metadata": {},
   "source": [
    "### Filtering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "79ac3f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|word_nonull|\n",
      "+-----------+\n",
      "|        the|\n",
      "|    project|\n",
      "|  gutenberg|\n",
      "|      ebook|\n",
      "|         of|\n",
      "|      pride|\n",
      "|        and|\n",
      "|  prejudice|\n",
      "|         by|\n",
      "|       jane|\n",
      "|     austen|\n",
      "|       this|\n",
      "|      ebook|\n",
      "|         is|\n",
      "|        for|\n",
      "|        the|\n",
      "|        use|\n",
      "|         of|\n",
      "|     anyone|\n",
      "|   anywhere|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove empty records\n",
    "\n",
    "word_nonull = word_norm.filter(col(\"word_normalized\") != \"\") \\\n",
    "                       .withColumnRenamed('word_normalized', 'word_nonull')\n",
    "word_nonull.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fae867-330b-4053-98b3-e7f7bba48f96",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### 2.1\n",
    "Rewrite the following code snippet, removing the withColumnRenamed method. Which version is clearer and easier to read?\n",
    "\n",
    "```py\n",
    "from pyspark.sql.functions import col, length\n",
    "\n",
    "# The `length` function returns the number of characters in a string column.\n",
    "\n",
    "ex21 = (\n",
    "    spark.read.text(\"./data/Ch02/1342-0.txt\")\n",
    "    .select(length(col(\"value\")))\n",
    "    .withColumnRenamed(\"length(value)\", \"number_of_char\")\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "76cd3269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|values|\n",
      "+------+\n",
      "|    66|\n",
      "|     0|\n",
      "|    64|\n",
      "|    68|\n",
      "|    67|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, length\n",
    "ex21 = (\n",
    "    spark.read.text(\"./data/Ch02/1342-0.txt\")\n",
    "    .select(length(col(\"value\")).alias('values'))\n",
    ")\n",
    "ex21.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96bba75-7f86-473d-a1af-357211f9e0fd",
   "metadata": {},
   "source": [
    "### 2.2\n",
    "The following code blocks gives an error. What is the problem and how can you solve it?\n",
    "\n",
    "```py\n",
    "from pyspark.sql.functions import col, greatest\n",
    "\n",
    "ex22.printSchema()\n",
    "# root\n",
    "#  |-- key: string (containsNull = true)\n",
    "#  |-- value1: long (containsNull = true)\n",
    "#  |-- value2: long (containsNull = true)\n",
    "\n",
    "# `greatest` will return the greatest value of the list of column names,\n",
    "# skipping null value\n",
    "\n",
    "# The following statement will return an error\n",
    "ex22.select(\n",
    "    greatest(col(\"value1\"), col(\"value2\")).alias(\"maximum_value\")\n",
    ").select(\n",
    "    \"key\", \"max_value\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Answer\n",
    "\n",
    "The columns given are not in a list?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c36aaed-e08c-4835-816a-2808972566b9",
   "metadata": {},
   "source": [
    "### 2.3\n",
    "\n",
    "Let’s take our words_nonull data frame, available in listing 2.19. You can use the code in the repository (code/Ch02/end_of_chapter.py) into your REPL to get the data frame loaded.\n",
    "\n",
    "a) Remove all of the occurrences of the word \"is\"\n",
    "\n",
    "b) (Challenge) Using the length function explained in exercise 2.1, keep only the words with more than 3 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4cc748cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|words_greater_than_3|\n",
      "+--------------------+\n",
      "|             project|\n",
      "|           gutenberg|\n",
      "|               ebook|\n",
      "|               pride|\n",
      "|           prejudice|\n",
      "|                jane|\n",
      "|              austen|\n",
      "|                this|\n",
      "|               ebook|\n",
      "|              anyone|\n",
      "|            anywhere|\n",
      "|                cost|\n",
      "|                with|\n",
      "|              almost|\n",
      "|        restrictions|\n",
      "|          whatsoever|\n",
      "|                copy|\n",
      "|                give|\n",
      "|                away|\n",
      "|               under|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Remove all of the occurences of the word \"is\",\n",
    "# 2. Using the length function explained in exercise 2.1, keep only the words with more than 3 characters.\n",
    "word_nonull.filter(col(\"word_nonull\") != \"is\") \\\n",
    "           .filter(length(col(\"word_nonull\")) > 3) \\\n",
    "           .withColumnRenamed('word_nonull', 'words_greater_than_3') \\\n",
    "           .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57d55ab-9e8e-47a0-accd-609baff4a6dd",
   "metadata": {},
   "source": [
    "### 2.4\n",
    "\n",
    "Remove the words is, not, the and if from your list of words, using a single `where()` method on the words_nonull data frame (see exercise 2.3). Write the code to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5887d9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|word_nonull|\n",
      "+-----------+\n",
      "|    project|\n",
      "|  gutenberg|\n",
      "|      ebook|\n",
      "|         of|\n",
      "|      pride|\n",
      "|        and|\n",
      "|  prejudice|\n",
      "|         by|\n",
      "|       jane|\n",
      "|     austen|\n",
      "|       this|\n",
      "|      ebook|\n",
      "|        for|\n",
      "|        use|\n",
      "|         of|\n",
      "|     anyone|\n",
      "|   anywhere|\n",
      "|         at|\n",
      "|         no|\n",
      "|       cost|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_nonull.where(~col(\"word_nonull\").isin(['is', 'not', 'the', 'if'])) \\\n",
    "           .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5f40aa-7d0c-4fe1-8b6f-4bbf8d3a5a7e",
   "metadata": {},
   "source": [
    "### 2.5\n",
    "\n",
    "One of your friends come to you with the following code. They have no idea why it doesn’t work. Can you diagnose the problem, explain why it is an error and provide a fix?\n",
    "\n",
    "```py\n",
    "from pyspark.sql.functions import col, split\n",
    "\n",
    "book = spark.read.text(\"./data/ch02/1342-0.txt\")\n",
    "\n",
    "book = book.printSchema()\n",
    "\n",
    "lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
    "\n",
    "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    "```\n",
    "\n",
    "#### Answer\n",
    "\n",
    "They're assigning the output of `book.printSchema()` to `book`, hence writing over the spark data frame.\n",
    "\n",
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "aacc64b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+----------+\n",
      "|      word|\n",
      "+----------+\n",
      "|       The|\n",
      "|   Project|\n",
      "| Gutenberg|\n",
      "|     EBook|\n",
      "|        of|\n",
      "|     Pride|\n",
      "|       and|\n",
      "|Prejudice,|\n",
      "|        by|\n",
      "|      Jane|\n",
      "|    Austen|\n",
      "|          |\n",
      "|      This|\n",
      "|     eBook|\n",
      "|        is|\n",
      "|       for|\n",
      "|       the|\n",
      "|       use|\n",
      "|        of|\n",
      "|    anyone|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, split\n",
    "\n",
    "book = spark.read.text(\"./data/ch02/1342-0.txt\")\n",
    "\n",
    "# Don't assign it back to `book`\n",
    "book.printSchema()\n",
    "\n",
    "lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
    "\n",
    "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    "\n",
    "words.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95db7068",
   "metadata": {},
   "source": [
    "# Chapter 3: Submitting and scaling your first PySpark program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5a56523",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T02:17:07.776223Z",
     "start_time": "2021-05-13T02:16:59.806539Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"Analyzing the vocabulary of Pride and Prejudice.\"\n",
    ").getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08c5b660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Frame Setup\n",
    "# Set up\n",
    "from pyspark.sql.functions import col, split, lower, explode, regexp_extract\n",
    "\n",
    "book = spark.read.text(\"data/Ch02/1342-0.txt\")\n",
    "lines = book.select(split(col(\"value\"), \" \").alias(\"line\"))\n",
    "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    "words_lower = words.select(lower(\"word\").alias(\"word_lower\"))\n",
    "word_norm = words_lower.select(\n",
    "    regexp_extract(col(\"word_lower\"), \"[a-z]*\", 0).alias(\"word_normalized\")\n",
    ")\n",
    "word_nonull = word_norm.filter(col(\"word_normalized\") != \"\").withColumnRenamed(\n",
    "    \"word_normalized\", \"word_nonull\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efc2a5a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Aggregation: `groupBy` and `count`\n",
    "\n",
    "- `GroupedData` allows you to perform an aggregate function on each group. \n",
    "- Use `groupby` to count record occurrence, passing columns we want to group.  Returned value is a `GroupedData` object, not a `DataFrame`.  Once you apply a function to it like `count()`, it returns a  `DataFrame`.\n",
    "    - Note that `groupby` and `groupBy` are the same thing.\n",
    "- You can sort the output by `orderBy`\n",
    "    - Note that `orderBy` only exists as camel case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bb3da6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T02:22:17.223409Z",
     "start_time": "2021-05-13T02:22:16.540819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x11b77b910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|word_nonull|count|\n",
      "+-----------+-----+\n",
      "|        the| 4480|\n",
      "|         to| 4218|\n",
      "|         of| 3711|\n",
      "|        and| 3504|\n",
      "|        her| 2199|\n",
      "|          a| 1982|\n",
      "|         in| 1909|\n",
      "|        was| 1838|\n",
      "|          i| 1750|\n",
      "|        she| 1668|\n",
      "|       that| 1487|\n",
      "|         it| 1482|\n",
      "|        not| 1427|\n",
      "|        you| 1301|\n",
      "|         he| 1296|\n",
      "|         be| 1257|\n",
      "|        his| 1247|\n",
      "|         as| 1174|\n",
      "|        had| 1170|\n",
      "|       with| 1092|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "groups = word_nonull.groupBy(col(\"word_nonull\"))\n",
    "display(groups)\n",
    "\n",
    "results = groups.count().orderBy(\"count\", ascending=False)\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728a775f",
   "metadata": {},
   "source": [
    "## Writing to file: `csv`\n",
    "\n",
    "- data frame has `write` method, which can be chained with `csv`\n",
    "- default writes a bunch of separate files (1 file per partition) + `_SUCCESS` file.\n",
    "- use `coalesce` to concat to 1 file\n",
    "- use `.mode('overwrite')` to force write\n",
    "\n",
    "> __TIP:__\n",
    "Never assume that your data frame will keep the same ordering of records unless you explicitly ask via orderBy()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c52925f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write multiple partitions + success file\n",
    "results.write.mode(\"overwrite\").csv(\"./output/results\")\n",
    "\n",
    "# Concatenate into 1 file, then write to disk\n",
    "results.coalesce(1).write.mode(\"overwrite\").csv(\"./output/result_single_partition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238657bc",
   "metadata": {},
   "source": [
    "## Streamlining the code by chaining\n",
    "\n",
    "### Method chaining\n",
    "\n",
    "In PySpark, every transformation returns an object, which is why we need to assign a variable to the result.  This means that PySpark doesn’t perform modifications in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "268dc246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qualified import; import the whole module\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# chain methods together instead of multiple variables\n",
    "results = (\n",
    "    spark.read.text(\"./data/ch02/1342-0.txt\")\n",
    "    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
    "    .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
    "    .select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
    "    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\n",
    "    .where(F.col(\"word\") != \"\")\n",
    "    .groupby(\"word\")\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58c6c2d",
   "metadata": {},
   "source": [
    "## Submitting code in batch mode using `spark-submit`\n",
    "\n",
    "When wrapping a script to be executed with `spark-submit` ratherh than with the `pyspark` command, you'll need to define your `SparkSession` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd9fa7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the|38895|\n",
      "| and|23919|\n",
      "|  of|21199|\n",
      "|  to|20526|\n",
      "|   a|14464|\n",
      "|   i|13973|\n",
      "|  in|12777|\n",
      "|that| 9623|\n",
      "|  it| 9099|\n",
      "| was| 8920|\n",
      "| her| 7923|\n",
      "|  my| 7385|\n",
      "| his| 6642|\n",
      "|with| 6575|\n",
      "|  he| 6444|\n",
      "|  as| 6439|\n",
      "| you| 6295|\n",
      "| had| 5718|\n",
      "| she| 5617|\n",
      "| for| 5425|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This can be wrapped into a `word_counter.py` file and be executed\n",
    "# using `spark-submit`\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"Analyzing the vocabulary of Pride and Prejudice.\"\n",
    ").getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "results = (\n",
    "    spark.read.text(\"./data/ch02/*.txt\")\n",
    "    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
    "    .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
    "    .select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
    "    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\n",
    "    .where(F.col(\"word\") != \"\")\n",
    "    .groupby(\"word\")\n",
    "    .count()\n",
    "    .orderBy(\"count\", ascending=False)\n",
    ")\n",
    "\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac462b0",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "See [chapter 3 code](./code/Ch03/word_count_submit.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3b9afb",
   "metadata": {},
   "source": [
    "# Chapter 4: Analyzing tabular data with pyspark.sql\n",
    "\n",
    "## Summary\n",
    "\n",
    "- PySpark uses the `SparkReader` object to read any kind of data directly in a data frame. The specialized `CSV SparkReader` is used to ingest comma-separated value (CSV) files. Just like when reading text, the only mandatory parameter is the source location.\n",
    "- The CSV format is very versatile, so PySpark provides many optional parameters to account for this flexibility. The most important ones are the `field` delimiter, the `record` delimiter, and the `quotation` character. All of those parameters have sensible defaults.\n",
    "- PySpark can infer the Schema of a CSV file by setting the `inferSchema` optional parameter to True. PySpark accomplishes this by reading the data twice: once for setting the appropriate types for each columns, and another time to ingest the data in the inferred format.\n",
    "- Tabular data is represented into a data frame in a series of Columns, each having a name and a type. Since the data frame is a column-major data structure, the concept of row is less relevant.\n",
    "- You can use Python code to explore the data efficiently, using the column list as any Python list to expose the elements of the data frame of interest.\n",
    "- The most common operations on a data frame are the selection, deletion, and creation or columns. In PySpark, the methods used are `select()`, `drop()` and `withColumn()`, respectively.\n",
    "- select can be used for column re-ordering by passing a re-ordered list of columns.\n",
    "- You can rename columns one by one with the `withColumnRenamed()` method, or all at once by using the `toDF()` method.\n",
    "- You can display a summary of the columns with the `describe()` or `summary()` method. `describe()` has a fixed set of metrics, while `summary()` will take functions as parameters and apply them to all columns.\n",
    "\n",
    "## On dataframes\n",
    "> PySpark operates either on the whole __data frame__ objects (via methods such as `select()` and `groupby()`) or on __Column__ objects (for instance when using a function like `split()`). \n",
    ">\n",
    "> - The data frame is __column-major__, so its API focuses on manipulating the columns to transform the data. \n",
    "> - Hence with data transformations, think about what operations to do and which columns will be impacted.\n",
    "\n",
    "- RDDs on the other hand are _row-major_.  Hence you're thinking about items with attributes in which you apply functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14c2d372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70efeca8",
   "metadata": {},
   "source": [
    "## Data Source Info\n",
    "\n",
    "> For this exercise, we’ll use some open data from the Government of Canada, more specifically the CRTC (Canadian Radio-television and Telecommunications Commission). Every broadcaster is mandated to provide a complete log of the programs, commercials and all, showcased to the Canadian public. \n",
    ">\n",
    "> This gives us a lot of potential questions to answer, but we’ll select one specific one: __what are the channels with the most and least proportion of commercials?__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c0c6b0",
   "metadata": {},
   "source": [
    "## Creating a data frame\n",
    "\n",
    "`spark.createDataFrame`\n",
    "- 1st param: data (list of lists, pandas dataframe, RDD)\n",
    "- 2nd param: schema (ie. think column headers in SQL)\n",
    "- Master node knows the structure of the dataframe, but actual data is on worker nodes (ie. cluster memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5693c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Item: string (nullable = true)\n",
      " |-- Quantity: long (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example creating a data frame with toy data\n",
    "my_grocery_list = [\n",
    "    [\"Banana\", 2, 1.74],\n",
    "    [\"Apple\", 4, 2.04],\n",
    "    [\"Carrot\", 1, 1.09],\n",
    "    [\"Cake\", 1, 10.99],\n",
    "]\n",
    "\n",
    "df_grocery_list = spark.createDataFrame(my_grocery_list, [\"Item\", \"Quantity\", \"Price\"])\n",
    "\n",
    "df_grocery_list.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc8b1bf",
   "metadata": {},
   "source": [
    "## Reading a data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9e058e",
   "metadata": {},
   "source": [
    "### Data frame structure\n",
    "\n",
    "Composed of _row delimiter_ (e.g. newline `\\n`) and _column delimiter_ (e.g. tabs `\\t` for TSVs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c02f8f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = \"./data/Ch04\"\n",
    "logs = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"BroadcastLogs_2018_Q3_M8_sample.CSV\"),\n",
    "    sep=\"|\",  # default is \",\"\n",
    "    quote='\"',  # default is double quote.\n",
    "    header=True,  # set first row as column names\n",
    "    inferSchema=True,  # infer schema from column names default False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e873d5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- BroadcastLogID: integer (nullable = true)\n",
      " |-- LogServiceID: integer (nullable = true)\n",
      " |-- LogDate: string (nullable = true)\n",
      " |-- SequenceNO: integer (nullable = true)\n",
      " |-- AudienceTargetAgeID: integer (nullable = true)\n",
      " |-- AudienceTargetEthnicID: integer (nullable = true)\n",
      " |-- CategoryID: integer (nullable = true)\n",
      " |-- ClosedCaptionID: integer (nullable = true)\n",
      " |-- CountryOfOriginID: integer (nullable = true)\n",
      " |-- DubDramaCreditID: integer (nullable = true)\n",
      " |-- EthnicProgramID: integer (nullable = true)\n",
      " |-- ProductionSourceID: integer (nullable = true)\n",
      " |-- ProgramClassID: integer (nullable = true)\n",
      " |-- FilmClassificationID: integer (nullable = true)\n",
      " |-- ExhibitionID: integer (nullable = true)\n",
      " |-- Duration: string (nullable = true)\n",
      " |-- EndTime: string (nullable = true)\n",
      " |-- LogEntryDate: string (nullable = true)\n",
      " |-- ProductionNO: string (nullable = true)\n",
      " |-- ProgramTitle: string (nullable = true)\n",
      " |-- StartTime: string (nullable = true)\n",
      " |-- Subtitle: string (nullable = true)\n",
      " |-- NetworkAffiliationID: integer (nullable = true)\n",
      " |-- SpecialAttentionID: integer (nullable = true)\n",
      " |-- BroadcastOriginPointID: integer (nullable = true)\n",
      " |-- CompositionID: integer (nullable = true)\n",
      " |-- Producer1: string (nullable = true)\n",
      " |-- Producer2: string (nullable = true)\n",
      " |-- Language1: integer (nullable = true)\n",
      " |-- Language2: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a897d14",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "##  Exercises\n",
    "\n",
    "### 4.1\n",
    "Take the following file, called sample.csv, and read it into a dataframe.\n",
    "\n",
    "```\n",
    "Item,Quantity,Price\n",
    "$Banana, organic$,1,0.99\n",
    "Pear,7,1.24\n",
    "$Cake, chocolate$,1,14.50\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99a510a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+-----+\n",
      "|           Item|Quantity|Price|\n",
      "+---------------+--------+-----+\n",
      "|Banana, organic|       1| 0.99|\n",
      "|           Pear|       7| 1.24|\n",
      "|Cake, chocolate|       1| 14.5|\n",
      "+---------------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"ch4_exercise.csv\"),\n",
    "    sep=\",\",\n",
    "    header=True,\n",
    "    quote=\"$\",\n",
    "    inferSchema=True,\n",
    ")\n",
    "\n",
    "sample.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14367303",
   "metadata": {},
   "source": [
    "### 4.2\n",
    "\n",
    "Re-read the data in a `logs_raw` data frame, taking inspiration from the code in listing 4.3, this time without passing any optional parameters. Print the first 5 rows of data, as well as the schema. What are the differences in terms of data and schema between logs and logs_raw?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4d12541d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|_c0                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|BroadcastLogID|LogServiceID|LogDate|SequenceNO|AudienceTargetAgeID|AudienceTargetEthnicID|CategoryID|ClosedCaptionID|CountryOfOriginID|DubDramaCreditID|EthnicProgramID|ProductionSourceID|ProgramClassID|FilmClassificationID|ExhibitionID|Duration|EndTime|LogEntryDate|ProductionNO|ProgramTitle|StartTime|Subtitle|NetworkAffiliationID|SpecialAttentionID|BroadcastOriginPointID|CompositionID|Producer1|Producer2|Language1|Language2|\n",
      "|1196192316|3157|2018-08-01|1|4||13|3|3|||10|19||2|02:00:00.0000000|08:00:00.0000000|2018-08-01|A39082|Newlywed and Dead|06:00:00.0000000||||||||94|                                                                                                                                                                                                                                                                                        |\n",
      "|1196192317|3157|2018-08-01|2||||1|||||20|||00:00:30.0000000|06:13:45.0000000|2018-08-01||15-SPECIALTY CHANNELS-Canadian Generic|06:13:15.0000000|||||||||                                                                                                                                                                                                                                                                                  |\n",
      "|1196192318|3157|2018-08-01|3||||1|||||3|||00:00:15.0000000|06:14:00.0000000|2018-08-01||3-PROCTER & GAMBLE INC-Anti-Perspirant 3rd|06:13:45.0000000|||||||||                                                                                                                                                                                                                                                                               |\n",
      "|1196192319|3157|2018-08-01|4||||1|||||3|||00:00:15.0000000|06:14:15.0000000|2018-08-01||12-CREDIT KARMA-Bank/Credit Union/Trust 3rd|06:14:00.0000000|||||||||                                                                                                                                                                                                                                                                              |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DIRECTORY = \"./data/Ch04\"\n",
    "raw_logs = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"BroadcastLogs_2018_Q3_M8_sample.CSV\"),\n",
    ")\n",
    "raw_logs.show(5, False)  # False = show entire contents\n",
    "raw_logs.printSchema()\n",
    "\n",
    "# Result shows entire row concatenated into one column (_c0). Not what we want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92526f2d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exploring the shape of our data universe\n",
    "\n",
    "### About Star Schema\n",
    "\n",
    "\n",
    "Wiki:\n",
    "> In computing, the __star schema__ is the simplest style of data mart schema and is the approach most widely used to develop data warehouses and dimensional data marts. The star schema consists of one or more fact tables referencing any number of dimension tables.\n",
    "\n",
    "Star schemas are common in the relational database world because of __normalization__, a process used to avoid duplicating pieces of data and improve data integrity.\n",
    "\n",
    "Spark uses __denormalized__ tables (ie __fat__ tables). Why? Mainly because it is easier to run analyses on a single table.  \n",
    "  - If you do need to analyze complex star schema, best bet is to work with a database manger to get a denormalized table.\n",
    "  \n",
    "### `select`-ing what we want to see\n",
    "\n",
    "Four ways to `select` colums in PySpark, all equivalent in term of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "afaf6793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[BroadCastLogID: int, LogServiceID: int, LogDate: string]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the string to column conversion\n",
    "logs.select(\"BroadCastLogID\", \"LogServiceID\", \"LogDate\")\n",
    "logs.select(\n",
    "    *[\"BroadCastLogID\", \"LogServiceID\", \"LogDate\"]\n",
    ")  # Unpack list with star prefix\n",
    "\n",
    "# Passing the column object explicitly\n",
    "logs.select(F.col(\"BroadCastLogID\"), F.col(\"LogServiceID\"), F.col(\"LogDate\"))\n",
    "logs.select(\n",
    "    *[F.col(\"BroadCastLogID\"), F.col(\"LogServiceID\"), F.col(\"LogDate\")]\n",
    ")  # Unpack list with star prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b77192",
   "metadata": {},
   "source": [
    "Because of the width of our data frame, we could split our columns into manageable sets of three to keep the output tidy on the screen. This gives a high-level view of what the data frame contains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fe0d2b1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Columns in groups of three'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[array(['BroadcastLogID', 'LogServiceID', 'LogDate'], dtype='<U22'),\n",
       " array(['SequenceNO', 'AudienceTargetAgeID', 'AudienceTargetEthnicID'],\n",
       "       dtype='<U22'),\n",
       " array(['CategoryID', 'ClosedCaptionID', 'CountryOfOriginID'], dtype='<U22'),\n",
       " array(['DubDramaCreditID', 'EthnicProgramID', 'ProductionSourceID'],\n",
       "       dtype='<U22'),\n",
       " array(['ProgramClassID', 'FilmClassificationID', 'ExhibitionID'],\n",
       "       dtype='<U22'),\n",
       " array(['Duration', 'EndTime', 'LogEntryDate'], dtype='<U22'),\n",
       " array(['ProductionNO', 'ProgramTitle', 'StartTime'], dtype='<U22'),\n",
       " array(['Subtitle', 'NetworkAffiliationID', 'SpecialAttentionID'],\n",
       "       dtype='<U22'),\n",
       " array(['BroadcastOriginPointID', 'CompositionID', 'Producer1'],\n",
       "       dtype='<U22'),\n",
       " array(['Producer2', 'Language1', 'Language2'], dtype='<U22')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Table display in column groups of three'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+----------+\n",
      "|BroadcastLogID|LogServiceID|LogDate   |\n",
      "+--------------+------------+----------+\n",
      "|1196192316    |3157        |2018-08-01|\n",
      "|1196192317    |3157        |2018-08-01|\n",
      "|1196192318    |3157        |2018-08-01|\n",
      "|1196192319    |3157        |2018-08-01|\n",
      "|1196192320    |3157        |2018-08-01|\n",
      "+--------------+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+-------------------+----------------------+\n",
      "|SequenceNO|AudienceTargetAgeID|AudienceTargetEthnicID|\n",
      "+----------+-------------------+----------------------+\n",
      "|1         |4                  |null                  |\n",
      "|2         |null               |null                  |\n",
      "|3         |null               |null                  |\n",
      "|4         |null               |null                  |\n",
      "|5         |null               |null                  |\n",
      "+----------+-------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+---------------+-----------------+\n",
      "|CategoryID|ClosedCaptionID|CountryOfOriginID|\n",
      "+----------+---------------+-----------------+\n",
      "|13        |3              |3                |\n",
      "|null      |1              |null             |\n",
      "|null      |1              |null             |\n",
      "|null      |1              |null             |\n",
      "|null      |1              |null             |\n",
      "+----------+---------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------------+---------------+------------------+\n",
      "|DubDramaCreditID|EthnicProgramID|ProductionSourceID|\n",
      "+----------------+---------------+------------------+\n",
      "|null            |null           |10                |\n",
      "|null            |null           |null              |\n",
      "|null            |null           |null              |\n",
      "|null            |null           |null              |\n",
      "|null            |null           |null              |\n",
      "+----------------+---------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------+--------------------+------------+\n",
      "|ProgramClassID|FilmClassificationID|ExhibitionID|\n",
      "+--------------+--------------------+------------+\n",
      "|19            |null                |2           |\n",
      "|20            |null                |null        |\n",
      "|3             |null                |null        |\n",
      "|3             |null                |null        |\n",
      "|3             |null                |null        |\n",
      "+--------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------------+----------------+------------+\n",
      "|Duration        |EndTime         |LogEntryDate|\n",
      "+----------------+----------------+------------+\n",
      "|02:00:00.0000000|08:00:00.0000000|2018-08-01  |\n",
      "|00:00:30.0000000|06:13:45.0000000|2018-08-01  |\n",
      "|00:00:15.0000000|06:14:00.0000000|2018-08-01  |\n",
      "|00:00:15.0000000|06:14:15.0000000|2018-08-01  |\n",
      "|00:00:15.0000000|06:14:30.0000000|2018-08-01  |\n",
      "+----------------+----------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------+-------------------------------------------+----------------+\n",
      "|ProductionNO|ProgramTitle                               |StartTime       |\n",
      "+------------+-------------------------------------------+----------------+\n",
      "|A39082      |Newlywed and Dead                          |06:00:00.0000000|\n",
      "|null        |15-SPECIALTY CHANNELS-Canadian Generic     |06:13:15.0000000|\n",
      "|null        |3-PROCTER & GAMBLE INC-Anti-Perspirant 3rd |06:13:45.0000000|\n",
      "|null        |12-CREDIT KARMA-Bank/Credit Union/Trust 3rd|06:14:00.0000000|\n",
      "|null        |3-L'OREAL CANADA-Hair Products 3rd         |06:14:15.0000000|\n",
      "+------------+-------------------------------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+--------------------+------------------+\n",
      "|Subtitle|NetworkAffiliationID|SpecialAttentionID|\n",
      "+--------+--------------------+------------------+\n",
      "|null    |null                |null              |\n",
      "|null    |null                |null              |\n",
      "|null    |null                |null              |\n",
      "|null    |null                |null              |\n",
      "|null    |null                |null              |\n",
      "+--------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------------------+-------------+---------+\n",
      "|BroadcastOriginPointID|CompositionID|Producer1|\n",
      "+----------------------+-------------+---------+\n",
      "|null                  |null         |null     |\n",
      "|null                  |null         |null     |\n",
      "|null                  |null         |null     |\n",
      "|null                  |null         |null     |\n",
      "|null                  |null         |null     |\n",
      "+----------------------+-------------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+---------+---------+\n",
      "|Producer2|Language1|Language2|\n",
      "+---------+---------+---------+\n",
      "|null     |94       |null     |\n",
      "|null     |null     |null     |\n",
      "|null     |null     |null     |\n",
      "|null     |null     |null     |\n",
      "|null     |null     |null     |\n",
      "+---------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting columns in groups of three using numpy\n",
    "display(\"Columns in groups of three\")\n",
    "column_split = np.array_split(np.array(logs.columns), len(logs.columns) // 3)\n",
    "display(column_split)\n",
    "\n",
    "# Show columns in groups of three\n",
    "display(\"Table display in column groups of three\")\n",
    "for x in column_split:\n",
    "    logs.select(*x).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94f6f4b",
   "metadata": {},
   "source": [
    "### `drop`-ing columns we don't need\n",
    "\n",
    "Remove `BroadCastLogID` (primary key not needed in single table) and `SequenceNo`.  `drop()` returns a new data frame.\n",
    "\n",
    "> __Warning with `drop`__: Unlike `select()`, where selecting a column that doesn’t exist will return a runtime error, dropping a non-existent column is a no-op. PySpark will __just ignore the columns it doesn’t find__. Careful with the spelling of your column names!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b863dd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = logs.drop(\"BroadCastLogID\", \"SequenceNo\")\n",
    "\n",
    "assert all(col not in logs.columns for col in [\"BroadCastLogID\", \"SequenceNo\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe7cfb5",
   "metadata": {},
   "source": [
    "Alternate method of above just using `select` using list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3ee828cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = logs.select(\n",
    "    *[col for col in logs.columns if col not in [\"BroadCastLogID\", \"SequenceNo\"]]\n",
    ")\n",
    "\n",
    "assert all(col not in logs.columns for col in [\"BroadCastLogID\", \"SequenceNo\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8a8a81",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### 4.3\n",
    "\n",
    "Create a new data frame logs_clean that contains only the columns that do not end with ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2f1da7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LogDate', 'SequenceNO', 'Duration', 'EndTime', 'LogEntryDate', 'ProductionNO', 'ProgramTitle', 'StartTime', 'Subtitle', 'Producer1', 'Producer2', 'Language1', 'Language2']\n"
     ]
    }
   ],
   "source": [
    "print([col for col in logs.columns if col[-2:] != \"ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d1d55eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered results (not end with 'ID')\n",
      "root\n",
      " |-- LogDate: string (nullable = true)\n",
      " |-- SequenceNO: integer (nullable = true)\n",
      " |-- Duration: string (nullable = true)\n",
      " |-- EndTime: string (nullable = true)\n",
      " |-- LogEntryDate: string (nullable = true)\n",
      " |-- ProductionNO: string (nullable = true)\n",
      " |-- ProgramTitle: string (nullable = true)\n",
      " |-- StartTime: string (nullable = true)\n",
      " |-- Subtitle: string (nullable = true)\n",
      " |-- Producer1: string (nullable = true)\n",
      " |-- Producer2: string (nullable = true)\n",
      " |-- Language1: integer (nullable = true)\n",
      " |-- Language2: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load original CSV again\n",
    "DIRECTORY = \"./data/Ch04\"\n",
    "logs = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"BroadcastLogs_2018_Q3_M8_sample.CSV\"),\n",
    "    sep=\"|\",  # default is \",\"\n",
    "    quote='\"',  # default is double quote.\n",
    "    header=True,  # set first row as column names\n",
    "    inferSchema=True,  # infer schema from column names default False\n",
    ")\n",
    "\n",
    "# Filter to columns that don't end with \"ID\"\n",
    "logs_no_id = logs.select(*[col for col in logs.columns if col[-2:].lower() != \"id\"])\n",
    "print(\"Filtered results (not end with 'ID')\")\n",
    "logs_no_id.printSchema()\n",
    "\n",
    "assert all(\"id\" not in col[-2:] for col in logs_no_id.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06af334f",
   "metadata": {},
   "source": [
    "## Creating new columns with `withColumn`\n",
    "\n",
    "### 1. Check the data type of 'Duration' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0519d4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|        Duration|\n",
      "+----------------+\n",
      "|02:00:00.0000000|\n",
      "|00:00:30.0000000|\n",
      "|00:00:15.0000000|\n",
      "|00:00:15.0000000|\n",
      "|00:00:15.0000000|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "dtype of 'Duration' column is 'string'. Best to convert to timestamp:\n",
      " [('Duration', 'string')]\n"
     ]
    }
   ],
   "source": [
    "logs.select(F.col(\"Duration\")).show(5)\n",
    "\n",
    "print(\n",
    "    \"dtype of 'Duration' column is 'string'. Best to convert to timestamp:\\n\",\n",
    "    logs.select(F.col(\"Duration\")).dtypes,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bbd96e",
   "metadata": {},
   "source": [
    "### 2. Extract time features from Duration column only show distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c47cdc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+-------+-------+----------------+\n",
      "|        Duration|hours|minutes|seconds|duration_seconds|\n",
      "+----------------+-----+-------+-------+----------------+\n",
      "|00:00:19.0000000|    0|      0|     19|              19|\n",
      "|00:07:09.0000000|    0|      7|      9|             429|\n",
      "|00:53:26.0000000|    0|     53|     26|            3206|\n",
      "|00:30:43.0000000|    0|     30|     43|            1843|\n",
      "|00:02:41.0000000|    0|      2|     41|             161|\n",
      "+----------------+-----+-------+-------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs.select(\n",
    "    F.col(\"Duration\"),\n",
    "    F.col(\"Duration\").substr(1, 2).cast(\"int\").alias(\"hours\"),\n",
    "    F.col(\"Duration\").substr(4, 2).cast(\"int\").alias(\"minutes\"),\n",
    "    F.col(\"Duration\").substr(7, 2).cast(\"int\").alias(\"seconds\"),\n",
    "    # Add final column converting duration into total seconds\n",
    "    (\n",
    "        F.col(\"Duration\").substr(1, 2).cast(\"int\") * 60 * 60\n",
    "        + F.col(\"Duration\").substr(4, 2).cast(\"int\") * 60\n",
    "        + F.col(\"Duration\").substr(7, 2).cast(\"int\")\n",
    "    ).alias(\"duration_seconds\"),\n",
    ").distinct().show(\n",
    "    5\n",
    ")  # only show distinct entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e55593d",
   "metadata": {},
   "source": [
    "### 3. Use `withColumn()` to add 'duration_seconds' to original data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3b560ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = logs.withColumn(\n",
    "    \"duration_seconds\",\n",
    "    F.col(\"Duration\").substr(1, 2).cast(\"int\") * 60 * 60\n",
    "    + F.col(\"Duration\").substr(4, 2).cast(\"int\") * 60\n",
    "    + F.col(\"Duration\").substr(7, 2).cast(\"int\"),\n",
    ")\n",
    "\n",
    "assert \"duration_seconds\" in logs.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee404944",
   "metadata": {},
   "source": [
    "> __Warning__: If you’re creating a column withColumn() and give it a name that already exists in your data frame, PySpark will happily overwrite the column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2634de",
   "metadata": {},
   "source": [
    "## Batch renaming with `toDF()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "51af50c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- broadcastlogid: integer (nullable = true)\n",
      " |-- logserviceid: integer (nullable = true)\n",
      " |-- logdate: string (nullable = true)\n",
      " |-- sequenceno: integer (nullable = true)\n",
      " |-- audiencetargetageid: integer (nullable = true)\n",
      " |-- audiencetargetethnicid: integer (nullable = true)\n",
      " |-- categoryid: integer (nullable = true)\n",
      " |-- closedcaptionid: integer (nullable = true)\n",
      " |-- countryoforiginid: integer (nullable = true)\n",
      " |-- dubdramacreditid: integer (nullable = true)\n",
      " |-- ethnicprogramid: integer (nullable = true)\n",
      " |-- productionsourceid: integer (nullable = true)\n",
      " |-- programclassid: integer (nullable = true)\n",
      " |-- filmclassificationid: integer (nullable = true)\n",
      " |-- exhibitionid: integer (nullable = true)\n",
      " |-- duration: string (nullable = true)\n",
      " |-- endtime: string (nullable = true)\n",
      " |-- logentrydate: string (nullable = true)\n",
      " |-- productionno: string (nullable = true)\n",
      " |-- programtitle: string (nullable = true)\n",
      " |-- starttime: string (nullable = true)\n",
      " |-- subtitle: string (nullable = true)\n",
      " |-- networkaffiliationid: integer (nullable = true)\n",
      " |-- specialattentionid: integer (nullable = true)\n",
      " |-- broadcastoriginpointid: integer (nullable = true)\n",
      " |-- compositionid: integer (nullable = true)\n",
      " |-- producer1: string (nullable = true)\n",
      " |-- producer2: string (nullable = true)\n",
      " |-- language1: integer (nullable = true)\n",
      " |-- language2: integer (nullable = true)\n",
      " |-- duration_seconds: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs.toDF(*[x.lower() for x in logs.columns]).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faf39c7",
   "metadata": {},
   "source": [
    "## Sorting column order with `sort`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b60e7274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- AudienceTargetAgeID: integer (nullable = true)\n",
      " |-- AudienceTargetEthnicID: integer (nullable = true)\n",
      " |-- BroadcastLogID: integer (nullable = true)\n",
      " |-- BroadcastOriginPointID: integer (nullable = true)\n",
      " |-- CategoryID: integer (nullable = true)\n",
      " |-- ClosedCaptionID: integer (nullable = true)\n",
      " |-- CompositionID: integer (nullable = true)\n",
      " |-- CountryOfOriginID: integer (nullable = true)\n",
      " |-- DubDramaCreditID: integer (nullable = true)\n",
      " |-- Duration: string (nullable = true)\n",
      " |-- EndTime: string (nullable = true)\n",
      " |-- EthnicProgramID: integer (nullable = true)\n",
      " |-- ExhibitionID: integer (nullable = true)\n",
      " |-- FilmClassificationID: integer (nullable = true)\n",
      " |-- Language1: integer (nullable = true)\n",
      " |-- Language2: integer (nullable = true)\n",
      " |-- LogDate: string (nullable = true)\n",
      " |-- LogEntryDate: string (nullable = true)\n",
      " |-- LogServiceID: integer (nullable = true)\n",
      " |-- NetworkAffiliationID: integer (nullable = true)\n",
      " |-- Producer1: string (nullable = true)\n",
      " |-- Producer2: string (nullable = true)\n",
      " |-- ProductionNO: string (nullable = true)\n",
      " |-- ProductionSourceID: integer (nullable = true)\n",
      " |-- ProgramClassID: integer (nullable = true)\n",
      " |-- ProgramTitle: string (nullable = true)\n",
      " |-- SequenceNO: integer (nullable = true)\n",
      " |-- SpecialAttentionID: integer (nullable = true)\n",
      " |-- StartTime: string (nullable = true)\n",
      " |-- Subtitle: string (nullable = true)\n",
      " |-- duration_seconds: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs.select(sorted(logs.columns)).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0aea22",
   "metadata": {},
   "source": [
    "## Getting high level summary of your dataframe with `describe` and `summary`\n",
    "- `describe` only works for numerical and string columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0fc8d628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|      BroadcastLogID|\n",
      "+-------+--------------------+\n",
      "|  count|              238945|\n",
      "|   mean|1.2168651122760174E9|\n",
      "| stddev| 1.496913424143109E7|\n",
      "|    min|          1195788151|\n",
      "|    max|          1249431576|\n",
      "+-------+--------------------+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|      LogServiceID|\n",
      "+-------+------------------+\n",
      "|  count|            238945|\n",
      "|   mean| 3450.890284375065|\n",
      "| stddev|199.50673962555592|\n",
      "|    min|              3157|\n",
      "|    max|              3925|\n",
      "+-------+------------------+\n",
      "\n",
      "+-------+----------+\n",
      "|summary|   LogDate|\n",
      "+-------+----------+\n",
      "|  count|    238945|\n",
      "|   mean|      null|\n",
      "| stddev|      null|\n",
      "|    min|2018-08-01|\n",
      "|    max|2018-08-01|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show stats for the first three columns\n",
    "for i in logs.columns[:3]:\n",
    "    logs.describe(i).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6006ec29",
   "metadata": {},
   "source": [
    "- `summary` shows extra stats like 25-50% and 75% percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "689a27cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|      BroadcastLogID|\n",
      "+-------+--------------------+\n",
      "|  count|              238945|\n",
      "|   mean|1.2168651122760174E9|\n",
      "| stddev| 1.496913424143109E7|\n",
      "|    min|          1195788151|\n",
      "|    25%|          1249431576|\n",
      "|    50%|          1213242718|\n",
      "|    75%|          1226220081|\n",
      "|    max|          1249431576|\n",
      "+-------+--------------------+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|      LogServiceID|\n",
      "+-------+------------------+\n",
      "|  count|            238945|\n",
      "|   mean| 3450.890284375065|\n",
      "| stddev|199.50673962555592|\n",
      "|    min|              3157|\n",
      "|    25%|              3287|\n",
      "|    50%|              3379|\n",
      "|    75%|              3627|\n",
      "|    max|              3925|\n",
      "+-------+------------------+\n",
      "\n",
      "+-------+----------+\n",
      "|summary|   LogDate|\n",
      "+-------+----------+\n",
      "|  count|    238945|\n",
      "|   mean|      null|\n",
      "| stddev|      null|\n",
      "|    min|2018-08-01|\n",
      "|    25%|      null|\n",
      "|    50%|      null|\n",
      "|    75%|      null|\n",
      "|    max|2018-08-01|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show stats for the first three columns\n",
    "for i in logs.columns[:3]:\n",
    "    logs.select(i).summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9244193",
   "metadata": {},
   "source": [
    "## Write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "92d26e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write checkpoint file\n",
    "logs.coalesce(1).write.mode(\"overwrite\").csv(\"./output/ch04/logs.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e6debb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- BroadcastLogID: integer (nullable = true)\n",
      " |-- LogServiceID: integer (nullable = true)\n",
      " |-- LogDate: string (nullable = true)\n",
      " |-- SequenceNO: integer (nullable = true)\n",
      " |-- AudienceTargetAgeID: integer (nullable = true)\n",
      " |-- AudienceTargetEthnicID: integer (nullable = true)\n",
      " |-- CategoryID: integer (nullable = true)\n",
      " |-- ClosedCaptionID: integer (nullable = true)\n",
      " |-- CountryOfOriginID: integer (nullable = true)\n",
      " |-- DubDramaCreditID: integer (nullable = true)\n",
      " |-- EthnicProgramID: integer (nullable = true)\n",
      " |-- ProductionSourceID: integer (nullable = true)\n",
      " |-- ProgramClassID: integer (nullable = true)\n",
      " |-- FilmClassificationID: integer (nullable = true)\n",
      " |-- ExhibitionID: integer (nullable = true)\n",
      " |-- Duration: string (nullable = true)\n",
      " |-- EndTime: string (nullable = true)\n",
      " |-- LogEntryDate: string (nullable = true)\n",
      " |-- ProductionNO: string (nullable = true)\n",
      " |-- ProgramTitle: string (nullable = true)\n",
      " |-- StartTime: string (nullable = true)\n",
      " |-- Subtitle: string (nullable = true)\n",
      " |-- NetworkAffiliationID: integer (nullable = true)\n",
      " |-- SpecialAttentionID: integer (nullable = true)\n",
      " |-- BroadcastOriginPointID: integer (nullable = true)\n",
      " |-- CompositionID: integer (nullable = true)\n",
      " |-- Producer1: string (nullable = true)\n",
      " |-- Producer2: string (nullable = true)\n",
      " |-- Language1: integer (nullable = true)\n",
      " |-- Language2: integer (nullable = true)\n",
      " |-- duration_seconds: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11feddd6",
   "metadata": {},
   "source": [
    "## Chapter 5: Joining and Grouping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d5917857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- BroadcastLogID: integer (nullable = true)\n",
      " |-- LogServiceID: integer (nullable = true)\n",
      " |-- LogDate: string (nullable = true)\n",
      " |-- SequenceNO: integer (nullable = true)\n",
      " |-- AudienceTargetAgeID: integer (nullable = true)\n",
      " |-- AudienceTargetEthnicID: integer (nullable = true)\n",
      " |-- CategoryID: integer (nullable = true)\n",
      " |-- ClosedCaptionID: integer (nullable = true)\n",
      " |-- CountryOfOriginID: integer (nullable = true)\n",
      " |-- DubDramaCreditID: integer (nullable = true)\n",
      " |-- EthnicProgramID: integer (nullable = true)\n",
      " |-- ProductionSourceID: integer (nullable = true)\n",
      " |-- ProgramClassID: integer (nullable = true)\n",
      " |-- FilmClassificationID: integer (nullable = true)\n",
      " |-- ExhibitionID: integer (nullable = true)\n",
      " |-- Duration: string (nullable = true)\n",
      " |-- EndTime: string (nullable = true)\n",
      " |-- LogEntryDate: string (nullable = true)\n",
      " |-- ProductionNO: string (nullable = true)\n",
      " |-- ProgramTitle: string (nullable = true)\n",
      " |-- StartTime: string (nullable = true)\n",
      " |-- Subtitle: string (nullable = true)\n",
      " |-- NetworkAffiliationID: integer (nullable = true)\n",
      " |-- SpecialAttentionID: integer (nullable = true)\n",
      " |-- BroadcastOriginPointID: integer (nullable = true)\n",
      " |-- CompositionID: integer (nullable = true)\n",
      " |-- Producer1: string (nullable = true)\n",
      " |-- Producer2: string (nullable = true)\n",
      " |-- Language1: integer (nullable = true)\n",
      " |-- Language2: integer (nullable = true)\n",
      " |-- duration_seconds: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- LogIdentifierID: string (nullable = true)\n",
      " |-- LogServiceID: integer (nullable = true)\n",
      " |-- PrimaryFG: integer (nullable = true)\n",
      "\n",
      "+---------------+------------+---------+\n",
      "|LogIdentifierID|LogServiceID|PrimaryFG|\n",
      "+---------------+------------+---------+\n",
      "|           13ST|        3157|        1|\n",
      "|         2000SM|        3466|        1|\n",
      "|           70SM|        3883|        1|\n",
      "|           80SM|        3590|        1|\n",
      "|           90SM|        3470|        1|\n",
      "+---------------+------------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Unique primary channels:  758\n"
     ]
    }
   ],
   "source": [
    "# Set up\n",
    "import os\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Read the data\n",
    "DIRECTORY = \"./data/Ch04\"\n",
    "logs = spark.read.csv(\n",
    "    \"./output/ch04/logs.csv\", # read in data transformed in Ch04\n",
    "    sep=\",\",  # default is \",\"\n",
    "    quote='\"',  # default is double quote.\n",
    "    header=True,  # set first row as column names\n",
    "    inferSchema=True,  # infer schema from column names default False\n",
    ")\n",
    "logs.printSchema()\n",
    "\n",
    "\n",
    "# Read link table and filter to only primary channels (ie. PrimaryFG == 1)\n",
    "log_identifier = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"ReferenceTables\", \"LogIdentifier.csv\"),\n",
    "    sep=\"|\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ")\n",
    "log_identifier = log_identifier.where(F.col(\"PrimaryFG\") == 1)\n",
    "\n",
    "\n",
    "# Show results\n",
    "log_identifier.printSchema()\n",
    "log_identifier.show(5)\n",
    "print(\"Unique primary channels: \", log_identifier.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df68467",
   "metadata": {},
   "source": [
    "### Understanding the `join` recipe\n",
    "\n",
    "```py\n",
    "[LEFT].join(\n",
    "    [RIGHT],\n",
    "    on=[PREDICTES],\n",
    "    how=[METHOD]\n",
    ")\n",
    "```\n",
    "\n",
    "#### Important points\n",
    "\n",
    "1. If one record in the left table resolves the predicate with more than one record in the right table (or vice versa), __this record will be duplicated in the joined table__.\n",
    "2. If one record in the left or in the right table does not resolve the predicate with any record in the other table, __it will not be present in the resulting table, unless the join method specifies a protocol for failed predicates__.\n",
    "\n",
    "#### Pyspark helpers in join logic\n",
    "\n",
    "- You can put multiple `and` predicates into a list, like:\n",
    "    ```py\n",
    "    [\n",
    "        left[\"col1\"] == right[\"colA\"], \n",
    "        left[\"col2\"] > right[\"colB\"],  # value on left table is greater than the right\n",
    "        left[\"col3\"] != right[\"colC\"]\n",
    "    ]\n",
    "    ```\n",
    "- You can test equality just by specifying the column name, or list of column names\n",
    "\n",
    "#### Setting up join logic with `how`\n",
    "\n",
    "1. `cross` - returns a record for every record pair. not common.\n",
    "2. `inner` = returns record if predicate is true, otherwise drops it. most common, pyspark `join` default. \n",
    "3. `left` & `right` - similar to `inner`, except on what to do with false predicates:\n",
    "    - `left` join adds unmatched records from the left table in the joined table, and fills in columns from right able with `None`\n",
    "    - `right` join adds unmatched records nad fills in column vice versa.\n",
    "4. `outer` - adds unmatched records from the left and right able, padding with `None`.\n",
    "5. `left_semi` - same as inner join but only keeps columns in left table. \n",
    "6. `left_anti` - returns only records that don't match the predicate with any record in the right table.  opposite of `left` join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dae12c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join `logs` with `log_identifier` using the 'LogServiceID' column\n",
    "joined = logs.join(log_identifier, on=\"LogServiceID\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "958684fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ProgramClassID: integer (nullable = true)\n",
      " |-- CategoryID: integer (nullable = true)\n",
      " |-- LogServiceID: integer (nullable = true)\n",
      " |-- BroadcastLogID: integer (nullable = true)\n",
      " |-- LogDate: string (nullable = true)\n",
      " |-- SequenceNO: integer (nullable = true)\n",
      " |-- AudienceTargetAgeID: integer (nullable = true)\n",
      " |-- AudienceTargetEthnicID: integer (nullable = true)\n",
      " |-- ClosedCaptionID: integer (nullable = true)\n",
      " |-- CountryOfOriginID: integer (nullable = true)\n",
      " |-- DubDramaCreditID: integer (nullable = true)\n",
      " |-- EthnicProgramID: integer (nullable = true)\n",
      " |-- ProductionSourceID: integer (nullable = true)\n",
      " |-- FilmClassificationID: integer (nullable = true)\n",
      " |-- ExhibitionID: integer (nullable = true)\n",
      " |-- Duration: string (nullable = true)\n",
      " |-- EndTime: string (nullable = true)\n",
      " |-- LogEntryDate: string (nullable = true)\n",
      " |-- ProductionNO: string (nullable = true)\n",
      " |-- ProgramTitle: string (nullable = true)\n",
      " |-- StartTime: string (nullable = true)\n",
      " |-- Subtitle: string (nullable = true)\n",
      " |-- NetworkAffiliationID: integer (nullable = true)\n",
      " |-- SpecialAttentionID: integer (nullable = true)\n",
      " |-- BroadcastOriginPointID: integer (nullable = true)\n",
      " |-- CompositionID: integer (nullable = true)\n",
      " |-- Producer1: string (nullable = true)\n",
      " |-- Producer2: string (nullable = true)\n",
      " |-- Language1: integer (nullable = true)\n",
      " |-- Language2: integer (nullable = true)\n",
      " |-- duration_seconds: integer (nullable = true)\n",
      " |-- LogIdentifierID: string (nullable = true)\n",
      " |-- PrimaryFG: integer (nullable = true)\n",
      " |-- CategoryCD: string (nullable = true)\n",
      " |-- Category_Description: string (nullable = true)\n",
      " |-- ProgramClassCD: string (nullable = true)\n",
      " |-- ProgramClass_Description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Additionally join CategoryID and ProgramClassID table\n",
    "# Use left joins since keys may not be available in the link table.\n",
    "\n",
    "# CategoryID\n",
    "cd_category = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"ReferenceTables\", \"CD_Category.csv\"),\n",
    "    sep=\"|\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ").select(\n",
    "    \"CategoryID\",\n",
    "    \"CategoryCD\",\n",
    "    F.col(\"EnglishDescription\").alias(\"Category_Description\"),\n",
    ")\n",
    "\n",
    "# ProgramClass\n",
    "cd_program_class = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"ReferenceTables\", \"CD_ProgramClass.csv\"),\n",
    "    sep=\"|\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ").select(\n",
    "    \"ProgramClassID\",\n",
    "    \"ProgramClassCD\",\n",
    "    F.col(\"EnglishDescription\").alias(\"ProgramClass_Description\"),\n",
    ")\n",
    "\n",
    "\n",
    "# Join all to joined table\n",
    "full_log = joined.join(cd_category, \"CategoryID\", how=\"left\",).join(\n",
    "    cd_program_class, \"ProgramClassID\", how=\"left\",\n",
    ")\n",
    "\n",
    "\n",
    "# Check if additional columns were joined to original log data frame\n",
    "full_log.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f54661b",
   "metadata": {},
   "source": [
    "### Warning: What happens when joining columns in a distributed environment\n",
    "\n",
    ">  To be able to process a comparison between records, the data needs to be on the same machine. If not, PySpark will move the data in an operation called a _shuffle_, which is slow and expensive.  More on join strategies in later chapters.\n",
    "\n",
    "### Warning: Joining tables with identically named columns leads to errors downstream\n",
    "\n",
    "PySpark happily joins the two data frames together but fails when we try to work with the ambiguous column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0b4007e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- BroadcastLogID: integer (nullable = true)\n",
      " |-- LogServiceID: integer (nullable = true)\n",
      " |-- LogDate: string (nullable = true)\n",
      " |-- SequenceNO: integer (nullable = true)\n",
      " |-- AudienceTargetAgeID: integer (nullable = true)\n",
      " |-- AudienceTargetEthnicID: integer (nullable = true)\n",
      " |-- CategoryID: integer (nullable = true)\n",
      " |-- ClosedCaptionID: integer (nullable = true)\n",
      " |-- CountryOfOriginID: integer (nullable = true)\n",
      " |-- DubDramaCreditID: integer (nullable = true)\n",
      " |-- EthnicProgramID: integer (nullable = true)\n",
      " |-- ProductionSourceID: integer (nullable = true)\n",
      " |-- ProgramClassID: integer (nullable = true)\n",
      " |-- FilmClassificationID: integer (nullable = true)\n",
      " |-- ExhibitionID: integer (nullable = true)\n",
      " |-- Duration: string (nullable = true)\n",
      " |-- EndTime: string (nullable = true)\n",
      " |-- LogEntryDate: string (nullable = true)\n",
      " |-- ProductionNO: string (nullable = true)\n",
      " |-- ProgramTitle: string (nullable = true)\n",
      " |-- StartTime: string (nullable = true)\n",
      " |-- Subtitle: string (nullable = true)\n",
      " |-- NetworkAffiliationID: integer (nullable = true)\n",
      " |-- SpecialAttentionID: integer (nullable = true)\n",
      " |-- BroadcastOriginPointID: integer (nullable = true)\n",
      " |-- CompositionID: integer (nullable = true)\n",
      " |-- Producer1: string (nullable = true)\n",
      " |-- Producer2: string (nullable = true)\n",
      " |-- Language1: integer (nullable = true)\n",
      " |-- Language2: integer (nullable = true)\n",
      " |-- duration_seconds: integer (nullable = true)\n",
      " |-- LogIdentifierID: string (nullable = true)\n",
      " |-- LogServiceID: integer (nullable = true)\n",
      " |-- PrimaryFG: integer (nullable = true)\n",
      "\n",
      "Joined table now has two \"LogServiceID\" columns:  ['LogServiceID', 'LogServiceID'] \n",
      "\n",
      "Selecting \"LogServiceID\" will now throw an error\n",
      "AnalysisException:  Reference 'LogServiceID' is ambiguous, could be: LogServiceID, LogServiceID.;\n"
     ]
    }
   ],
   "source": [
    "# Joining two tables with the same LogServiceID column\n",
    "logs_and_channels_verbose = logs.join(\n",
    "    log_identifier, logs[\"LogServiceID\"] == log_identifier[\"LogServiceID\"]\n",
    ")\n",
    "logs_and_channels_verbose.printSchema()\n",
    "\n",
    "\n",
    "print(\n",
    "    'Joined table now has two \"LogServiceID\" columns: ',\n",
    "    [col for col in logs_and_channels_verbose.columns if col == \"LogServiceID\"],\n",
    "    \"\\n\",\n",
    ")\n",
    "print('Selecting \"LogServiceID\" will now throw an error')\n",
    "\n",
    "\n",
    "# Selecting \"LogServiceID\" will throw an error\n",
    "try:\n",
    "    logs_and_channels_verbose.select(\"LogServiceID\")\n",
    "except AnalysisException as err:\n",
    "    print(\"AnalysisException: \", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8635c576",
   "metadata": {},
   "source": [
    "### Solutions for preventing ambiguous column references\n",
    "\n",
    "1. Use simplified syntax (ie. passing string of column you want). Auto-removes second instance of predicate column.  Can only use on equi-joins.\n",
    "    ```\n",
    "    logs_and_channels = logs.join(log_identifier, \"LogServiceID\")\n",
    "    ```\n",
    "2. Refer to the pre-existing table name.\n",
    "    ```\n",
    "    logs_and_channels_verbose.select(log_identifier[\"LogServiceID\"])\n",
    "    ```\n",
    "3. Use the `Column` object directly\n",
    "    ```\n",
    "    logs_and_channels_verbose = logs.alias(\"left\").join(\n",
    "    log_identifier.alias(\"right\"),\n",
    "    logs[\"LogServiceID\"] == log_identifier[\"LogServiceID\"],\n",
    "    )\n",
    "\n",
    "    logs_and_channels_verbose.drop(F.col(\"right.LogServiceID\")).select(\n",
    "        \"LogServiceID\"\n",
    "    )\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff6f195",
   "metadata": {},
   "source": [
    "## Advanced `groupby` with `GroupedData`\n",
    "\n",
    "Goal: __What channels have the most and least proportion of commercials?__\n",
    "\n",
    "Task:\n",
    "1. Get number of seconds when the program is a commerical\n",
    "2. Get total number of seconds.\n",
    "\n",
    "### `groupby` on multiple columns\n",
    "\n",
    "- Grouped by results are `GroupedData` objects, not `data frame`.  Can't call `show()` on it.\n",
    "- You can \"show\" by running summary functions on it, like `F.sum`.\n",
    "- `GroupedData` object holds all non-key columns in a group cell (see fig 5.7)\n",
    "\n",
    "![grouped](./notes/img/grouped.png)\n",
    "\n",
    "\n",
    "### `agg()` vs `sum()`\n",
    "\n",
    "- `agg` can take an arbitrary number of aggregate functions\n",
    "- You can alias resulting columns, unlike `sum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "53d715cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------------------------+--------------+\n",
      "|ProgramClassCD|ProgramClass_Description              |duration_total|\n",
      "+--------------+--------------------------------------+--------------+\n",
      "|PGR           |PROGRAM                               |20992510      |\n",
      "|COM           |COMMERCIAL MESSAGE                    |3519163       |\n",
      "|PFS           |PROGRAM FIRST SEGMENT                 |1344762       |\n",
      "|SEG           |SEGMENT OF A PROGRAM                  |1205998       |\n",
      "|PRC           |PROMOTION OF UPCOMING CANADIAN PROGRAM|880600        |\n",
      "|PGI           |PROGRAM INFOMERCIAL                   |679182        |\n",
      "|PRO           |PROMOTION OF NON-CANADIAN PROGRAM     |335701        |\n",
      "|OFF           |SCHEDULED OFF AIR TIME PERIOD         |142279        |\n",
      "|ID            |NETWORK IDENTIFICATION MESSAGE        |74926         |\n",
      "|NRN           |No recognized nationality             |59686         |\n",
      "|MAG           |MAGAZINE PROGRAM                      |57622         |\n",
      "|PSA           |PUBLIC SERVICE ANNOUNCEMENT           |51214         |\n",
      "|SO            |MAY IDENTIFY THE SIGN ON\\OFF OF A DAY |32509         |\n",
      "|OFT           |OFF AIR DUE TO TECHNICAL DIFFICULTY   |18263         |\n",
      "|LOC           |LOCAL ADVERTISING                     |13294         |\n",
      "|MVC           |MUSIC VIDEO CLIP                      |7907          |\n",
      "|REG           |REGIONAL                              |6749          |\n",
      "|MER           |MERCHANDISING                         |1680          |\n",
      "|SPO           |SPONSORSHIP MESSAGE                   |1544          |\n",
      "|SOL           |SOLICITATION MESSAGE                  |596           |\n",
      "|MOS           |Mosaic                                |null          |\n",
      "|COR           |CORNERSTONE                           |null          |\n",
      "+--------------+--------------------------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by ProgramClassCD and ProgramClass_Description, sum total duration for each\n",
    "\n",
    "full_log.groupby(\"ProgramClassCD\", \"ProgramClass_Description\").agg(\n",
    "    F.sum(\"duration_seconds\").alias(\"duration_total\")\n",
    ").orderBy(\"duration_total\", ascending=False).show(100, False)\n",
    "\n",
    "\n",
    "# Another way by passing dictionary to agg\n",
    "# full_log.groupby(\"ProgramClassCD\", \"ProgramClass_Description\").agg(\n",
    "#     {\"duration_seconds\": \"sum\"}\n",
    "# ).withColumnRenamed(\"sum(duration_seconds)\", \"duration_total\").orderBy(\n",
    "#     \"duration_total\", ascending=False\n",
    "# ).show(\n",
    "#     100, False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3406ca8",
   "metadata": {},
   "source": [
    "### Using agg with custom column definitions\n",
    "\n",
    "`when` logic:\n",
    "\n",
    "```py\n",
    "(\n",
    "F.when([BOOLEAN TEST], [RESULT IF TRUE])\n",
    " .when([ANOTHER BOOLEAN TEST], [RESULT IF TRUE])\n",
    " .otherwise([DEFAULT RESULT, WILL DEFAULT TO null IF OMITTED])\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ba9b4286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+--------------+------------------+\n",
      "|LogIdentifierID|duration_commercial|duration_total|commercial_ratio  |\n",
      "+---------------+-------------------+--------------+------------------+\n",
      "|CIMT           |775                |775           |1.0               |\n",
      "|TELENO         |17790              |17790         |1.0               |\n",
      "|MSET           |2700               |2700          |1.0               |\n",
      "|HPITV          |13                 |13            |1.0               |\n",
      "|TLNSP          |15480              |15480         |1.0               |\n",
      "|TANG           |8125               |8125          |1.0               |\n",
      "|MMAX           |23333              |23582         |0.9894410991434145|\n",
      "|MPLU           |20587              |20912         |0.9844586840091814|\n",
      "|INVST          |20094              |20470         |0.9816316560820714|\n",
      "|ZT�L�          |21542              |21965         |0.9807420896881403|\n",
      "|RAPT           |17916              |18279         |0.9801411455768915|\n",
      "|CANALD         |21437              |21875         |0.9799771428571429|\n",
      "|ONEBMS         |18084              |18522         |0.9763524457402009|\n",
      "|CANALVIE       |20780              |21309         |0.975174808766249 |\n",
      "|unis           |11630              |11998         |0.9693282213702283|\n",
      "|CIVM           |11370              |11802         |0.9633960345704118|\n",
      "|TV5            |10759              |11220         |0.9589126559714795|\n",
      "|LEAF           |11526              |12034         |0.9577862722286854|\n",
      "|VISION         |12946              |13621         |0.950444167094927 |\n",
      "|CJIL           |3904               |4213          |0.9266555898409684|\n",
      "+---------------+-------------------+--------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Goal: Compute only the commercial time for each program\n",
    "\n",
    "\n",
    "# Create custom column logic - get duration_seconds if ProgramClassCD matches an item in\n",
    "# the list\n",
    "is_commercial = F.when(\n",
    "    F.trim(F.col(\"ProgramClassCD\")).isin(\n",
    "        [\"COM\", \"PRC\", \"PGI\", \"PRO\", \"LOC\", \"SPO\", \"MER\", \"SOL\"]\n",
    "    ),\n",
    "    F.col(\"duration_seconds\"),\n",
    ").otherwise(0)\n",
    "\n",
    "\n",
    "# Use custom column logic to build a duration_commercial column,\n",
    "# along with duration_total\n",
    "commercial_time = (\n",
    "    full_log.groupby(\"LogIdentifierID\")\n",
    "    .agg(\n",
    "        F.sum(is_commercial).alias(\"duration_commercial\"),\n",
    "        F.sum(\"duration_seconds\").alias(\"duration_total\"),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"commercial_ratio\", F.col(\"duration_commercial\") / F.col(\"duration_total\")\n",
    "    )\n",
    ")\n",
    "\n",
    "commercial_time.orderBy(\"commercial_ratio\", ascending=False).show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82775c8",
   "metadata": {},
   "source": [
    "## Dropping unwanted records - `dropna` + `fillna`\n",
    "\n",
    "### `dropna`\n",
    "\n",
    "#### params\n",
    "1. `how`, which can take the value any or all. If any is selected, PySpark will drop records where at least one of the fields are null. In the case of all, only the records where all fields are null will be removed. By default, PySpark will take the any mode.\n",
    "2. `thresh` takes an integer value. If set (its default is None), PySpark will ignore the how parameter and only drop the records with less than thresh non-null values.\n",
    "3. `subset` will take an optional list of columns that drop will use to make its decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dd392e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+--------------+------------------+\n",
      "|LogIdentifierID|duration_commercial|duration_total|  commercial_ratio|\n",
      "+---------------+-------------------+--------------+------------------+\n",
      "|          HPITV|                 13|            13|               1.0|\n",
      "|           CIMT|                775|           775|               1.0|\n",
      "|           MSET|               2700|          2700|               1.0|\n",
      "|          TLNSP|              15480|         15480|               1.0|\n",
      "|         TELENO|              17790|         17790|               1.0|\n",
      "|           TANG|               8125|          8125|               1.0|\n",
      "|           MMAX|              23333|         23582|0.9894410991434145|\n",
      "|           MPLU|              20587|         20912|0.9844586840091814|\n",
      "|          INVST|              20094|         20470|0.9816316560820714|\n",
      "|          ZT�L�|              21542|         21965|0.9807420896881403|\n",
      "|           RAPT|              17916|         18279|0.9801411455768915|\n",
      "|         CANALD|              21437|         21875|0.9799771428571429|\n",
      "|         ONEBMS|              18084|         18522|0.9763524457402009|\n",
      "|       CANALVIE|              20780|         21309| 0.975174808766249|\n",
      "|           unis|              11630|         11998|0.9693282213702283|\n",
      "|           CIVM|              11370|         11802|0.9633960345704118|\n",
      "|            TV5|              10759|         11220|0.9589126559714795|\n",
      "|           LEAF|              11526|         12034|0.9577862722286854|\n",
      "|         VISION|              12946|         13621| 0.950444167094927|\n",
      "|           CJIL|               3904|          4213|0.9266555898409684|\n",
      "+---------------+-------------------+--------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Records in commercial_time:  324\n",
      "Records in c_time_no_null:  322\n"
     ]
    }
   ],
   "source": [
    "# Drop records that have a commericla_ratio of null\n",
    "\n",
    "c_time_no_null = commercial_time.dropna(subset=[\"commercial_ratio\"])\n",
    "c_time_no_null.orderBy(\"commercial_ratio\", ascending=False).show()\n",
    "\n",
    "\n",
    "# Check record counts for each\n",
    "print(\"Records in commercial_time: \", commercial_time.count())\n",
    "print(\"Records in c_time_no_null: \", c_time_no_null.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf9c9c5",
   "metadata": {},
   "source": [
    "### `fillna`\n",
    "\n",
    "#### params\n",
    "\n",
    "1. `value`, either a Python int, float, string or bool.\n",
    "2. `subset`, which columns to fill\n",
    "\n",
    "__Tip__: You can fill nulls differently for each column by passing a dictionary:\n",
    "\n",
    "```py\n",
    "answer_no_null = answer.fillna(\n",
    "    {\"duration_commercial\": 0, \"duration_total\": 0, \"commercial_ratio\": 0}\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "545392fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+--------------+------------------+\n",
      "|LogIdentifierID|duration_commercial|duration_total|  commercial_ratio|\n",
      "+---------------+-------------------+--------------+------------------+\n",
      "|           CIMT|                775|           775|               1.0|\n",
      "|           MSET|               2700|          2700|               1.0|\n",
      "|          TLNSP|              15480|         15480|               1.0|\n",
      "|          HPITV|                 13|            13|               1.0|\n",
      "|         TELENO|              17790|         17790|               1.0|\n",
      "|           TANG|               8125|          8125|               1.0|\n",
      "|           MMAX|              23333|         23582|0.9894410991434145|\n",
      "|           MPLU|              20587|         20912|0.9844586840091814|\n",
      "|          INVST|              20094|         20470|0.9816316560820714|\n",
      "|          ZT�L�|              21542|         21965|0.9807420896881403|\n",
      "|           RAPT|              17916|         18279|0.9801411455768915|\n",
      "|         CANALD|              21437|         21875|0.9799771428571429|\n",
      "|         ONEBMS|              18084|         18522|0.9763524457402009|\n",
      "|       CANALVIE|              20780|         21309| 0.975174808766249|\n",
      "|           unis|              11630|         11998|0.9693282213702283|\n",
      "|           CIVM|              11370|         11802|0.9633960345704118|\n",
      "|            TV5|              10759|         11220|0.9589126559714795|\n",
      "|           LEAF|              11526|         12034|0.9577862722286854|\n",
      "|         VISION|              12946|         13621| 0.950444167094927|\n",
      "|           CJIL|               3904|          4213|0.9266555898409684|\n",
      "+---------------+-------------------+--------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Records in commercial_time:  324\n",
      "Records in c_time_no_null:  324\n"
     ]
    }
   ],
   "source": [
    "# Fill null fields\n",
    "\n",
    "c_time_fill_null = commercial_time.fillna(0)\n",
    "c_time_fill_null.orderBy(\"commercial_ratio\", ascending=False).show()\n",
    "\n",
    "\n",
    "# Check record counts for each\n",
    "print(\"Records in commercial_time: \", commercial_time.count())\n",
    "print(\"Records in c_time_no_null: \", c_time_fill_null.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c5d208",
   "metadata": {},
   "source": [
    "## Pulling it all together\n",
    "\n",
    "[summary code of all the steps taken in this notebook as a spark script](code/Ch04-05/commercials.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df242784",
   "metadata": {},
   "source": [
    "## Chapter 6: Multi-dimensional data frames: using PySpark with JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "06d716ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151712d8",
   "metadata": {},
   "source": [
    "## Reading the Data\n",
    "\n",
    "For this chapter, we use a JSON dump of the information about the TV Show Silicon Valley, from TV Maze.\n",
    "\n",
    "### JSON params\n",
    "\n",
    "- No need for delimiters like CSV\n",
    "- No need to infer data type\n",
    "- Contains __hierarchical data__, unlike CSVs\n",
    "- Single JSON: __one JSON document, one line, one record__.\n",
    "- Multiple JSON (`multiLine`):  __one JSON document, one FILE, one record__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "61f92638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import a single JSON document\n",
    "sv = \"data/ch06/shows-silicon-valley.json\"\n",
    "shows = spark.read.json(sv)\n",
    "display(shows.count())\n",
    "\n",
    "\n",
    "# Read multiple JSON documents using multiLine param\n",
    "three_shows = spark.read.json(\"data/ch06/shows-*.json\", multiLine=True)\n",
    "display(three_shows.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "e6b4b3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _embedded: struct (nullable = true)\n",
      " |    |-- episodes: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- _links: struct (nullable = true)\n",
      " |    |    |    |    |-- self: struct (nullable = true)\n",
      " |    |    |    |    |    |-- href: string (nullable = true)\n",
      " |    |    |    |-- airdate: string (nullable = true)\n",
      " |    |    |    |-- airstamp: timestamp (nullable = true)\n",
      " |    |    |    |-- airtime: string (nullable = true)\n",
      " |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |-- image: struct (nullable = true)\n",
      " |    |    |    |    |-- medium: string (nullable = true)\n",
      " |    |    |    |    |-- original: string (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- number: long (nullable = true)\n",
      " |    |    |    |-- runtime: long (nullable = true)\n",
      " |    |    |    |-- season: long (nullable = true)\n",
      " |    |    |    |-- summary: string (nullable = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      " |-- _links: struct (nullable = true)\n",
      " |    |-- previousepisode: struct (nullable = true)\n",
      " |    |    |-- href: string (nullable = true)\n",
      " |    |-- self: struct (nullable = true)\n",
      " |    |    |-- href: string (nullable = true)\n",
      " |-- externals: struct (nullable = true)\n",
      " |    |-- imdb: string (nullable = true)\n",
      " |    |-- thetvdb: long (nullable = true)\n",
      " |    |-- tvrage: long (nullable = true)\n",
      " |-- genres: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- medium: string (nullable = true)\n",
      " |    |-- original: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- network: struct (nullable = true)\n",
      " |    |-- country: struct (nullable = true)\n",
      " |    |    |-- code: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- timezone: string (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |-- officialSite: string (nullable = true)\n",
      " |-- premiered: string (nullable = true)\n",
      " |-- rating: struct (nullable = true)\n",
      " |    |-- average: double (nullable = true)\n",
      " |-- runtime: long (nullable = true)\n",
      " |-- schedule: struct (nullable = true)\n",
      " |    |-- days: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- time: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- updated: long (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- webChannel: string (nullable = true)\n",
      " |-- weight: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the schema\n",
    "shows.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611fb285",
   "metadata": {},
   "source": [
    "## Spark's complex column types: `array`, `map` and `struct`\n",
    "\n",
    "### `array`\n",
    "\n",
    "- PySpark arrays are containers for values of the same type, unlike JSON.\n",
    "- __PySpark will not raise an error if you try to read an array-type column with multiple types__. Instead, it will simply default to the lowest common denominator, usually the string.\n",
    "- Many array functions are available from `pyspark.sql.functions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "efb20c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+\n",
      "|name          |genres  |\n",
      "+--------------+--------+\n",
      "|Silicon Valley|[Comedy]|\n",
      "+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting the name and genres columns of the shows dataframe\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "array_subset = shows.select(\"name\", \"genres\")\n",
    "array_subset.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "857313ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+-------------+--------------+--------------+\n",
      "|          name|dot_and_index|col_and_index|dot_and_method|col_and_method|\n",
      "+--------------+-------------+-------------+--------------+--------------+\n",
      "|Silicon Valley|       Comedy|       Comedy|        Comedy|        Comedy|\n",
      "+--------------+-------------+-------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multiple methods to extract the same array\n",
    "\n",
    "array_subset = array_subset.select(\n",
    "    \"name\",\n",
    "    array_subset.genres[0].alias(\"dot_and_index\"),\n",
    "    F.col(\"genres\")[0].alias(\"col_and_index\"),\n",
    "    array_subset.genres.getItem(0).alias(\"dot_and_method\"),\n",
    "    F.col(\"genres\").getItem(0).alias(\"col_and_method\"),\n",
    ")\n",
    "\n",
    "array_subset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7df8bba",
   "metadata": {},
   "source": [
    "> WARNING: Although the square bracket approach looks very Pythonic, __you can’t use it as a slicing tool__. PySpark will accept only one integer as an index.\n",
    "\n",
    "#### Creating an array column\n",
    "\n",
    "1. Create three literal columns (using `lit()` to create scalar columns, then `make_array()`) to create an array of possible genres.\n",
    "2. Use the function `array_repeat()` to create a column repeating the \"Comedy\" string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "5b8c64e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------------+----------------------------------------+\n",
      "|name          |Some_Genres            |Repeated_Genres                         |\n",
      "+--------------+-----------------------+----------------------------------------+\n",
      "|Silicon Valley|[Comedy, Horror, Drama]|[Comedy, Comedy, Comedy, Comedy, Comedy]|\n",
      "+--------------+-----------------------+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. Create three literal columns (using lit() to create scalar columns, \n",
    "   then make_array() to ) to create an array of possible genres.\n",
    "2. Use the function array_repeat() to create a column repeating the \"Comedy\" string\n",
    "\"\"\"\n",
    "\n",
    "array_subset_repeated = array_subset.select(\n",
    "    \"name\",\n",
    "    F.lit(\"Comedy\").alias(\"one\"),\n",
    "    F.lit(\"Horror\").alias(\"two\"),\n",
    "    F.lit(\"Drama\").alias(\"three\"),\n",
    "    F.col(\"dot_and_index\"),\n",
    ").select(\n",
    "    \"name\",\n",
    "    F.array(\"one\", \"two\", \"three\").alias(\"Some_Genres\"),\n",
    "    F.array_repeat(\"dot_and_index\", 5).alias(\"Repeated_Genres\"),\n",
    ")\n",
    "\n",
    "array_subset_repeated.show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0198277",
   "metadata": {},
   "source": [
    "#### Use `F.size` to show the number of elements in an array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "ac7f8334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------+---------------------+\n",
      "|          name|size(Some_Genres)|size(Repeated_Genres)|\n",
      "+--------------+-----------------+---------------------+\n",
      "|Silicon Valley|                3|                    5|\n",
      "+--------------+-----------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "array_subset_repeated.select(\n",
    "    \"name\", F.size(\"Some_Genres\"), F.size(\"Repeated_Genres\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8403ef",
   "metadata": {},
   "source": [
    "#### Use `F.array_distinct()` to remove duplicates (like SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "ebf566a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------------------+-------------------------------+\n",
      "|name          |array_distinct(Some_Genres)|array_distinct(Repeated_Genres)|\n",
      "+--------------+---------------------------+-------------------------------+\n",
      "|Silicon Valley|[Comedy, Horror, Drama]    |[Comedy]                       |\n",
      "+--------------+---------------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "array_subset_repeated.select(\n",
    "    \"name\",\n",
    "    F.array_distinct(\"Some_Genres\"),\n",
    "    F.array_distinct(\"Repeated_Genres\")\n",
    ").show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3134c4",
   "metadata": {},
   "source": [
    "#### Use `F.array_intersect` to show common values across arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d94acf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+\n",
      "|          name|  Genres|\n",
      "+--------------+--------+\n",
      "|Silicon Valley|[Comedy]|\n",
      "+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "array_subset_repeated = array_subset_repeated.select(\n",
    "    \"name\", \n",
    "    F.array_intersect(\"Some_Genres\", \"Repeated_Genres\").alias(\"Genres\")\n",
    ")\n",
    "\n",
    "array_subset_repeated.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da22d5e1",
   "metadata": {},
   "source": [
    "#### Use `array_position()` to get the position of the item in an array if it exists\n",
    "\n",
    "> WARNING: `array_position` is 1-based, unlike Python lists or extracting elements from arrays (e.g. ` array_subset.genres[0]` or `getItems(0)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "7499292e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|          name|Genres|\n",
      "+--------------+------+\n",
      "|Silicon Valley|     1|\n",
      "+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# When using array_position(), the first item of the array has position 1, \n",
    "# not 0 like in python.\n",
    "array_subset_repeated.select(\n",
    "    \"name\",\n",
    "    F.array_position(\"Genres\", \"Comedy\").alias(\"Genres\"),\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c84a2d",
   "metadata": {},
   "source": [
    "### `map`\n",
    "\n",
    "- Like Python typed dictionary: you have keys and values just like in a dictionary, \n",
    "- Like `array`, keys need to be of the same type and the values need to be of the same type\n",
    "- Values can usually be null, but keys can’t (like Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "fba3305c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two columns of arays\n",
      "+----------------------+-----------------------------------+\n",
      "|keys                  |values                             |\n",
      "+----------------------+-----------------------------------+\n",
      "|[name, language, type]|[Silicon Valley, English, Scripted]|\n",
      "+----------------------+-----------------------------------+\n",
      "\n",
      "root\n",
      " |-- mapped: map (nullable = false)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "1 column of map\n",
      "+---------------------------------------------------------------+\n",
      "|mapped                                                         |\n",
      "+---------------------------------------------------------------+\n",
      "|[name -> Silicon Valley, language -> English, type -> Scripted]|\n",
      "+---------------------------------------------------------------+\n",
      "\n",
      "3 ways to select a key in a map\n",
      "+--------------+--------------+--------------+\n",
      "|          name|  mapped[name]|  mapped[name]|\n",
      "+--------------+--------------+--------------+\n",
      "|Silicon Valley|Silicon Valley|Silicon Valley|\n",
      "+--------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a map from two arrays: one for the keys, one for the values. \n",
    "# This creates a hash-map within the column record.\n",
    "\n",
    "# 1. Create two columns of arrays\n",
    "columns = [\"name\", \"language\", \"type\"]\n",
    "shows_map = shows.select(\n",
    "    *[F.lit(column) for column in columns],\n",
    "    F.array(*columns).alias(\"values\")\n",
    ")\n",
    "shows_map = shows_map.select(F.array(*columns).alias(\"keys\"), \"values\")\n",
    "print(\"Two columns of arays\")\n",
    "shows_map.show(1, False)\n",
    "\n",
    "# 2. Map them together using one array as the key, and other as value\n",
    "shows_map = shows_map.select(\n",
    "    F.map_from_arrays(\"keys\", \"values\").alias(\"mapped\")\n",
    ")\n",
    "shows_map.printSchema()\n",
    "print(\"1 column of map\")\n",
    "shows_map.show(1, False)\n",
    "\n",
    "# 3. 3 ways to select a key in a map column\n",
    "print(\"3 ways to select a key in a map\")\n",
    "shows_map.select(\n",
    "    F.col(\"mapped.name\"), # dot_notation with col\n",
    "    F.col(\"mapped\")[\"name\"], # Python dictionary style\n",
    "    shows_map.mapped[\"name\"] # dot_notation to get the column + bracket\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114b9ae3",
   "metadata": {},
   "source": [
    "### `struct`\n",
    "\n",
    "- Similar to JSON object.  Key is a string and record can be of a different type.\n",
    "- Unlike array & map, __the number of fields and their names are known ahead of time__\n",
    "\n",
    "\n",
    "![](notes/img/struct.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "5234edb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- schedule: struct (nullable = true)\n",
      " |    |-- days: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \"schedule\" column contain array of strings and a string\n",
    "shows.select(\"schedule\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cd2201",
   "metadata": {},
   "source": [
    "![](notes/img/embedded.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "0d9444a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _embedded: struct (nullable = true)\n",
      " |    |-- episodes: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- _links: struct (nullable = true)\n",
      " |    |    |    |    |-- self: struct (nullable = true)\n",
      " |    |    |    |    |    |-- href: string (nullable = true)\n",
      " |    |    |    |-- airdate: string (nullable = true)\n",
      " |    |    |    |-- airstamp: timestamp (nullable = true)\n",
      " |    |    |    |-- airtime: string (nullable = true)\n",
      " |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |-- image: struct (nullable = true)\n",
      " |    |    |    |    |-- medium: string (nullable = true)\n",
      " |    |    |    |    |-- original: string (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- number: long (nullable = true)\n",
      " |    |    |    |-- runtime: long (nullable = true)\n",
      " |    |    |    |-- season: long (nullable = true)\n",
      " |    |    |    |-- summary: string (nullable = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A more complex struct\n",
    "shows.select(\"_embedded\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92dfba1",
   "metadata": {},
   "source": [
    "Above `struct` visualized:\n",
    "![](notes/img/embedded.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "f40180a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- episodes: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- _links: struct (nullable = true)\n",
      " |    |    |    |-- self: struct (nullable = true)\n",
      " |    |    |    |    |-- href: string (nullable = true)\n",
      " |    |    |-- airdate: string (nullable = true)\n",
      " |    |    |-- airstamp: timestamp (nullable = true)\n",
      " |    |    |-- airtime: string (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- image: struct (nullable = true)\n",
      " |    |    |    |-- medium: string (nullable = true)\n",
      " |    |    |    |-- original: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- number: long (nullable = true)\n",
      " |    |    |-- runtime: long (nullable = true)\n",
      " |    |    |-- season: long (nullable = true)\n",
      " |    |    |-- summary: string (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop useless _embedded column and promote the fields within\n",
    "shows_clean = shows.withColumn(\"episodes\", F.col(\"_embedded.episodes\")).drop(\n",
    "    \"_embedded\"\n",
    ")\n",
    "shows_clean.select(\"episodes\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1697e6",
   "metadata": {},
   "source": [
    "#### Using `explode` to split arrays into rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "5199f9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+-------------------------+\n",
      "|name                     |\n",
      "+-------------------------+\n",
      "|Minimum Viable Product   |\n",
      "|The Cap Table            |\n",
      "|Articles of Incorporation|\n",
      "+-------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \"episodes.name\" == array of strings\n",
    "episodes_name = shows_clean.select(F.col(\"episodes.name\"))\n",
    "episodes_name.printSchema()\n",
    "\n",
    "# Just showing episodes_name is messy, so explode the array to show the names\n",
    "episodes_name.select(F.explode(\"name\").alias(\"name\")).show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96de9f6",
   "metadata": {},
   "source": [
    "## How to define and use a schema with a PySpark data frame\n",
    "\n",
    "- Can build either 1) programmatically, or 2) DDL-style schema\n",
    "- Type objects used to build schema located in `pyspark.sql.types`, usually imported as `T`.\n",
    "\n",
    "Two object types in `pyspark.sql.types`\n",
    "1. types object - represent column of a certain type (e.g. `LongType()`, `DecimalType(precision, scale)`, `ArrayType(StringType())`, etc.\n",
    "2. field object - represent arbitrary number of named fields (e.g. StructField())\n",
    "  - 2 mandatory params, `name` (str) and `dataType` (type)\n",
    "  \n",
    "Putting it altogether:\n",
    "```\n",
    "T.StructField(\"summary\", T.StringType())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "52b9b47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _embedded: struct (nullable = true)\n",
      " |    |-- episodes: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- _links: struct (nullable = true)\n",
      " |    |    |    |    |-- self: struct (nullable = true)\n",
      " |    |    |    |    |    |-- href: string (nullable = true)\n",
      " |    |    |    |-- airdate: string (nullable = true)\n",
      " |    |    |    |-- airstamp: timestamp (nullable = true)\n",
      " |    |    |    |-- airtime: string (nullable = true)\n",
      " |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |-- image: struct (nullable = true)\n",
      " |    |    |    |    |-- medium: string (nullable = true)\n",
      " |    |    |    |    |-- original: string (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- number: long (nullable = true)\n",
      " |    |    |    |-- runtime: long (nullable = true)\n",
      " |    |    |    |-- season: long (nullable = true)\n",
      " |    |    |    |-- summary: string (nullable = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For reference\n",
    "shows.select(\"_embedded\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7e1a95",
   "metadata": {},
   "source": [
    "#### Building the entire schema from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "84bc6f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full schema from scratch\n",
    "\n",
    "# episode links\n",
    "episode_links_schema = T.StructType(\n",
    "    [T.StructField(\"self\", T.StructType([T.StructField(\"href\", T.StringType())]))]\n",
    ")\n",
    "\n",
    "# episode image\n",
    "episode_image_schema = T.StructType(\n",
    "    [\n",
    "        T.StructField(\"medium\", T.StringType()),\n",
    "        T.StructField(\"original\", T.StringType()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# episode metadata\n",
    "episode_schema = T.StructType(\n",
    "    [\n",
    "        T.StructField(\"_links\", episode_links_schema),\n",
    "        T.StructField(\"airdate\", T.DateType()),\n",
    "        T.StructField(\"airstamp\", T.TimestampType()),\n",
    "        T.StructField(\"airtime\", T.StringType()),\n",
    "        T.StructField(\"id\", T.StringType()),\n",
    "        T.StructField(\"image\", episode_image_schema),\n",
    "        T.StructField(\"name\", T.StringType()),\n",
    "        T.StructField(\"number\", T.LongType()),\n",
    "        T.StructField(\"runtime\", T.LongType()),\n",
    "        T.StructField(\"season\", T.LongType()),\n",
    "        T.StructField(\"summary\", T.StringType()),\n",
    "        T.StructField(\"url\", T.StringType()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# set top level array\n",
    "embedded_schema = T.StructType([T.StructField(\"episodes\", T.ArrayType(episode_schema))])\n",
    "\n",
    "# network\n",
    "network_schema = T.StructType(\n",
    "    [\n",
    "        T.StructField(\n",
    "            \"country\",\n",
    "            T.StructType(\n",
    "                [\n",
    "                    T.StructField(\"code\", T.StringType()),\n",
    "                    T.StructField(\"name\", T.StringType()),\n",
    "                    T.StructField(\"timezone\", T.StringType()),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        T.StructField(\"id\", T.LongType()),\n",
    "        T.StructField(\"name\", T.StringType()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# shows (with embedded_schema and network_schema)\n",
    "shows_schema = T.StructType(\n",
    "    [\n",
    "        T.StructField(\"_embedded\", embedded_schema),\n",
    "        T.StructField(\"language\", T.StringType()),\n",
    "        T.StructField(\"name\", T.StringType()),\n",
    "        T.StructField(\"network\", network_schema),\n",
    "        T.StructField(\"officialSite\", T.StringType()),\n",
    "        T.StructField(\"premiered\", T.StringType()),\n",
    "        T.StructField(\n",
    "            \"rating\", T.StructType([T.StructField(\"average\", T.DoubleType())])\n",
    "        ),\n",
    "        T.StructField(\"runtime\", T.LongType()),\n",
    "        T.StructField(\n",
    "            \"schedule\",\n",
    "            T.StructType(\n",
    "                [\n",
    "                    T.StructField(\"days\", T.ArrayType(T.StringType())),\n",
    "                    T.StructField(\"time\", T.StringType()),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        T.StructField(\"status\", T.StringType()),\n",
    "        T.StructField(\"summary\", T.StringType()),\n",
    "        T.StructField(\"type\", T.StringType()),\n",
    "        T.StructField(\"updated\", T.LongType()),\n",
    "        T.StructField(\"url\", T.StringType()),\n",
    "        T.StructField(\"webChannel\", T.StringType()),\n",
    "        T.StructField(\"weight\", T.LongType()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306a72ec",
   "metadata": {},
   "source": [
    "## Reading JSON with a strict schema\n",
    "\n",
    "Read the JSON file using the schema that we built up:\n",
    "- `mode=\"FAILFAST\"` is a param to throw an error if it reads a malformed record versus the schema provided.\n",
    "- If reading non-standard date/timestamp format, you'll need to pass the right format to `dateFormat` or `timestampFormat`.\n",
    "\n",
    "> Default for `mode` parameter is `PERMISSIVE`, which sets malformed records to `null`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "5ef02e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|col       |\n",
      "+----------+\n",
      "|2014-04-06|\n",
      "|2014-04-13|\n",
      "|2014-04-20|\n",
      "|2014-04-27|\n",
      "|2014-05-04|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------------+\n",
      "|col                |\n",
      "+-------------------+\n",
      "|2014-04-06 22:00:00|\n",
      "|2014-04-13 22:00:00|\n",
      "|2014-04-20 22:00:00|\n",
      "|2014-04-27 22:00:00|\n",
      "|2014-05-04 22:00:00|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shows_with_schema = spark.read.json(\"./data/Ch06/shows-silicon-valley.json\",\n",
    "                                   schema=shows_schema,\n",
    "                                   mode=\"FAILFAST\")\n",
    "\n",
    "# Check format for modified columns:\n",
    "for column in [\"airdate\", \"airstamp\"]:\n",
    "    shows_with_schema.select(f\"_embedded.episodes.{column}\") \\\n",
    "                     .select(F.explode(column)) \\\n",
    "                     .show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dfd7e9",
   "metadata": {},
   "source": [
    "Example of `FAILFAST` error due to conflicting schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "4c6e9771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py4j.protocol import Py4JJavaError\n",
    "\n",
    "shows_schema2 = T.StructType(\n",
    "    [\n",
    "        T.StructField(\"_embedded\", embedded_schema),\n",
    "        T.StructField(\"language\", T.StringType()),\n",
    "        T.StructField(\"name\", T.StringType()),\n",
    "        T.StructField(\"network\", network_schema),\n",
    "        T.StructField(\"officialSite\", T.StringType()),\n",
    "        T.StructField(\"premiered\", T.StringType()),\n",
    "        T.StructField(\n",
    "            \"rating\", T.StructType([T.StructField(\"average\", T.DoubleType())])\n",
    "        ),\n",
    "        T.StructField(\"runtime\", T.LongType()),\n",
    "        T.StructField(\n",
    "            \"schedule\",\n",
    "            T.StructType(\n",
    "                [\n",
    "                    T.StructField(\"days\", T.ArrayType(T.StringType())),\n",
    "                    T.StructField(\"time\", T.StringType()),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        T.StructField(\"status\", T.StringType()),\n",
    "        T.StructField(\"summary\", T.StringType()),\n",
    "        T.StructField(\"type\", T.LongType()),         # switch to LongType\n",
    "        T.StructField(\"updated\", T.LongType()),      # switch to LongType\n",
    "        T.StructField(\"url\", T.LongType()),          # switch to LongType\n",
    "        T.StructField(\"webChannel\", T.StringType()),\n",
    "        T.StructField(\"weight\", T.LongType()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "shows_with_schema_wrong = spark.read.json(\n",
    "    \"data/Ch06/shows-silicon-valley.json\", schema=shows_schema2, mode=\"FAILFAST\",\n",
    ")\n",
    "\n",
    "try:\n",
    "    shows_with_schema_wrong.show()\n",
    "except Py4JJavaError:\n",
    "    pass\n",
    "\n",
    "# Huge Spark ERROR stacktrace, relevant bit:\n",
    "#\n",
    "# Caused by: java.lang.RuntimeException: Failed to parse a value for data type\n",
    "#   bigint (current token: VALUE_STRING)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa58f4f6",
   "metadata": {},
   "source": [
    "## Defining your schema in JSON\n",
    "\n",
    "StructType comes with two methods for exporting its content into a JSON-esque format.\n",
    "1. `json()` outputs a string containing the json formatted schema\n",
    "2. `jsonValue()` returns the schema as a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "b10d96be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fields': [{'metadata': {},\n",
      "             'name': 'schedule',\n",
      "             'nullable': True,\n",
      "             'type': {'fields': [{'metadata': {},\n",
      "                                  'name': 'days',\n",
      "                                  'nullable': True,\n",
      "                                  'type': {'containsNull': True,\n",
      "                                           'elementType': 'string',\n",
      "                                           'type': 'array'}},\n",
      "                                 {'metadata': {},\n",
      "                                  'name': 'time',\n",
      "                                  'nullable': True,\n",
      "                                  'type': 'string'}],\n",
      "                      'type': 'struct'}}],\n",
      " 'type': 'struct'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(shows_with_schema.select('schedule').schema.jsonValue())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4b76b9",
   "metadata": {},
   "source": [
    "You can use `jsonValue` on complex schema to see its JSON representation. This is helpful when trying to remember a complex schema:\n",
    "\n",
    "### Array types\n",
    "  1. `containsNull`,\n",
    "  2. `elementType`,\n",
    "  3. `type` (always array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "888de508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metadata': {},\n",
      " 'name': 'array_example',\n",
      " 'nullable': True,\n",
      " 'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}}\n"
     ]
    }
   ],
   "source": [
    "pprint(T.StructField(\"array_example\", T.ArrayType(T.StringType())).jsonValue())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d5cbcb",
   "metadata": {},
   "source": [
    "### Map types\n",
    "\n",
    "1. `keyType`\n",
    "2. `type` (always map)\n",
    "3. `valueContainsNull`\n",
    "2. `valueType`\n",
    "3. `keyType`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "0a5cf050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metadata': {},\n",
      " 'name': 'map_example',\n",
      " 'nullable': True,\n",
      " 'type': {'keyType': 'string',\n",
      "          'type': 'map',\n",
      "          'valueContainsNull': True,\n",
      "          'valueType': 'long'}}\n"
     ]
    }
   ],
   "source": [
    "# Example 1\n",
    "pprint(\n",
    "    T.StructField(\"map_example\", T.MapType(T.StringType(), T.LongType())).jsonValue()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "95018e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fields': [{'metadata': {},\n",
      "             'name': 'map_example',\n",
      "             'nullable': True,\n",
      "             'type': {'keyType': 'string',\n",
      "                      'type': 'map',\n",
      "                      'valueContainsNull': True,\n",
      "                      'valueType': 'long'}},\n",
      "            {'metadata': {},\n",
      "             'name': 'array_example',\n",
      "             'nullable': True,\n",
      "             'type': {'containsNull': True,\n",
      "                      'elementType': 'string',\n",
      "                      'type': 'array'}}],\n",
      " 'type': 'struct'}\n"
     ]
    }
   ],
   "source": [
    "# With both\n",
    "pprint(\n",
    "    T.StructType(\n",
    "        [\n",
    "            T.StructField(\"map_example\", T.MapType(T.StringType(), T.LongType())),\n",
    "            T.StructField(\"array_example\", T.ArrayType(T.StringType())),\n",
    "        ]\n",
    "    ).jsonValue()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f33a67",
   "metadata": {},
   "source": [
    "> Finally, we can close the loop by making sure that our JSON-schema is consistent with the one currently being used. For this, we’ll export the schema of shows_with_schema in a JSON string, load it as a JSON object and then use StructType.fromJson() method to re-create the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "b93861c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "other_shows_schema = T.StructType.fromJson(json.loads(shows_with_schema.schema.json()))\n",
    "\n",
    "print(other_shows_schema == shows_with_schema.schema)  # True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7785b407",
   "metadata": {},
   "source": [
    "## Reducing duplicate data with complex data types\n",
    "\n",
    "### Hierarchichal vs 2-D row-column models\n",
    "\n",
    "If we were to make the `shows` data frame in a traditional relational database, we could have a `shows` table linked to an `episodes` table using a star schema.\n",
    "\n",
    " `shows` table\n",
    "\n",
    "| show_id | name           |\n",
    "|---------|----------------|\n",
    "| 143     | silicon valley |\n",
    "\n",
    "`episodes` table, joined to `shows` by `show_id`\n",
    "\n",
    "| show_id | episode_id     | name           |\n",
    "|---------|----------------|----------------|\n",
    "| 143     | 1 | Minimal Viable Product |\n",
    "| 143     | 2 | The Cap Table |\n",
    "| 143     | 3 | Articles of Incorporation |\n",
    "\n",
    "`episodes` could be extended with more columns, but starts to have duplicate entries\n",
    "\n",
    "| show_id | episode_id     | name           | genre           | day           |\n",
    "|---------|----------------|----------------|----------------|----------------|\n",
    "| 143     | 1 | Minimal Viable Product | Comedy | Sunday |\n",
    "| 143     | 2 | The Cap Table | Comedy | Sunday |\n",
    "| 143     | 3 | Articles of Incorporation | Comedy | Sunday |\n",
    "\n",
    "\n",
    "In contrast, a hierarchichal data frame contains complex columns with arrays and struct columns:\n",
    "- each record represents a show;\n",
    "- a show has multiple episodes (array of structs column);\n",
    "- each episode has many fields (struct column within the array);\n",
    "- each show can have multiple genres (array of string column)\n",
    "- each show has a schedule (struct column);\n",
    "- each schedule belonging to a show can have multiple days (array), but a single time (string).\n",
    "\n",
    "\n",
    "#### `shows` data frame using a hierarchical model\n",
    "\n",
    "![](./notes/img/hier_df.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79776d0",
   "metadata": {},
   "source": [
    "## How to use `explode` and `collect` operations to go from hierarchical to tabular and back\n",
    "\n",
    "> We will now revisit the exploding operation by generalizing it to the map, looking at the behavior when your data frame has multiple columns, and see the different options PySpark provided with exploding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "649cf0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- episodes: struct (nullable = true)\n",
      " |    |-- _links: struct (nullable = true)\n",
      " |    |    |-- self: struct (nullable = true)\n",
      " |    |    |    |-- href: string (nullable = true)\n",
      " |    |-- airdate: string (nullable = true)\n",
      " |    |-- airstamp: timestamp (nullable = true)\n",
      " |    |-- airtime: string (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- image: struct (nullable = true)\n",
      " |    |    |-- medium: string (nullable = true)\n",
      " |    |    |-- original: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- number: long (nullable = true)\n",
      " |    |-- runtime: long (nullable = true)\n",
      " |    |-- season: long (nullable = true)\n",
      " |    |-- summary: string (nullable = true)\n",
      " |    |-- url: string (nullable = true)\n",
      "\n",
      "+---+--------------------+\n",
      "| id|            episodes|\n",
      "+---+--------------------+\n",
      "|143|[[[http://api.tvm...|\n",
      "|143|[[[http://api.tvm...|\n",
      "|143|[[[http://api.tvm...|\n",
      "|143|[[[http://api.tvm...|\n",
      "|143|[[[http://api.tvm...|\n",
      "+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exploding _embeedded.episodes\n",
    "episodes = shows.select(\"id\", F.explode(\"_embedded.episodes\").alias(\"episodes\"))\n",
    "episodes.printSchema()\n",
    "episodes.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8b0ed7",
   "metadata": {},
   "source": [
    "### Exploding a `map`\n",
    "\n",
    "- keys and values exploded in two different fields\n",
    "- `posexplode`: explodes the column and also returns an additional column before the data that contains the array positions (LongType).\n",
    "- `explode` / `posexplode` skips null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "78d1e0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-------------------------+\n",
      "|position|id   |name                     |\n",
      "+--------+-----+-------------------------+\n",
      "|0       |10897|Minimum Viable Product   |\n",
      "|1       |10898|The Cap Table            |\n",
      "|2       |10899|Articles of Incorporation|\n",
      "|3       |10900|Fiduciary Duties         |\n",
      "|4       |10901|Signaling Risk           |\n",
      "+--------+-----+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "episode_name_id = shows.select(\n",
    "    F.map_from_arrays(\n",
    "        F.col(\"_embedded.episodes.id\"), F.col(\"_embedded.episodes.name\")\n",
    "    ).alias(\"name_id\")\n",
    ")\n",
    "\n",
    "episode_name_id = episode_name_id.select(\n",
    "    F.posexplode(\"name_id\").alias(\"position\", \"id\", \"name\")\n",
    ")\n",
    "\n",
    "episode_name_id.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9dacbe",
   "metadata": {},
   "source": [
    "### `collect`-ing records into a complex column\n",
    "\n",
    "#### `collect_list()` and `collect_set()`\n",
    "\n",
    "- takes column as arg, returns an array column\n",
    "- collect_list = 1 array per column record\n",
    "- collect_set = 1 array per distinct column record (like Python set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "d61bce05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- episodes: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = false)\n",
      " |    |    |-- _links: struct (nullable = true)\n",
      " |    |    |    |-- self: struct (nullable = true)\n",
      " |    |    |    |    |-- href: string (nullable = true)\n",
      " |    |    |-- airdate: string (nullable = true)\n",
      " |    |    |-- airstamp: timestamp (nullable = true)\n",
      " |    |    |-- airtime: string (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- image: struct (nullable = true)\n",
      " |    |    |    |-- medium: string (nullable = true)\n",
      " |    |    |    |-- original: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- number: long (nullable = true)\n",
      " |    |    |-- runtime: long (nullable = true)\n",
      " |    |    |-- season: long (nullable = true)\n",
      " |    |    |-- summary: string (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "collected = episodes.groupby(\"id\").agg(F.collect_list(\"episodes\").alias(\"episodes\"))\n",
    "print(collected.count())\n",
    "collected.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01830e37",
   "metadata": {},
   "source": [
    "### Building your own hierarchies with `struct()`\n",
    "\n",
    "`struct()` function takess columns as params, and returns struct column containing the columns passed as params as fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "9071937e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|info             |\n",
      "+-----------------+\n",
      "|[Ended, 96, true]|\n",
      "+-----------------+\n",
      "\n",
      "root\n",
      " |-- info: struct (nullable = false)\n",
      " |    |-- status: string (nullable = true)\n",
      " |    |-- weight: long (nullable = true)\n",
      " |    |-- has_watched: boolean (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a struct column\n",
    "\n",
    "struct_ex = shows.select(\n",
    "    F.struct(\n",
    "        F.col(\"status\"), F.col(\"weight\"), F.lit(True).alias(\"has_watched\")\n",
    "    ).alias(\"info\")\n",
    ")\n",
    "\n",
    "struct_ex.show(1, False)\n",
    "\n",
    "struct_ex.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "fd9bcb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _embedded: struct (nullable = true)\n",
      " |    |-- episodes: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- _links: struct (nullable = true)\n",
      " |    |    |    |    |-- self: struct (nullable = true)\n",
      " |    |    |    |    |    |-- href: string (nullable = true)\n",
      " |    |    |    |-- airdate: string (nullable = true)\n",
      " |    |    |    |-- airstamp: timestamp (nullable = true)\n",
      " |    |    |    |-- airtime: string (nullable = true)\n",
      " |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |-- image: struct (nullable = true)\n",
      " |    |    |    |    |-- medium: string (nullable = true)\n",
      " |    |    |    |    |-- original: string (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- number: long (nullable = true)\n",
      " |    |    |    |-- runtime: long (nullable = true)\n",
      " |    |    |    |-- season: long (nullable = true)\n",
      " |    |    |    |-- summary: string (nullable = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      " |-- _links: struct (nullable = true)\n",
      " |    |-- previousepisode: struct (nullable = true)\n",
      " |    |    |-- href: string (nullable = true)\n",
      " |    |-- self: struct (nullable = true)\n",
      " |    |    |-- href: string (nullable = true)\n",
      " |-- externals: struct (nullable = true)\n",
      " |    |-- imdb: string (nullable = true)\n",
      " |    |-- thetvdb: long (nullable = true)\n",
      " |    |-- tvrage: long (nullable = true)\n",
      " |-- genres: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- medium: string (nullable = true)\n",
      " |    |-- original: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- network: struct (nullable = true)\n",
      " |    |-- country: struct (nullable = true)\n",
      " |    |    |-- code: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- timezone: string (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |-- officialSite: string (nullable = true)\n",
      " |-- premiered: string (nullable = true)\n",
      " |-- rating: struct (nullable = true)\n",
      " |    |-- average: double (nullable = true)\n",
      " |-- runtime: long (nullable = true)\n",
      " |-- schedule: struct (nullable = true)\n",
      " |    |-- days: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- time: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- updated: long (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- webChannel: string (nullable = true)\n",
      " |-- weight: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shows.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e29e80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48ccab8d",
   "metadata": {},
   "source": [
    "## Chapter 7: Bilingual PySpark: blending Python and SQL\n",
    "\n",
    "> This chapter is dedicated to using SQL with, and on top of PySpark. I cover how we can move from one language to the other. I also cover how we can use a SQL-like syntax within data frame methods to speed up your code and some of trade-offs you can face. Finally, we blend Python and SQL code together to get the best of both worlds.\n",
    "\n",
    "## Summary\n",
    "- Spark provides an SQL API for data manipulation. This API supports ANSI SQL.\n",
    "- PySpark’s data frames need to be registered as views or tables before they can be queried with Spark SQL. You can give them a different name than the data frame you’re registering.\n",
    "- Spark SQL queries can be inserted in a PySpark program through the spark.sql function, where spark is the running SparkSession.\n",
    "- Spark SQL tables references are kept in a Catalog which contains the metadata for all tables accessible to Spark SQL.\n",
    "- PySpark will accept SQL-style clauses in `where()` , `expr()` and `selectExpr()`, which can simplify the syntax for complex filtering and selection.\n",
    "- When using Spark SQL queries with user-provided input, be careful about sanitizing the inputs to avoid potential SQL injection attacks.\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "We will be using a periodic table of elements database for the initial section, followed by a public data set provided by BackBlaze, which provides hard drive data and statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d7c9882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import numpy as np\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e0d82b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- AtomicNumber: integer (nullable = true)\n",
      " |-- Element: string (nullable = true)\n",
      " |-- Symbol: string (nullable = true)\n",
      " |-- AtomicMass: double (nullable = true)\n",
      " |-- NumberofNeutrons: integer (nullable = true)\n",
      " |-- NumberofProtons: integer (nullable = true)\n",
      " |-- NumberofElectrons: integer (nullable = true)\n",
      " |-- Period: integer (nullable = true)\n",
      " |-- Group: integer (nullable = true)\n",
      " |-- Phase: string (nullable = true)\n",
      " |-- Radioactive: string (nullable = true)\n",
      " |-- Natural: string (nullable = true)\n",
      " |-- Metal: string (nullable = true)\n",
      " |-- Nonmetal: string (nullable = true)\n",
      " |-- Metalloid: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- AtomicRadius: double (nullable = true)\n",
      " |-- Electronegativity: double (nullable = true)\n",
      " |-- FirstIonization: double (nullable = true)\n",
      " |-- Density: double (nullable = true)\n",
      " |-- MeltingPoint: double (nullable = true)\n",
      " |-- BoilingPoint: double (nullable = true)\n",
      " |-- NumberOfIsotopes: integer (nullable = true)\n",
      " |-- Discoverer: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- SpecificHeat: double (nullable = true)\n",
      " |-- NumberofShells: integer (nullable = true)\n",
      " |-- NumberofValence: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in table of elements data\n",
    "elements = spark.read.csv(\n",
    "    \"data/Ch07/Periodic_Table_Of_Elements.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ")\n",
    "\n",
    "# Inspect the data frame\n",
    "elements.printSchema()\n",
    "\n",
    "# View the data frame in chunks of 3-4 columns\n",
    "# column_split = np.array_split(np.array(elements.columns), len(elements.columns) // 3)\n",
    "\n",
    "# for x in column_split:\n",
    "#     elements.select(*x).show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23e20f1",
   "metadata": {},
   "source": [
    "## `pyspark.sql` vs SQL\n",
    "\n",
    "### Order of execution\n",
    "\n",
    "![](./notes/img/order.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a825a643",
   "metadata": {},
   "source": [
    "The code below selects the `phrase` column that contain `\"liq\"`, then runs groupby and count.\n",
    "\n",
    "SQL equivalent would be:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "  period,\n",
    "  count(*)\n",
    "FROM elements\n",
    "WHERE phase = \"liq\"\n",
    "GROUP BY period;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e35848dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|period|count|\n",
      "+------+-----+\n",
      "|     6|    1|\n",
      "|     4|    1|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "elements.where(F.col(\"phase\") == \"liq\").groupby(\"period\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdb1127",
   "metadata": {},
   "source": [
    "### Using SQL queries on a data frame\n",
    "\n",
    "- In order to allow a data frame to be queried via SQL, we need to _register_ them as tables.\n",
    "- Spark SQL does not have visibility over the variables Python assigns.\n",
    "- Use `createOrReplaceTempView()` to read a data frame and create a Spark SQL reference.  Functionally equivalent to `CREATE_OR_REPLACE_VIEW` in SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "135b72b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table or view not found: elements; line 1 pos 29;\n",
      "'Aggregate ['period], ['period, unresolvedalias(count(1), None)]\n",
      "+- 'Filter ('phase = liq)\n",
      "   +- 'UnresolvedRelation [elements]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Directly querying a data frame SQL-style does not work\n",
    "try:\n",
    "    spark.sql(\n",
    "        \"select period, count(*) from elements where phase='liq' group by period\"\n",
    "    ).show(5)\n",
    "except AnalysisException as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4ef2af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|period|count(1)|\n",
      "+------+--------+\n",
      "|     6|       1|\n",
      "|     4|       1|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using createOrReplaceTempView\n",
    "\n",
    "elements.createOrReplaceTempView(\"elements\")\n",
    "\n",
    "spark.sql(\n",
    "    \"select period, count(*) from elements where phase='liq' group by period\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ce6835",
   "metadata": {},
   "source": [
    "### Table vs View concept\n",
    "\n",
    "> In SQL, they are distinct concepts: the table is materialized in memory and the view is computed on the fly. Spark’s temp views are conceptually closer to a view than a table. Spark SQL also has tables but we will not be using them, preferring reading and materializing our data into a data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e3d8f4",
   "metadata": {},
   "source": [
    "## Using the Spark catalog for multiple views\n",
    "\n",
    "- Spark catalog mainly deals with managing metadata of multiple SQL tables, and their level of caching.\n",
    "- Catalogs manages views we've registered and drops them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9930edb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='elements', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instantiate\n",
    "spark.catalog\n",
    "\n",
    "# List tables we've registered\n",
    "display(spark.catalog.listTables())\n",
    "\n",
    "# Drop a table\n",
    "spark.catalog.dropTempView(\"elements\")\n",
    "display(spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42c547c",
   "metadata": {},
   "source": [
    "### Data Source - Backblaze Data Set\n",
    "\n",
    "(Note: Only reading in Q3 data due to local compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "020751d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read backblaze data set into a data frame and register a SQL view\n",
    "\n",
    "DATA_DIRECTORY = \"./data/Ch07/\"\n",
    "\n",
    "# q1 = spark.read.csv(\n",
    "#     DATA_DIRECTORY + \"drive_stats_2019_Q1\", header=True, inferSchema=True\n",
    "# )\n",
    "# q2 = spark.read.csv(\n",
    "#     DATA_DIRECTORY + \"data_Q2_2019\", header=True, inferSchema=True\n",
    "# )\n",
    "q3 = spark.read.csv(\n",
    "    DATA_DIRECTORY + \"data_Q3_2019\", header=True, inferSchema=True\n",
    ")\n",
    "# q4 = spark.read.csv(\n",
    "#     DATA_DIRECTORY + \"data_Q4_2019\", header=True, inferSchema=True\n",
    "# )\n",
    "\n",
    "# Q4 has two more fields than the rest\n",
    "\n",
    "# q4_fields_extra = set(q4.columns) - set(q1.columns)\n",
    "\n",
    "# for i in q4_fields_extra:\n",
    "#     q1 = q1.withColumn(i, F.lit(None).cast(T.StringType()))\n",
    "#     q2 = q2.withColumn(i, F.lit(None).cast(T.StringType()))\n",
    "#     q3 = q3.withColumn(i, F.lit(None).cast(T.StringType()))\n",
    "\n",
    "\n",
    "# Union the data frames\n",
    "\n",
    "# if you are only using the minimal set of data, use this version\n",
    "backblaze_2019 = q3\n",
    "\n",
    "# if you are using the full set of data, use this version\n",
    "# backblaze_2019 = (\n",
    "#     q1.select(q4.columns)\n",
    "#     .union(q2.select(q4.columns))\n",
    "#     .union(q3.select(q4.columns))\n",
    "#     .union(q4)\n",
    "# )\n",
    "\n",
    "# Setting the layout for each column according to the schema\n",
    "q = backblaze_2019.select(\n",
    "    [\n",
    "        F.col(x).cast(T.LongType()) if x.startswith(\"smart\") else F.col(x)\n",
    "        for x in backblaze_2019.columns\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Register the view\n",
    "backblaze_2019.createOrReplaceTempView(\"backblaze_stats_2019\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd52eba9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- serial_number: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- capacity_bytes: long (nullable = true)\n",
      " |-- failure: integer (nullable = true)\n",
      " |-- smart_1_normalized: integer (nullable = true)\n",
      " |-- smart_1_raw: integer (nullable = true)\n",
      " |-- smart_2_normalized: integer (nullable = true)\n",
      " |-- smart_2_raw: integer (nullable = true)\n",
      " |-- smart_3_normalized: integer (nullable = true)\n",
      " |-- smart_3_raw: integer (nullable = true)\n",
      " |-- smart_4_normalized: integer (nullable = true)\n",
      " |-- smart_4_raw: integer (nullable = true)\n",
      " |-- smart_5_normalized: integer (nullable = true)\n",
      " |-- smart_5_raw: integer (nullable = true)\n",
      " |-- smart_7_normalized: integer (nullable = true)\n",
      " |-- smart_7_raw: long (nullable = true)\n",
      " |-- smart_8_normalized: integer (nullable = true)\n",
      " |-- smart_8_raw: integer (nullable = true)\n",
      " |-- smart_9_normalized: integer (nullable = true)\n",
      " |-- smart_9_raw: integer (nullable = true)\n",
      " |-- smart_10_normalized: integer (nullable = true)\n",
      " |-- smart_10_raw: integer (nullable = true)\n",
      " |-- smart_11_normalized: integer (nullable = true)\n",
      " |-- smart_11_raw: integer (nullable = true)\n",
      " |-- smart_12_normalized: integer (nullable = true)\n",
      " |-- smart_12_raw: integer (nullable = true)\n",
      " |-- smart_13_normalized: string (nullable = true)\n",
      " |-- smart_13_raw: string (nullable = true)\n",
      " |-- smart_15_normalized: string (nullable = true)\n",
      " |-- smart_15_raw: string (nullable = true)\n",
      " |-- smart_16_normalized: integer (nullable = true)\n",
      " |-- smart_16_raw: integer (nullable = true)\n",
      " |-- smart_17_normalized: integer (nullable = true)\n",
      " |-- smart_17_raw: integer (nullable = true)\n",
      " |-- smart_22_normalized: integer (nullable = true)\n",
      " |-- smart_22_raw: integer (nullable = true)\n",
      " |-- smart_23_normalized: integer (nullable = true)\n",
      " |-- smart_23_raw: integer (nullable = true)\n",
      " |-- smart_24_normalized: integer (nullable = true)\n",
      " |-- smart_24_raw: integer (nullable = true)\n",
      " |-- smart_168_normalized: integer (nullable = true)\n",
      " |-- smart_168_raw: integer (nullable = true)\n",
      " |-- smart_170_normalized: integer (nullable = true)\n",
      " |-- smart_170_raw: long (nullable = true)\n",
      " |-- smart_173_normalized: integer (nullable = true)\n",
      " |-- smart_173_raw: long (nullable = true)\n",
      " |-- smart_174_normalized: integer (nullable = true)\n",
      " |-- smart_174_raw: integer (nullable = true)\n",
      " |-- smart_177_normalized: integer (nullable = true)\n",
      " |-- smart_177_raw: integer (nullable = true)\n",
      " |-- smart_179_normalized: string (nullable = true)\n",
      " |-- smart_179_raw: string (nullable = true)\n",
      " |-- smart_181_normalized: string (nullable = true)\n",
      " |-- smart_181_raw: string (nullable = true)\n",
      " |-- smart_182_normalized: string (nullable = true)\n",
      " |-- smart_182_raw: string (nullable = true)\n",
      " |-- smart_183_normalized: integer (nullable = true)\n",
      " |-- smart_183_raw: integer (nullable = true)\n",
      " |-- smart_184_normalized: integer (nullable = true)\n",
      " |-- smart_184_raw: integer (nullable = true)\n",
      " |-- smart_187_normalized: integer (nullable = true)\n",
      " |-- smart_187_raw: integer (nullable = true)\n",
      " |-- smart_188_normalized: integer (nullable = true)\n",
      " |-- smart_188_raw: long (nullable = true)\n",
      " |-- smart_189_normalized: integer (nullable = true)\n",
      " |-- smart_189_raw: integer (nullable = true)\n",
      " |-- smart_190_normalized: integer (nullable = true)\n",
      " |-- smart_190_raw: integer (nullable = true)\n",
      " |-- smart_191_normalized: integer (nullable = true)\n",
      " |-- smart_191_raw: integer (nullable = true)\n",
      " |-- smart_192_normalized: integer (nullable = true)\n",
      " |-- smart_192_raw: integer (nullable = true)\n",
      " |-- smart_193_normalized: integer (nullable = true)\n",
      " |-- smart_193_raw: integer (nullable = true)\n",
      " |-- smart_194_normalized: integer (nullable = true)\n",
      " |-- smart_194_raw: integer (nullable = true)\n",
      " |-- smart_195_normalized: integer (nullable = true)\n",
      " |-- smart_195_raw: integer (nullable = true)\n",
      " |-- smart_196_normalized: integer (nullable = true)\n",
      " |-- smart_196_raw: integer (nullable = true)\n",
      " |-- smart_197_normalized: integer (nullable = true)\n",
      " |-- smart_197_raw: integer (nullable = true)\n",
      " |-- smart_198_normalized: integer (nullable = true)\n",
      " |-- smart_198_raw: integer (nullable = true)\n",
      " |-- smart_199_normalized: integer (nullable = true)\n",
      " |-- smart_199_raw: integer (nullable = true)\n",
      " |-- smart_200_normalized: integer (nullable = true)\n",
      " |-- smart_200_raw: integer (nullable = true)\n",
      " |-- smart_201_normalized: string (nullable = true)\n",
      " |-- smart_201_raw: string (nullable = true)\n",
      " |-- smart_218_normalized: integer (nullable = true)\n",
      " |-- smart_218_raw: integer (nullable = true)\n",
      " |-- smart_220_normalized: integer (nullable = true)\n",
      " |-- smart_220_raw: integer (nullable = true)\n",
      " |-- smart_222_normalized: integer (nullable = true)\n",
      " |-- smart_222_raw: integer (nullable = true)\n",
      " |-- smart_223_normalized: integer (nullable = true)\n",
      " |-- smart_223_raw: integer (nullable = true)\n",
      " |-- smart_224_normalized: integer (nullable = true)\n",
      " |-- smart_224_raw: integer (nullable = true)\n",
      " |-- smart_225_normalized: integer (nullable = true)\n",
      " |-- smart_225_raw: integer (nullable = true)\n",
      " |-- smart_226_normalized: integer (nullable = true)\n",
      " |-- smart_226_raw: integer (nullable = true)\n",
      " |-- smart_231_normalized: integer (nullable = true)\n",
      " |-- smart_231_raw: long (nullable = true)\n",
      " |-- smart_232_normalized: integer (nullable = true)\n",
      " |-- smart_232_raw: long (nullable = true)\n",
      " |-- smart_233_normalized: integer (nullable = true)\n",
      " |-- smart_233_raw: integer (nullable = true)\n",
      " |-- smart_235_normalized: integer (nullable = true)\n",
      " |-- smart_235_raw: long (nullable = true)\n",
      " |-- smart_240_normalized: integer (nullable = true)\n",
      " |-- smart_240_raw: long (nullable = true)\n",
      " |-- smart_241_normalized: integer (nullable = true)\n",
      " |-- smart_241_raw: long (nullable = true)\n",
      " |-- smart_242_normalized: integer (nullable = true)\n",
      " |-- smart_242_raw: long (nullable = true)\n",
      " |-- smart_250_normalized: integer (nullable = true)\n",
      " |-- smart_250_raw: integer (nullable = true)\n",
      " |-- smart_251_normalized: integer (nullable = true)\n",
      " |-- smart_251_raw: integer (nullable = true)\n",
      " |-- smart_252_normalized: integer (nullable = true)\n",
      " |-- smart_252_raw: integer (nullable = true)\n",
      " |-- smart_254_normalized: integer (nullable = true)\n",
      " |-- smart_254_raw: integer (nullable = true)\n",
      " |-- smart_255_normalized: string (nullable = true)\n",
      " |-- smart_255_raw: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "backblaze_2019.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d535e9",
   "metadata": {},
   "source": [
    "## `select` and `where`\n",
    "\n",
    "Use select and where to show a few hard drives serial numbers that have failed at some point (failure = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09ed49e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+--------------+\n",
      "|serial_number|        model|capacity_bytes|\n",
      "+-------------+-------------+--------------+\n",
      "|     ZA10MCJ5|  ST8000DM002| 8001563222016|\n",
      "|     ZCH07T9K|ST12000NM0007|12000138625024|\n",
      "|     ZCH0CA7Z|ST12000NM0007|12000138625024|\n",
      "|     Z302F381|  ST4000DM000| 4000787030016|\n",
      "|     ZCH0B3Z2|ST12000NM0007|12000138625024|\n",
      "+-------------+-------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------+-------------+--------------+\n",
      "|serial_number|        model|capacity_bytes|\n",
      "+-------------+-------------+--------------+\n",
      "|     ZA10MCJ5|  ST8000DM002| 8001563222016|\n",
      "|     ZCH07T9K|ST12000NM0007|12000138625024|\n",
      "|     ZCH0CA7Z|ST12000NM0007|12000138625024|\n",
      "|     Z302F381|  ST4000DM000| 4000787030016|\n",
      "|     ZCH0B3Z2|ST12000NM0007|12000138625024|\n",
      "+-------------+-------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL order of operations: 1) select columns, then 2) filter \n",
    "spark.sql(\"select serial_number, model, capacity_bytes from backblaze_stats_2019 where failure = 1\").show(5)\n",
    "\n",
    "# PySpark order of operations: 1) filter, then 2) select columns\n",
    "backblaze_2019.where(\"failure=1\").select(\n",
    "    F.col('serial_number'),\n",
    "    F.col('model'),\n",
    "    F.col('capacity_bytes')\n",
    ").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1824138",
   "metadata": {},
   "source": [
    "## `groupby` and `orderby`\n",
    "\n",
    "Look at the capacity in gigabytes of the hard drives included in the data, by model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b623e438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|               model|              min_GB| max_GB|\n",
      "+--------------------+--------------------+-------+\n",
      "| TOSHIBA MG07ACA14TA|             13039.0|13039.0|\n",
      "|HGST HUH721212ALE600|             11176.0|11176.0|\n",
      "|       ST12000NM0117|             11176.0|11176.0|\n",
      "|       ST12000NM0007|-9.31322574615478...|11176.0|\n",
      "|HGST HUH721212ALN604|-9.31322574615478...|11176.0|\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Groupby and order in SQL\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        model,\n",
    "        min(capacity_bytes / pow(1024, 3)) min_GB,\n",
    "        max(capacity_bytes / pow(1024, 3)) max_GB\n",
    "    FROM backblaze_stats_2019\n",
    "    GROUP BY model\n",
    "    ORDER BY max_GB DESC\n",
    "\"\"\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba34de23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|               model|              min_GB| max_GB|\n",
      "+--------------------+--------------------+-------+\n",
      "| TOSHIBA MG07ACA14TA|             13039.0|13039.0|\n",
      "|HGST HUH721212ALE600|             11176.0|11176.0|\n",
      "|       ST12000NM0117|             11176.0|11176.0|\n",
      "|       ST12000NM0007|-9.31322574615478...|11176.0|\n",
      "|HGST HUH721212ALN604|-9.31322574615478...|11176.0|\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySpark\n",
    "backblaze_2019.groupby(F.col(\"model\")).agg(\n",
    "    F.min(F.col(\"capacity_bytes\") / F.pow(F.lit(1024), 3)).alias(\"min_GB\"),\n",
    "    F.max(F.col(\"capacity_bytes\") / F.pow(F.lit(1024), 3)).alias(\"max_GB\"),\n",
    ").orderBy(F.col(\"max_GB\"), ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc987fe9",
   "metadata": {},
   "source": [
    "## Filtering after grouping with `having`\n",
    "\n",
    "`having` in SQL is a condition block used after grouping is done.\n",
    "\n",
    "Filter the groupby with only those that have different min_GB and max_GB numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e65fcbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------+\n",
      "|               model|              min_GB|           max_GB|\n",
      "+--------------------+--------------------+-----------------+\n",
      "|       ST12000NM0007|-9.31322574615478...|          11176.0|\n",
      "|HGST HUH721212ALN604|-9.31322574615478...|          11176.0|\n",
      "|HGST HUH721010ALE600|-9.31322574615478...|           9314.0|\n",
      "|       ST10000NM0086|-9.31322574615478...|           9314.0|\n",
      "|         ST8000DM002|-9.31322574615478...|7452.036460876465|\n",
      "+--------------------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        model,\n",
    "        min(capacity_bytes / pow(1024, 3)) min_GB,\n",
    "        max(capacity_bytes / pow(1024, 3)) max_GB\n",
    "    FROM backblaze_stats_2019\n",
    "    GROUP BY model\n",
    "    HAVING min_GB <> max_GB\n",
    "    ORDER BY max_GB DESC\n",
    "\"\"\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a0a974cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------+\n",
      "|               model|              min_GB|           max_GB|\n",
      "+--------------------+--------------------+-----------------+\n",
      "|       ST12000NM0007|-9.31322574615478...|          11176.0|\n",
      "|HGST HUH721212ALN604|-9.31322574615478...|          11176.0|\n",
      "|HGST HUH721010ALE600|-9.31322574615478...|           9314.0|\n",
      "|       ST10000NM0086|-9.31322574615478...|           9314.0|\n",
      "|         ST8000DM002|-9.31322574615478...|7452.036460876465|\n",
      "+--------------------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "backblaze_2019.groupby(F.col(\"model\")).agg(\n",
    "    F.min(F.col(\"capacity_bytes\") / F.pow(F.lit(1024), 3)).alias(\"min_GB\"),\n",
    "    F.max(F.col(\"capacity_bytes\") / F.pow(F.lit(1024), 3)).alias(\"max_GB\"),\n",
    ").where(F.col(\"min_GB\") != F.col(\"max_GB\")).orderBy(\n",
    "    F.col(\"max_GB\"), ascending=False\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceecbea0",
   "metadata": {},
   "source": [
    "## Saving tables/views using `create`\n",
    "\n",
    "- With SQL, prefix query with `CREATE TABLE/VIEW`\n",
    "    - creating a table will materialize the data\n",
    "    - creating a view will only keep the query\n",
    "- With PySpark, just save to variable\n",
    "\n",
    "Compute the number of days of operation a model has and the number of drive failures it has had"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "97d06794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SQL\n",
    "\n",
    "spark.catalog.dropTempView('drive_days')\n",
    "spark.catalog.dropTempView('failures')\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    CREATE TEMP VIEW drive_days AS\n",
    "        SELECT model, count(*) AS drive_days\n",
    "        FROM backblaze_stats_2019\n",
    "        GROUP BY model\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    CREATE TEMP VIEW failures AS\n",
    "        SELECT model, count(*) AS failures\n",
    "        FROM backblaze_stats_2019\n",
    "        WHERE failure = 1\n",
    "        GROUP BY model\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eeeb8557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark\n",
    "\n",
    "drive_days = backblaze_2019.groupBy(F.col(\"model\")).agg(\n",
    "    F.count(F.col(\"*\")).alias(\"drive_days\")\n",
    ")\n",
    "\n",
    "failures = (\n",
    "    backblaze_2019.where(F.col(\"failure\") == 1)\n",
    "    .groupBy(F.col(\"model\"))\n",
    "    .agg(F.count(F.col(\"*\")).alias(\"failures\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "122132e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+\n",
      "|              model|failures|\n",
      "+-------------------+--------+\n",
      "|        ST4000DM000|      72|\n",
      "|      ST12000NM0007|     365|\n",
      "|        ST8000DM005|       1|\n",
      "|TOSHIBA MQ01ABF050M|       5|\n",
      "|       ST8000NM0055|      50|\n",
      "+-------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "failures.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6a52ed",
   "metadata": {},
   "source": [
    "## Adding data to table using `UNION` and `JOIN`\n",
    "\n",
    "- SQL `UNION` removes duplicate records, while PySpark doesn't.  \n",
    "- PySpark `UNION` is equal to SQL `UNION ALL`\n",
    "- To get SQL `UNION` equivalent with PySpark, run `distinct()` after `union()`\n",
    "\n",
    "(Note: Not running 2 cells below since I only loaded Q3 data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d1649f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_backblaze = \", \".join(q4.columns)\n",
    "\n",
    "q1.createOrReplaceTempView(\"Q1\")\n",
    "q1.createOrReplaceTempView(\"Q2\")\n",
    "q1.createOrReplaceTempView(\"Q3\")\n",
    "q1.createOrReplaceTempView(\"Q4\")\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    CREATE VIEW backblaze_2019 AS\n",
    "    SELECT {col} FROM Q1 UNION ALL\n",
    "    SELECT {col} FROM Q2 UNION ALL\n",
    "    SELECT {col} FROM Q3 UNION ALL\n",
    "    SELECT {col} FROM Q4\n",
    "\"\"\".format(\n",
    "        col=columns_backblaze\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c61a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "backblaze_2019 = (\n",
    "    q1.select(q4.columns)\n",
    "    .union(q2.select(q4.columns))\n",
    "    .union(q3.select(q4.columns))\n",
    "    .union(q4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39e0882",
   "metadata": {},
   "source": [
    "Joining `drive_days` and `failures` tables together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "08e6da5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+--------+\n",
      "|        model|drive_days|failures|\n",
      "+-------------+----------+--------+\n",
      "|  ST9250315AS|        89|    null|\n",
      "|  ST4000DM000|   1796728|      72|\n",
      "|ST12000NM0007|   3212635|     365|\n",
      "|   ST320LT007|        89|    null|\n",
      "|  ST8000DM005|      2280|       1|\n",
      "+-------------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        drive_days.model,\n",
    "        drive_days,\n",
    "        failures\n",
    "    FROM drive_days\n",
    "    LEFT JOIN failures\n",
    "    ON\n",
    "        drive_days.model = failures.model\n",
    "\"\"\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "71cc5d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+--------+\n",
      "|        model|drive_days|failures|\n",
      "+-------------+----------+--------+\n",
      "|  ST9250315AS|        89|    null|\n",
      "|  ST4000DM000|   1796728|      72|\n",
      "|ST12000NM0007|   3212635|     365|\n",
      "|   ST320LT007|        89|    null|\n",
      "|  ST8000DM005|      2280|       1|\n",
      "+-------------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "drive_days.join(failures, on=\"model\", how=\"left\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c68b23",
   "metadata": {},
   "source": [
    "## Organizing code with subqueries and common table expressions (CTE)\n",
    "\n",
    "Take drive_days and failures table definitions and bundle them into a single query using CTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5198df07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               model|        failure_rate|\n",
      "+--------------------+--------------------+\n",
      "|       ST12000NM0117|0.019305019305019305|\n",
      "|Seagate BarraCuda...|6.341154090044388E-4|\n",
      "|  TOSHIBA MQ01ABF050|5.579360828423496E-4|\n",
      "|         ST8000DM005|4.385964912280702E-4|\n",
      "|          ST500LM030| 4.19639110365086E-4|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    WITH drive_days as (\n",
    "        SELECT\n",
    "            model,\n",
    "            count(*) AS drive_days\n",
    "        FROM backblaze_stats_2019\n",
    "        GROUP BY model),\n",
    "    failures as (\n",
    "        SELECT\n",
    "            model,\n",
    "            count(*) AS failures\n",
    "        FROM backblaze_stats_2019\n",
    "        WHERE failure = 1\n",
    "        GROUP BY model)\n",
    "        \n",
    "    SELECT\n",
    "        drive_days.model,\n",
    "        failures / drive_days failure_rate\n",
    "    FROM drive_days\n",
    "    INNER JOIN failures\n",
    "    ON drive_days.model = failures.model\n",
    "    ORDER BY failure_rate DESC\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d36fb811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------+--------------------+\n",
      "|               model|drive_days|failures|        failure_rate|\n",
      "+--------------------+----------+--------+--------------------+\n",
      "|       ST12000NM0117|       259|       5|0.019305019305019305|\n",
      "|Seagate BarraCuda...|      1577|       1|6.341154090044388E-4|\n",
      "|  TOSHIBA MQ01ABF050|     44808|      25|5.579360828423496E-4|\n",
      "|         ST8000DM005|      2280|       1|4.385964912280702E-4|\n",
      "|          ST500LM030|     21447|       9| 4.19639110365086E-4|\n",
      "+--------------------+----------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# CTE sort of similar to python functions\n",
    "def failure_rate(drive_stats):\n",
    "    drive_days = drive_stats.groupby(F.col(\"model\")).agg(\n",
    "        F.count(F.col(\"*\")).alias(\"drive_days\")\n",
    "    )\n",
    "\n",
    "    failures = (\n",
    "        drive_stats.where(F.col(\"failure\") == 1)\n",
    "        .groupby(F.col(\"model\"))\n",
    "        .agg(F.count(F.col(\"*\")).alias(\"failures\"))\n",
    "    )\n",
    "    answer = (\n",
    "        drive_days.join(failures, on=\"model\", how=\"inner\")\n",
    "        .withColumn(\"failure_rate\", F.col(\"failures\") / F.col(\"drive_days\"))\n",
    "        .orderBy(F.col(\"failure_rate\").desc())\n",
    "    )\n",
    "    return answer\n",
    "\n",
    "\n",
    "failure_rate(backblaze_2019).show(5)\n",
    "\n",
    "print(\"drive_days\" in dir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae88ddf2",
   "metadata": {},
   "source": [
    "## Mix and match PySpark and SQL code\n",
    "\n",
    "This section will build on the code we’ve written so far. We’re going to write a function that, for a given capacity, will return the top 3 most reliable drives according to our failure rate.\n",
    "\n",
    "`selectExpr()` is just like `select()`, but will process SQL-style operations. Nice because it removes `F.col` sort of syntax.\n",
    "\n",
    "`expr()` wraps SQL-style expression into a PySpark column.  Can use in lieu of `F.col()` when you want to modify a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c99300ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Ingestion using Python\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "DATA_DIRECTORY = \"./data/Ch07/\"\n",
    "\n",
    "DATA_FILES = [\n",
    "#     \"drive_stats_2019_Q1\",\n",
    "#     \"data_Q2_2019\",\n",
    "    \"data_Q3_2019\",\n",
    "#     \"data_Q4_2019\",\n",
    "]\n",
    "\n",
    "data = [\n",
    "    spark.read.csv(DATA_DIRECTORY + file, header=True, inferSchema=True)\n",
    "    for file in DATA_FILES\n",
    "]\n",
    "\n",
    "common_columns = list(\n",
    "    reduce(lambda x, y: x.intersection(y), [set(df.columns) for df in data])\n",
    ")\n",
    "\n",
    "assert set([\"model\", \"capacity_bytes\", \"date\", \"failure\"]).issubset(\n",
    "    set(common_columns)\n",
    ")\n",
    "\n",
    "full_data = reduce(\n",
    "    lambda x, y: x.select(common_columns).union(y.select(common_columns)), data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "561c7543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing data for the query function with selectExpr\n",
    "\n",
    "full_data = full_data.selectExpr( # <===\n",
    "    \"model\", \"capacity_bytes / pow(1024, 3) capacity_GB\", \"date\", \"failure\"\n",
    ")\n",
    "\n",
    "drive_days = full_data.groupby(\"model\", \"capacity_GB\").agg(\n",
    "    F.count(\"*\").alias(\"drive_days\")\n",
    ")\n",
    "\n",
    "failures = (\n",
    "    full_data.where(\"failure = 1\")\n",
    "    .groupby(\"model\", \"capacity_GB\")\n",
    "    .agg(F.count(\"*\").alias(\"failures\"))\n",
    ")\n",
    "\n",
    "summarized_data = (\n",
    "    drive_days.join(failures, on=[\"model\", \"capacity_GB\"], how=\"left\")\n",
    "    .fillna(0.0, [\"failures\"])\n",
    "    .selectExpr(\"model\", \"capacity_GB\", \"failures / drive_days failure_rate\")\n",
    "    .cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "698633f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating failures variable with expr\n",
    "\n",
    "failures = (\n",
    "    full_data.where(\"failure = 1\")\n",
    "    .groupby(\"model\", \"capacity_GB\")\n",
    "    .agg(F.expr(\"count(*) failures\")) # <===\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c617ff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning failure_rate in to a function using a mix of PySpark and SQL syntax\n",
    "\n",
    "def most_reliable_drive_for_capacity(data, capacity_GB=2048, precision=0.25, top_n=3):\n",
    "    \"\"\"Returns the top 3 drives for a given approximate capacity.\n",
    "\n",
    "    Given a capacity in GB and a precision as a decimal number, we keep the N\n",
    "    drives where:\n",
    "\n",
    "    - the capacity is between (capacity * 1/(1+precision)), capacity * (1+precision)\n",
    "    - the failure rate is the lowest\n",
    "\n",
    "    \"\"\"\n",
    "    capacity_min = capacity_GB / (1 + precision)\n",
    "    capacity_max = capacity_GB * (1 + precision)\n",
    "\n",
    "    answer = (\n",
    "        data.where(f\"capacity_GB between {capacity_min} and {capacity_max}\")\n",
    "        .orderBy(\"failure_rate\", \"capacity_GB\", ascending=[True, False])\n",
    "        .limit(top_n)\n",
    "    )\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030e1350",
   "metadata": {},
   "source": [
    "## Chapter 8: RDD and user-defined functions\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "- The resilient distributed dataset allows for better flexibility compared to the records and columns approach of the data frame.\n",
    "- The most low level and flexible way of running Python code within the distributed Spark environment is to use the RDD. With an RDD, you have no structure imposed on your data and need to manage type information into your program, and defensively code against potential exceptions.\n",
    "- The API for data processing on the RDD is heavily inspired by the MapReduce framework. You use higher order functions such as map(), filter() and reduce() on the objects of the RDD.\n",
    "- The data frame’s most basic Python code promotion functionality, called the (PySpark) UDF, emulates the \"map\" part of the RDD. You use it as a scalar function, taking Column objects as parameters and returning a single Column.\n",
    "\n",
    "\n",
    "## Terminology\n",
    "\n",
    "### Resilient Distributed Dataset (RDD)\n",
    "\n",
    "- Bag of elements, independent, no schema\n",
    "- Flexible with what you want to do but no safeguards\n",
    "\n",
    "### User-defined functions (UDF)\n",
    "\n",
    "- Simple way to promote Python functions to be used on a data frame.\n",
    "\n",
    "RDD's Pros\n",
    "\n",
    "1. When you have unordered collection of Python objects that can be pickled\n",
    "2. Unordered `key value` pairs i.e. Python dict\n",
    "\n",
    "\n",
    "## Example: Creating an RDD from a Python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "536f0a39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T17:44:16.031789Z",
     "start_time": "2021-05-31T17:44:16.023065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:262\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "collection = [1, \"two\", 3.0, (\"four\", 4), {\"five\": 5}]\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "collection_rdd = sc.parallelize(collection)\n",
    "\n",
    "print(collection_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70608d02",
   "metadata": {},
   "source": [
    "## Manipulating data with `map`, `filter` and `reduce`\n",
    "\n",
    "- Each take a function as their only param, ie. they are _higher-order functions_.\n",
    "\n",
    "### `map`\n",
    "\n",
    "- apply one function to every object\n",
    "- need to be careful with unsupported types on whatever function you're trying to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9b1c142",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T18:05:24.929778Z",
     "start_time": "2021-05-31T18:05:24.821124Z"
    }
   },
   "outputs": [],
   "source": [
    "# Map a simple function to each element to an RDD.\n",
    "# This will raise an error because not all of the elements are integers\n",
    "\n",
    "from py4j.protocol import Py4JJavaError\n",
    "import re\n",
    "\n",
    "\n",
    "def add_one(value):\n",
    "    return value + 1\n",
    "\n",
    "\n",
    "collection_rdd = collection_rdd.map(add_one)\n",
    "\n",
    "try:\n",
    "    print(collection_rdd.collect())\n",
    "except Py4JJavaError as e:\n",
    "    pass\n",
    "\n",
    "# Stack trace galore! The important bit, you'll get one of the following:\n",
    "# TypeError: can only concatenate str (not \"int\") to str\n",
    "# TypeError: unsupported operand type(s) for +: 'dict' and 'int'\n",
    "# TypeError: can only concatenate tuple (not \"int\") to tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "45411621",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T18:12:03.883151Z",
     "start_time": "2021-05-31T18:12:03.824825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:  [1, 'two', 3.0, ('four', 4), {'five': 5}]\n",
      "After :  [2, 'two', 4.0, ('four', 4), {'five': 5}]\n"
     ]
    }
   ],
   "source": [
    "# Safer option with a try/except inside the function\n",
    "def safer_add_one(value):\n",
    "    try:\n",
    "        return value + 1\n",
    "    except TypeError:\n",
    "        return value\n",
    "    \n",
    "# reset rdd\n",
    "collection_rdd = sc.parallelize(collection)\n",
    "print(\"Before: \", collection)\n",
    "\n",
    "# run safe adding method\n",
    "collection_rdd = collection_rdd.map(safer_add_one)\n",
    "print(\"After : \", collection_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd8c252",
   "metadata": {},
   "source": [
    "![](notes/img/rdd_error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37784c9b",
   "metadata": {},
   "source": [
    "### `filter`\n",
    "\n",
    "- Returns RDD element if True, else drops it.\n",
    "-  As a reminder, False, number 0, empty sequences and collections (list, tuple, dict, set, range) are falsey \n",
    "  - ref: https://docs.python.org/3/library/stdtypes.html#truth-value-testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2671b7f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T18:16:26.531609Z",
     "start_time": "2021-05-31T18:16:26.445077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3.0]\n",
      "['two']\n"
     ]
    }
   ],
   "source": [
    "# Filtering RDD with lambda function to keep only int and floats\n",
    "\n",
    "collection_rdd = sc.parallelize(collection)\n",
    "\n",
    "\n",
    "collection_rdd = collection_rdd.filter(lambda x: isinstance(x, (float, int)))\n",
    "print(collection_rdd.collect())\n",
    "\n",
    "\n",
    "# Alternative: Creating a separate function\n",
    "\n",
    "collection_rdd = sc.parallelize(collection)\n",
    "\n",
    "def is_string(elem):\n",
    "    return True if isinstance(elem, str) else False\n",
    "\n",
    "collection_rdd = collection_rdd.filter(is_string)\n",
    "print(collection_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc17715",
   "metadata": {},
   "source": [
    "## `reduce`\n",
    "\n",
    "- Used for summarization (ie. groupby and agg with dataframe)\n",
    "- Takes 2 elements and returns 1 element. If list > 2, will taking first 2 elements, then apply result again to third and so forth.\n",
    "\n",
    "![](notes/img/reduce.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "63fd0d6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T18:23:27.679935Z",
     "start_time": "2021-05-31T18:23:27.631762Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "# Add list of numbers through reduce\n",
    "\n",
    "from operator import add\n",
    "\n",
    "collection_rdd = sc.parallelize(range(10))\n",
    "print(collection_rdd.reduce(add))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f80d53",
   "metadata": {},
   "source": [
    "### Use _Commutative_ and _associate_ functions (for distributed computing)\n",
    "\n",
    "Only give `reduce` _commutative_ and _associate_ functions.\n",
    "\n",
    "- Commutative function: Function in which order of arguments doesn't matter\n",
    "- Associative function: Function in which grouping of arguments doesn't matter, \n",
    "  - `subtract` is not because `(a - b) - c != a - (b - c)`\n",
    "- `add`, `multiply`, `min` and `max` are both associative and commutative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bf5503",
   "metadata": {},
   "source": [
    "## User-defined Functions\n",
    "\n",
    "- UDFs allow you to implement custom functions on PySpark data frame columns\n",
    "\n",
    "### Using UDF to create a `fractions` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "92d719eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T23:03:44.918577Z",
     "start_time": "2021-05-31T23:03:44.503614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|fraction|\n",
      "+--------+\n",
      "|[0, 1]  |\n",
      "|[0, 2]  |\n",
      "|[0, 3]  |\n",
      "|[0, 4]  |\n",
      "|[0, 5]  |\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "fractions = [[x,y] for x in range(100) for y in range(1, 100)]\n",
    "frac_df = spark.createDataFrame(fractions, [\"numerator\", \"denominator\"])\n",
    "\n",
    "frac_df = frac_df.select(\n",
    "    F.array(F.col(\"numerator\"), F.col(\"denominator\")).alias(\"fraction\"),\n",
    ")\n",
    "\n",
    "frac_df.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2682fe2",
   "metadata": {},
   "source": [
    "### Using typed Python functions\n",
    "\n",
    "This section will create a function to reduce a fraction and one to transform a fraction into a floating-point number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6d49c36d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T23:07:46.677717Z",
     "start_time": "2021-05-31T23:07:46.673130Z"
    }
   },
   "outputs": [],
   "source": [
    "from fractions import Fraction\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "Frac = Tuple[int, int]\n",
    "\n",
    "def py_reduce_fraction(frac: Frac) -> Optional[Frac]:\n",
    "    \"\"\"Reduce a fraction represented as a 2-tuple of integers\"\"\"\n",
    "    num, denom = frac\n",
    "    if denom:\n",
    "        answer = Fraction(num, denom)\n",
    "        return answer.numerator, answer.denominator\n",
    "    return None\n",
    "\n",
    "assert py_reduce_fraction((3,6)) == (1, 2)\n",
    "assert py_reduce_fraction((1, 0)) is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "28d6565a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T23:08:53.251760Z",
     "start_time": "2021-05-31T23:08:53.248059Z"
    }
   },
   "outputs": [],
   "source": [
    "def py_fraction_to_float(frac: Frac) -> Optional[float]:\n",
    "    \"\"\"Transforms a fraction represented as a 2-tuple of integer into a float\"\"\"\n",
    "    num, denom = frac\n",
    "    if denom:\n",
    "        return num / denom\n",
    "    return None\n",
    "\n",
    "assert py_fraction_to_float((2, 8)) == 0.25\n",
    "assert py_fraction_to_float((10, 0)) is None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab473ae5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T23:22:00.678157Z",
     "start_time": "2021-05-31T23:22:00.675719Z"
    }
   },
   "source": [
    "### Promoting Python functions to `udf`\n",
    "\n",
    "The function takes two parameters.\n",
    "\n",
    "1. The function you want to promote.\n",
    "2. Optionally, the return type of the generated UDF.\n",
    "\n",
    "#### Option 1: Creating a UDF explicitily with `udf()` and apply it to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2cb2db71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T23:25:34.406890Z",
     "start_time": "2021-05-31T23:25:34.185345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+\n",
      "|fraction|reduced_fraction|\n",
      "+--------+----------------+\n",
      "|[0, 1]  |[0, 1]          |\n",
      "|[0, 2]  |[0, 1]          |\n",
      "|[0, 3]  |[0, 1]          |\n",
      "|[0, 4]  |[0, 1]          |\n",
      "|[0, 5]  |[0, 1]          |\n",
      "+--------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SparkFrac = T.ArrayType(T.LongType())\n",
    "\n",
    "# Promote python func to udf, passing SparkFrac type alias\n",
    "reduce_fraction = F.udf(py_reduce_fraction, SparkFrac)\n",
    "\n",
    "# apply to existing dataframe\n",
    "frac_df = frac_df.withColumn(\n",
    "    \"reduced_fraction\", reduce_fraction(F.col(\"fraction\"))\n",
    ")\n",
    "\n",
    "frac_df.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a44d6a",
   "metadata": {},
   "source": [
    "#### Option 2: Creating a UDF directly using `udf()` decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6399b54a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T23:30:01.606078Z",
     "start_time": "2021-05-31T23:30:01.119592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+\n",
      "|reduced_fraction|fraction_float     |\n",
      "+----------------+-------------------+\n",
      "|[3, 50]         |0.06               |\n",
      "|[3, 67]         |0.04477611940298507|\n",
      "|[7, 76]         |0.09210526315789473|\n",
      "|[9, 23]         |0.391304347826087  |\n",
      "|[9, 25]         |0.36               |\n",
      "+----------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@F.udf(T.DoubleType())\n",
    "def fraction_to_float(frac: Frac) -> Optional[float]:\n",
    "    num, denom = frac\n",
    "    if denom:\n",
    "        return num / denom\n",
    "    return None\n",
    "\n",
    "\n",
    "frac_df = frac_df.withColumn(\n",
    "    \"fraction_float\", fraction_to_float(F.col(\"reduced_fraction\"))\n",
    ")\n",
    "\n",
    "frac_df.select(\"reduced_fraction\", \"fraction_float\").distinct().show(5, False)\n",
    "\n",
    "assert fraction_to_float.func((1, 2)) == 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1064f6c0",
   "metadata": {},
   "source": [
    "## Chapter 9: Using Pandas UDF\n",
    "\n",
    "## Summary\n",
    "- Pandas UDFs allow you to take code that works on Pandas data frames and scale it to the Spark Data Frame structure. Efficient serialization between the two data structures is ensured by PyArrow.\n",
    "- We can group Pandas UDF into two main families, depending on the level of control we need over the batches. Series and Iterator of Series (and Iterator of data frame/mapInPandas) will batch efficiently with the user having no control over the batch composition.\n",
    "- If you need control over the content of each batch, you can use grouped data UDF with the split-apply-combing programming pattern. PySpark provides access to the values inside each batch of a GroupedData object either as Series (group aggregate UDF) of as data frame (group map UDF).\n",
    "\n",
    "## Requirements\n",
    "\n",
    "This chapter will use:\n",
    "1. pandas\n",
    "2. scikit-learn\n",
    "3. PyArrow\n",
    "\n",
    "The chapter assumes you are using PySpark 3.0 and above.\n",
    "\n",
    "## Column transformation using Series UDF\n",
    "\n",
    "### Types of Series UDF\n",
    "\n",
    "__Series to Series__ \n",
    "\n",
    "- Takes `Columns` objects, converts to Pandas `Series` and return `Series` object that gets promoted back to `Column` object.\n",
    "\n",
    "__Iterator of Series to Iterator of Series__ \n",
    "\n",
    "- `Column` is batched, then fed as a Iterator object.  \n",
    "- Takes single `Column`, returns single `Column`\n",
    "- Good when you need to initialize an expensive state\n",
    "\n",
    "__Iterator of multiples Series to Iterator of Series__\n",
    "\n",
    "- Takes multiple `Columns` as input but preserves iterator pattern.\n",
    "\n",
    "### Dataset - Google BigQuery\n",
    "\n",
    "We will use the National Oceanic and Atmospheric Administration (NOAA) Global Surface Summary of the Day (GSOD) dataset.\n",
    "\n",
    "#### Steps to data connection\n",
    "\n",
    "1. Install and configure the connector (if necessary), following the vendor’s documentation.\n",
    "2. Customize the SparkReader object to account for the new data source type.\n",
    "3. Read the data, authenticating as needed.\n",
    "\n",
    "#### Installation + Configuration\n",
    "\n",
    "After setting up Google Cloud Platform account, intiialize PySpark with the BigQuery connector enabled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00775ff4",
   "metadata": {},
   "source": [
    "### Errata \n",
    "The code below doesn't work due to a lot of issues with PyArrow compatability with Java 11.  I've skipped this part and just downloaded the dataset from the author's github.\n",
    "\n",
    "Reference:\n",
    "- https://stackoverflow.com/questions/62109276/errorjava-lang-unsupportedoperationexception-for-pyspark-pandas-udf-documenta\n",
    "- https://github.com/GoogleCloudDataproc/spark-bigquery-connector/issues/200\n",
    "- https://stackoverflow.com/questions/64960642/rewrite-udf-to-pandas-udf-with-arraytype-column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32e2e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\n",
    "    \"spark.executor.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\"\n",
    ")\n",
    "conf.set(\"spark.driver.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\")\n",
    "conf.set(\n",
    "    \"spark.jars.packages\",\n",
    "    \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.19.1\",\n",
    ")\n",
    "\n",
    "# spark = (\n",
    "#     SparkSession.builder\n",
    "#     .config(\n",
    "#         \"spark.jars.packages\",\n",
    "#         \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.19.1\",\n",
    "#     )\n",
    "#     .config(\n",
    "#         \"spark.driver.extraJavaOptions\",\n",
    "#         \"-Dio.netty.tryReflectionSetAccessible=true\"\n",
    "#     )\n",
    "#     .config(\n",
    "#         \"spark.executor.extraJavaOptions\",\n",
    "#         \"-Dio.netty.tryReflectionSetAccessible=true\"\n",
    "#     )\n",
    "#     .getOrCreate()\n",
    "# )\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc316b59",
   "metadata": {},
   "source": [
    "After initializing, read the `stations` and `gsod` tables for 2010 to 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f83076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "def read_df_from_bq(year):\n",
    "    return (\n",
    "        spark.read.format(\"bigquery\").option(\n",
    "            \"table\", f\"bigquery-public-data.noaa_gsod.gsod{year}\"\n",
    "        )\n",
    "        .option(\"credentialsFile\", \"/Users/taichinakatani/dotfiles/keys/bq-key.json\")\n",
    "        .option(\"parentProject\", \"still-vim-244001\")\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "\n",
    "# Because gsod2020 has an additional date column that the previous years do not have,\n",
    "# unionByName will fill the values with null\n",
    "gsod = (\n",
    "    reduce(\n",
    "        lambda x, y: x.unionByName(y, allowMissingColumns=True),\n",
    "        [read_df_from_bq(year) for year in range(2020, 2021)],\n",
    "    )\n",
    "    .dropna(subset=[\"year\", \"mo\", \"da\", \"temp\"])\n",
    "    .where(F.col(\"temp\") != 9999.9)\n",
    "    .drop(\"date\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25df7fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsod.select(F.col('year')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9241810b",
   "metadata": {},
   "source": [
    "### Read data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15ad1834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# Read from local parquet instead\n",
    "gsod = spark.read.load(\"data/gsod_noaa/gsod2018.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ebf8f4",
   "metadata": {},
   "source": [
    "## Series to Series UDF\n",
    "\n",
    "- Python UDFs work on one record at a time, while Scalar UDF work on one _Series_ at a time and is written through Pandas code.\n",
    "- Pandas has simpler data types than PySpark, so need to be careful to align the types.  `pandas_udf` helps with this.\n",
    "\n",
    "![](notes/img/pd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4feedf",
   "metadata": {},
   "source": [
    "### Converting Fahrenheit to Celsius with a S-to-S UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49d76c8",
   "metadata": {},
   "source": [
    "#### Errata\n",
    "\n",
    "Using the `pandas_udf` decorator is killing the kernel for some reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f14f7d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# note the syntax \"pandas_udf\" and how it returns a pd.Series\n",
    "# @F.pandas_udf(T.DoubleType())\n",
    "def f_to_c(degrees: pd.Series) -> pd.Series:\n",
    "    \"\"\"Transforms Farhenheit to Celcius.\"\"\"\n",
    "    return (degrees - 32) * 5 / 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63e2973d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|temp|            temp_c|\n",
      "+----+------------------+\n",
      "|37.2|2.8888888888888906|\n",
      "|71.6|21.999999999999996|\n",
      "|53.5|11.944444444444445|\n",
      "|24.7|-4.055555555555555|\n",
      "|70.4|21.333333333333336|\n",
      "+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gsod = gsod.withColumn(\"temp_c\", f_to_c(F.col(\"temp\")))\n",
    "gsod.select(\"temp\", \"temp_c\").distinct().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d04fc5",
   "metadata": {},
   "source": [
    "## Iterator of Series UDF\n",
    "\n",
    "- signature goes from `(pd.Series) → pd.Series` to `(Iterator[pd.Series]) → Iterator[pd.Series]`\n",
    "- Since we are working with an Iterator of Series, we are explicitly iterating over each batch one by one. PySpark will take care of distributing the work for us.\n",
    "- Uses `yield` than `return` so function returns an iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67791e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from typing import Iterator\n",
    "\n",
    "\n",
    "@F.pandas_udf(T.DoubleType())\n",
    "def f_to_c2(degrees: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    \"\"\"Transforms Farhenheit to Celcius.\"\"\"\n",
    "    sleep(5)\n",
    "    for batch in degrees:\n",
    "        yield (batch - 32) * 5 / 9\n",
    "\n",
    "\n",
    "gsod.select(\n",
    "    \"temp\", f_to_c2(F.col(\"temp\")).alias(\"temp_c\")\n",
    ").distinct().show(5)\n",
    "\n",
    "# +-----+-------------------+\n",
    "# | temp|             temp_c|\n",
    "# +-----+-------------------+\n",
    "# | 37.2| 2.8888888888888906|\n",
    "# | 85.9| 29.944444444444443|\n",
    "# | 53.5| 11.944444444444445|\n",
    "# | 71.6| 21.999999999999996|\n",
    "# |-27.6|-33.111111111111114|\n",
    "# +-----+-------------------+\n",
    "# only showing top 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9737c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13b2d261",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Dataset\" data-toc-modified-id=\"Dataset-1\">Dataset</a></span></li><li><span><a href=\"#Summarizing-data-with-over\" data-toc-modified-id=\"Summarizing-data-with-over-2\">Summarizing data with <code>over</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#Q:-When-was-the-lowest-temperature-recorded-each-year?\" data-toc-modified-id=\"Q:-When-was-the-lowest-temperature-recorded-each-year?-2.1\">Q: <strong>When</strong> was the lowest temperature recorded each year?</a></span></li><li><span><a href=\"#Using-a-window-function\" data-toc-modified-id=\"Using-a-window-function-2.2\">Using a window function</a></span></li></ul></li><li><span><a href=\"#Ranking-functions\" data-toc-modified-id=\"Ranking-functions-3\">Ranking functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#rank-&amp;-dense_rank\" data-toc-modified-id=\"rank-&amp;-dense_rank-3.1\"><code>rank</code> &amp; <code>dense_rank</code></a></span></li><li><span><a href=\"#percent_rank\" data-toc-modified-id=\"percent_rank-3.2\"><code>percent_rank</code></a></span></li><li><span><a href=\"#ntile()\" data-toc-modified-id=\"ntile()-3.3\"><code>ntile()</code></a></span></li><li><span><a href=\"#row_number()\" data-toc-modified-id=\"row_number()-3.4\"><code>row_number()</code></a></span></li></ul></li><li><span><a href=\"#Analytic-functions:-looking-back-and-ahead\" data-toc-modified-id=\"Analytic-functions:-looking-back-and-ahead-4\">Analytic functions: looking back and ahead</a></span><ul class=\"toc-item\"><li><span><a href=\"#lag-and-lead\" data-toc-modified-id=\"lag-and-lead-4.1\"><code>lag</code> and <code>lead</code></a></span></li><li><span><a href=\"#cume_dist()\" data-toc-modified-id=\"cume_dist()-4.2\"><code>cume_dist()</code></a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cfe2fd",
   "metadata": {},
   "source": [
    "# Window functions\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We will use the National Oceanic and Atmospheric Administration (NOAA) Global Surface Summary of the Day (GSOD) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93d78f45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T01:45:54.457457Z",
     "start_time": "2021-06-04T01:45:54.221195Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# Read from local parquet\n",
    "gsod = spark.read.parquet(\"data/gsod_noaa/gsod*.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cb99b3",
   "metadata": {},
   "source": [
    "## Summarizing data with `over`\n",
    "\n",
    "### Q: __When__ was the lowest temperature recorded each year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73fe48b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T01:52:27.473400Z",
     "start_time": "2021-06-04T01:51:45.112733Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|year|  temp|\n",
      "+----+------+\n",
      "|2019|-114.7|\n",
      "|2017|-114.7|\n",
      "|2012|-113.5|\n",
      "|2018|-113.5|\n",
      "|2016|-111.7|\n",
      "|2013|-110.7|\n",
      "|2010|-110.7|\n",
      "|2014|-110.5|\n",
      "|2015|-110.2|\n",
      "|2011|-106.8|\n",
      "|2020|-105.0|\n",
      "+----+------+\n",
      "\n",
      "+------+----+---+---+------+\n",
      "|   stn|year| mo| da|  temp|\n",
      "+------+----+---+---+------+\n",
      "|896060|2010| 06| 03|-110.7|\n",
      "|896060|2011| 05| 19|-106.8|\n",
      "|896060|2012| 06| 11|-113.5|\n",
      "|895770|2013| 07| 31|-110.7|\n",
      "|896060|2014| 08| 20|-110.5|\n",
      "|895360|2015| 07| 12|-110.2|\n",
      "|896060|2015| 08| 21|-110.2|\n",
      "|896060|2015| 08| 27|-110.2|\n",
      "|896060|2016| 07| 11|-111.7|\n",
      "|896250|2017| 06| 20|-114.7|\n",
      "|896060|2018| 08| 27|-113.5|\n",
      "|895770|2019| 06| 15|-114.7|\n",
      "|896060|2020| 08| 11|-105.0|\n",
      "|896250|2020| 08| 13|-105.0|\n",
      "+------+----+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using vanilla groupBy, we can get the lowest temperature but not when.\n",
    "\n",
    "coldest_temp = gsod.groupby(\"year\").agg(F.min(\"temp\").alias(\"temp\"))\n",
    "coldest_temp.orderBy(\"temp\").show()\n",
    "\n",
    "\n",
    "# Using left-semi self-join to get the \"when\"\n",
    "# Self joins are generally an anti-pattern because it is SLOW.\n",
    "\n",
    "coldest_when = gsod.join(coldest_temp, how=\"left_semi\", on=[\"year\", \"temp\"]) \\\n",
    "                   .select(\"stn\", \"year\", \"mo\", \"da\", \"temp\")\n",
    "coldest_when.orderBy(\"year\", \"mo\", \"da\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "640c592e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T01:48:38.565661Z",
     "start_time": "2021-06-04T01:48:38.553755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.window.WindowSpec object at 0x139ed3250>\n"
     ]
    }
   ],
   "source": [
    "# Using a window function instead\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# To partition according to the values of one or more columns, \n",
    "# we pass the column name (or a Column object) to the partitionBy() method.\n",
    "each_year = Window.partitionBy(\"year\")\n",
    "\n",
    "# Window is a builder class, just like SparkSession.builder\n",
    "print(each_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a32b05",
   "metadata": {},
   "source": [
    "### Using a window function\n",
    "\n",
    "- `each_year` runs the aggregate function `F.min(\"temp\")` over each year, rather than the entire data frame.\n",
    "- `F.min(\"temp\")` applies the minimum temperature for that year to all rows.  This is then filtered to rows with `temp` that matches the aggregate `min_temp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9c15d3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T01:50:26.334142Z",
     "start_time": "2021-06-04T01:48:38.827935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+------+------+\n",
      "|year| mo| da|   stn|  temp|\n",
      "+----+---+---+------+------+\n",
      "|2010| 06| 03|896060|-110.7|\n",
      "|2011| 05| 19|896060|-106.8|\n",
      "|2012| 06| 11|896060|-113.5|\n",
      "|2013| 07| 31|895770|-110.7|\n",
      "|2014| 08| 20|896060|-110.5|\n",
      "|2015| 07| 12|895360|-110.2|\n",
      "|2015| 08| 21|896060|-110.2|\n",
      "|2015| 08| 27|896060|-110.2|\n",
      "|2016| 07| 11|896060|-111.7|\n",
      "|2017| 06| 20|896250|-114.7|\n",
      "|2018| 08| 27|896060|-113.5|\n",
      "|2019| 06| 15|895770|-114.7|\n",
      "|2020| 08| 11|896060|-105.0|\n",
      "|2020| 08| 13|896250|-105.0|\n",
      "+----+---+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the each_year builder class\n",
    "\n",
    "gsod.withColumn(\"min_temp\", F.min(\"temp\").over(each_year)).where(\n",
    "    \"temp = min_temp\"\n",
    ").select(\"year\", \"mo\", \"da\", \"stn\", \"temp\").orderBy(\n",
    "    \"year\", \"mo\", \"da\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d16b6d",
   "metadata": {},
   "source": [
    "Bonus:\n",
    "- `partitionBy()` can be used on more than one column\n",
    "- You can also directly use a window function inside a `select`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc24fd53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T01:51:44.697186Z",
     "start_time": "2021-06-04T01:50:26.335870Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+------+------+\n",
      "|year| mo| da|   stn|  temp|\n",
      "+----+---+---+------+------+\n",
      "|2010| 06| 03|896060|-110.7|\n",
      "|2011| 05| 19|896060|-106.8|\n",
      "|2012| 06| 11|896060|-113.5|\n",
      "|2013| 07| 31|895770|-110.7|\n",
      "|2014| 08| 20|896060|-110.5|\n",
      "|2015| 07| 12|895360|-110.2|\n",
      "|2015| 08| 21|896060|-110.2|\n",
      "|2015| 08| 27|896060|-110.2|\n",
      "|2016| 07| 11|896060|-111.7|\n",
      "|2017| 06| 20|896250|-114.7|\n",
      "|2018| 08| 27|896060|-113.5|\n",
      "|2019| 06| 15|895770|-114.7|\n",
      "|2020| 08| 11|896060|-105.0|\n",
      "|2020| 08| 13|896250|-105.0|\n",
      "+----+---+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using window function inside a select\n",
    "gsod.select(\n",
    "    \"year\",\n",
    "    \"mo\",\n",
    "    \"da\",\n",
    "    \"stn\",\n",
    "    \"temp\",\n",
    "    F.min(\"temp\").over(each_year).alias(\"min_temp\"),\n",
    ").where(\"temp = min_temp\").drop(\"min_temp\").orderBy(\n",
    "    \"year\", \"mo\", \"da\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288c1f8c",
   "metadata": {},
   "source": [
    "## Ranking functions\n",
    "\n",
    "- Rank functions rank records based on the value of a field.\n",
    "- Functions: `rank()`, `dense_rank()`, `percent_rank()`, `ntile()` and `row_number()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2888fd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T02:09:44.343817Z",
     "start_time": "2021-06-04T02:09:44.339325Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load lightweight dataset\n",
    "gsod_light = spark.read.parquet(\"data/Window/gsod_light.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99cd3ecb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T01:53:28.832567Z",
     "start_time": "2021-06-04T01:53:28.772990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- stn: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- mo: string (nullable = true)\n",
      " |-- da: string (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- count_temp: long (nullable = true)\n",
      "\n",
      "+------+----+---+---+----+----------+\n",
      "|   stn|year| mo| da|temp|count_temp|\n",
      "+------+----+---+---+----+----------+\n",
      "|994979|2017| 12| 11|21.3|        21|\n",
      "|998012|2017| 03| 02|31.4|        24|\n",
      "|719200|2017| 10| 09|60.5|        11|\n",
      "|917350|2018| 04| 21|82.6|         9|\n",
      "|076470|2018| 06| 07|65.0|        24|\n",
      "|996470|2018| 03| 12|55.6|        12|\n",
      "|041680|2019| 02| 19|16.1|        15|\n",
      "|949110|2019| 11| 23|54.9|        14|\n",
      "|998252|2019| 04| 18|44.7|        11|\n",
      "|998166|2019| 03| 20|34.8|        12|\n",
      "+------+----+---+---+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect\n",
    "gsod_light.printSchema()\n",
    "gsod_light.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f9fb02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T02:09:44.343817Z",
     "start_time": "2021-06-04T02:09:44.339325Z"
    }
   },
   "source": [
    "### `rank` & `dense_rank`\n",
    "- `rank` gives Olympic ranking (non-consecutive, when you have multiple records that tie for a rank, the next one will be offset by the number of ties)\n",
    "- `dense_rank` ranks consecutively.  Ties share the same rank, but there won’t be any gap between the ranks.  Useful when you just want a cardinal position over a window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3b5dcbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T01:53:28.832567Z",
     "start_time": "2021-06-04T01:53:28.772990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- stn: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- mo: string (nullable = true)\n",
      " |-- da: string (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- count_temp: long (nullable = true)\n",
      "\n",
      "+------+----+---+---+----+----------+\n",
      "|   stn|year| mo| da|temp|count_temp|\n",
      "+------+----+---+---+----+----------+\n",
      "|994979|2017| 12| 11|21.3|        21|\n",
      "|998012|2017| 03| 02|31.4|        24|\n",
      "|719200|2017| 10| 09|60.5|        11|\n",
      "|917350|2018| 04| 21|82.6|         9|\n",
      "|076470|2018| 06| 07|65.0|        24|\n",
      "|996470|2018| 03| 12|55.6|        12|\n",
      "|041680|2019| 02| 19|16.1|        15|\n",
      "|949110|2019| 11| 23|54.9|        14|\n",
      "|998252|2019| 04| 18|44.7|        11|\n",
      "|998166|2019| 03| 20|34.8|        12|\n",
      "+------+----+---+---+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect\n",
    "gsod_light.printSchema()\n",
    "gsod_light.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df41ab5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T02:07:17.562889Z",
     "start_time": "2021-06-04T02:07:15.904807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using rank()\n",
      "+------+----+---+---+----+----------+--------+\n",
      "|   stn|year| mo| da|temp|count_temp|rank_tpm|\n",
      "+------+----+---+---+----+----------+--------+\n",
      "|949110|2019| 11| 23|54.9|        14|       1|\n",
      "|996470|2018| 03| 12|55.6|        12|       1|\n",
      "|998166|2019| 03| 20|34.8|        12|       1|\n",
      "|998012|2017| 03| 02|31.4|        24|       3|\n",
      "|041680|2019| 02| 19|16.1|        15|       1|\n",
      "|076470|2018| 06| 07|65.0|        24|       1|\n",
      "|719200|2017| 10| 09|60.5|        11|       1|\n",
      "|994979|2017| 12| 11|21.3|        21|       1|\n",
      "|917350|2018| 04| 21|82.6|         9|       1|\n",
      "|998252|2019| 04| 18|44.7|        11|       2|\n",
      "+------+----+---+---+----+----------+--------+\n",
      "\n",
      "Using dense_rank()\n",
      "+------+----+---+---+----+----------+--------+\n",
      "|   stn|year| mo| da|temp|count_temp|rank_tpm|\n",
      "+------+----+---+---+----+----------+--------+\n",
      "|949110|2019| 11| 23|54.9|        14|       1|\n",
      "|996470|2018| 03| 12|55.6|        12|       1|\n",
      "|998166|2019| 03| 20|34.8|        12|       1|\n",
      "|998012|2017| 03| 02|31.4|        24|       2|\n",
      "|041680|2019| 02| 19|16.1|        15|       1|\n",
      "|076470|2018| 06| 07|65.0|        24|       1|\n",
      "|719200|2017| 10| 09|60.5|        11|       1|\n",
      "|994979|2017| 12| 11|21.3|        21|       1|\n",
      "|917350|2018| 04| 21|82.6|         9|       1|\n",
      "|998252|2019| 04| 18|44.7|        11|       2|\n",
      "+------+----+---+---+----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create new window, partitioning by year and ordering by number of temperature readings\n",
    "temp_per_year_asc = Window.partitionBy(\"year\").orderBy(\"count_temp\")\n",
    "temp_per_month_asc = Window.partitionBy(\"mo\").orderBy(\"count_temp\")\n",
    "\n",
    "\n",
    "# Using rank() with window, we get the rank accordintg the value of count_temp column\n",
    "print(\"Using rank()\")\n",
    "gsod_light.withColumn(\"rank_tpm\", F.rank().over(temp_per_month_asc)).show()\n",
    "\n",
    "\n",
    "# Using dense_rank() instead to get consecutive ranking by month\n",
    "print(\"Using dense_rank()\")\n",
    "gsod_light.withColumn(\"rank_tpm\", F.dense_rank().over(temp_per_month_asc)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519df071",
   "metadata": {},
   "source": [
    "### `percent_rank`\n",
    "\n",
    "For every window `percent_rank()` computes percentage rank (0-1) based on ordered value.\n",
    "\n",
    "formula = # records with lower value than the current / # of records in the window - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8fba88c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T02:12:30.442474Z",
     "start_time": "2021-06-04T02:12:29.065050Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----------+------------------+\n",
      "|   stn|year| mo| da|temp|count_temp|          rank_tpm|\n",
      "+------+----+---+---+----+----------+------------------+\n",
      "|041680|2019| 02| 19|16.1|        15|               0.0|\n",
      "|998166|2019| 03| 20|34.8|        12|0.3333333333333333|\n",
      "|998252|2019| 04| 18|44.7|        11|0.6666666666666666|\n",
      "|949110|2019| 11| 23|54.9|        14|               1.0|\n",
      "|994979|2017| 12| 11|21.3|        21|               0.0|\n",
      "|998012|2017| 03| 02|31.4|        24|               0.5|\n",
      "|719200|2017| 10| 09|60.5|        11|               1.0|\n",
      "|996470|2018| 03| 12|55.6|        12|               0.0|\n",
      "|076470|2018| 06| 07|65.0|        24|               0.5|\n",
      "|917350|2018| 04| 21|82.6|         9|               1.0|\n",
      "+------+----+---+---+----+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_each_year = each_year.orderBy(\"temp\")\n",
    "\n",
    "\n",
    "gsod_light.withColumn(\"rank_tpm\", F.percent_rank().over(temp_each_year)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879f29f8",
   "metadata": {},
   "source": [
    "### `ntile()`\n",
    "\n",
    "Gives n-tile for a given param.\n",
    "\n",
    "![](notes/img/ntile.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f246fbc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T02:13:41.431400Z",
     "start_time": "2021-06-04T02:13:36.381640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----------+--------+\n",
      "|   stn|year| mo| da|temp|count_temp|rank_tpm|\n",
      "+------+----+---+---+----+----------+--------+\n",
      "|041680|2019| 02| 19|16.1|        15|       1|\n",
      "|998166|2019| 03| 20|34.8|        12|       1|\n",
      "|998252|2019| 04| 18|44.7|        11|       2|\n",
      "|949110|2019| 11| 23|54.9|        14|       2|\n",
      "|994979|2017| 12| 11|21.3|        21|       1|\n",
      "|998012|2017| 03| 02|31.4|        24|       1|\n",
      "|719200|2017| 10| 09|60.5|        11|       2|\n",
      "|996470|2018| 03| 12|55.6|        12|       1|\n",
      "|076470|2018| 06| 07|65.0|        24|       1|\n",
      "|917350|2018| 04| 21|82.6|         9|       2|\n",
      "+------+----+---+---+----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gsod_light.withColumn(\"rank_tpm\", F.ntile(2).over(temp_each_year)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d916d2",
   "metadata": {},
   "source": [
    "### `row_number()`\n",
    "\n",
    "Given an ordered window, it will give a increasing rank regardless of ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a2531415",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T02:20:32.873109Z",
     "start_time": "2021-06-04T02:20:32.153274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----------+----------+\n",
      "|   stn|year| mo| da|temp|count_temp|row_number|\n",
      "+------+----+---+---+----+----------+----------+\n",
      "|041680|2019| 02| 19|16.1|        15|         1|\n",
      "|998166|2019| 03| 20|34.8|        12|         2|\n",
      "|998252|2019| 04| 18|44.7|        11|         3|\n",
      "|949110|2019| 11| 23|54.9|        14|         4|\n",
      "|994979|2017| 12| 11|21.3|        21|         1|\n",
      "|998012|2017| 03| 02|31.4|        24|         2|\n",
      "|719200|2017| 10| 09|60.5|        11|         3|\n",
      "|996470|2018| 03| 12|55.6|        12|         1|\n",
      "|076470|2018| 06| 07|65.0|        24|         2|\n",
      "|917350|2018| 04| 21|82.6|         9|         3|\n",
      "+------+----+---+---+----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gsod_light.withColumn(\"row_number\", F.row_number().over(temp_each_year)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4d44e2fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T02:20:28.387938Z",
     "start_time": "2021-06-04T02:20:27.691039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----------+----------+\n",
      "|   stn|year| mo| da|temp|count_temp|row_number|\n",
      "+------+----+---+---+----+----------+----------+\n",
      "|949110|2019| 11| 23|54.9|        14|         1|\n",
      "|998012|2017| 03| 02|31.4|        24|         1|\n",
      "|996470|2018| 03| 12|55.6|        12|         2|\n",
      "|998166|2019| 03| 20|34.8|        12|         3|\n",
      "|041680|2019| 02| 19|16.1|        15|         1|\n",
      "|076470|2018| 06| 07|65.0|        24|         1|\n",
      "|719200|2017| 10| 09|60.5|        11|         1|\n",
      "|994979|2017| 12| 11|21.3|        21|         1|\n",
      "|998252|2019| 04| 18|44.7|        11|         1|\n",
      "|917350|2018| 04| 21|82.6|         9|         2|\n",
      "+------+----+---+---+----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a window with a descending ordered column\n",
    "\n",
    "temp_per_month_desc = Window.partitionBy(\"mo\").orderBy(F.col(\"count_temp\").desc())\n",
    "\n",
    "gsod_light.withColumn(\"row_number\", F.row_number().over(temp_per_month_desc)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09b52da",
   "metadata": {},
   "source": [
    "## Analytic functions: looking back and ahead\n",
    "\n",
    "\n",
    "### `lag` and `lead`\n",
    "\n",
    "> The two most important functions of the analytics functions family are called `lag(col, n=1, default=None)` and `lead(col, n=1, default=None)`, which will give you the value of the col column of the n-th record before and after the record you’re over, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b365563d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T02:29:05.522208Z",
     "start_time": "2021-06-04T02:29:04.226128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp of previous two records over each year\n",
      "+------+----+---+---+----+----------+-------------+---------------+\n",
      "|   stn|year| mo| da|temp|count_temp|previous_temp|previous_temp_2|\n",
      "+------+----+---+---+----+----------+-------------+---------------+\n",
      "|041680|2019| 02| 19|16.1|        15|         null|           null|\n",
      "|998166|2019| 03| 20|34.8|        12|         16.1|           null|\n",
      "|998252|2019| 04| 18|44.7|        11|         34.8|           16.1|\n",
      "|949110|2019| 11| 23|54.9|        14|         44.7|           34.8|\n",
      "|994979|2017| 12| 11|21.3|        21|         null|           null|\n",
      "|998012|2017| 03| 02|31.4|        24|         21.3|           null|\n",
      "|719200|2017| 10| 09|60.5|        11|         31.4|           21.3|\n",
      "|996470|2018| 03| 12|55.6|        12|         null|           null|\n",
      "|076470|2018| 06| 07|65.0|        24|         55.6|           null|\n",
      "|917350|2018| 04| 21|82.6|         9|         65.0|           55.6|\n",
      "+------+----+---+---+----+----------+-------------+---------------+\n",
      "\n",
      "Temp delta of previous record over each year\n",
      "+----+---+----+-------------------+\n",
      "|year| mo|temp|previous_temp_delta|\n",
      "+----+---+----+-------------------+\n",
      "|2019| 02|16.1|               null|\n",
      "|2019| 03|34.8|               18.7|\n",
      "|2019| 04|44.7|                9.9|\n",
      "|2019| 11|54.9|               10.2|\n",
      "|2017| 12|21.3|               null|\n",
      "|2017| 03|31.4|               10.1|\n",
      "|2017| 10|60.5|               29.1|\n",
      "|2018| 03|55.6|               null|\n",
      "|2018| 06|65.0|                9.4|\n",
      "|2018| 04|82.6|               17.6|\n",
      "+----+---+----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get temp of previous two records using lag()\n",
    "\n",
    "print(\"Temp of previous two records over each year\")\n",
    "gsod_light.withColumn(\n",
    "    \"previous_temp\", F.lag(\"temp\").over(temp_each_year)\n",
    ").withColumn(\n",
    "    \"previous_temp_2\", F.lag(\"temp\", 2).over(temp_each_year)\n",
    ").show()\n",
    "\n",
    "\n",
    "print(\"Temp delta of previous record over each year\")\n",
    "gsod_light.withColumn(\n",
    "    \"previous_temp_delta\", F.round(F.col(\"temp\") - F.lag(\"temp\").over(temp_each_year), 2)\n",
    ").select([\"year\", \"mo\", \"temp\", \"previous_temp_delta\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f883d460",
   "metadata": {},
   "source": [
    "### `cume_dist()`\n",
    "\n",
    "- Provides cumulative distribution rather than ranking.  Useful for EDA of cume-distro of variables.\n",
    "- Does not rank, but provides the cumulative density function `F(x)` for the records in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b679e605",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T02:32:37.633265Z",
     "start_time": "2021-06-04T02:32:36.625956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent rank vs. Cumulative distribution of temperature over each year\n",
      "+------+----+---+---+----+----------+------------------+------------------+\n",
      "|   stn|year| mo| da|temp|count_temp|       percen_rank|         cume_dist|\n",
      "+------+----+---+---+----+----------+------------------+------------------+\n",
      "|041680|2019| 02| 19|16.1|        15|               0.0|              0.25|\n",
      "|998166|2019| 03| 20|34.8|        12|0.3333333333333333|               0.5|\n",
      "|998252|2019| 04| 18|44.7|        11|0.6666666666666666|              0.75|\n",
      "|949110|2019| 11| 23|54.9|        14|               1.0|               1.0|\n",
      "|994979|2017| 12| 11|21.3|        21|               0.0|0.3333333333333333|\n",
      "|998012|2017| 03| 02|31.4|        24|               0.5|0.6666666666666666|\n",
      "|719200|2017| 10| 09|60.5|        11|               1.0|               1.0|\n",
      "|996470|2018| 03| 12|55.6|        12|               0.0|0.3333333333333333|\n",
      "|076470|2018| 06| 07|65.0|        24|               0.5|0.6666666666666666|\n",
      "|917350|2018| 04| 21|82.6|         9|               1.0|               1.0|\n",
      "+------+----+---+---+----+----------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Percent rank vs. Cumulative distribution of temperature over each year\")\n",
    "gsod_light.withColumn(\n",
    "    \"percen_rank\" , F.percent_rank().over(temp_each_year)\n",
    ").withColumn(\"cume_dist\", F.cume_dist().over(temp_each_year)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a1bb06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
