{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-1\">Summary</a></span></li><li><span><a href=\"#Terminology\" data-toc-modified-id=\"Terminology-2\">Terminology</a></span><ul class=\"toc-item\"><li><span><a href=\"#Resilient-Distributed-Dataset-(RDD)\" data-toc-modified-id=\"Resilient-Distributed-Dataset-(RDD)-2.1\">Resilient Distributed Dataset (RDD)</a></span></li><li><span><a href=\"#User-defined-functions-(UDF)\" data-toc-modified-id=\"User-defined-functions-(UDF)-2.2\">User-defined functions (UDF)</a></span></li></ul></li><li><span><a href=\"#Example:-Creating-an-RDD-from-a-Python-list\" data-toc-modified-id=\"Example:-Creating-an-RDD-from-a-Python-list-3\">Example: Creating an RDD from a Python list</a></span></li><li><span><a href=\"#Manipulating-data-with-map,-filter-and-reduce\" data-toc-modified-id=\"Manipulating-data-with-map,-filter-and-reduce-4\">Manipulating data with <code>map</code>, <code>filter</code> and <code>reduce</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#map\" data-toc-modified-id=\"map-4.1\"><code>map</code></a></span></li><li><span><a href=\"#filter\" data-toc-modified-id=\"filter-4.2\"><code>filter</code></a></span></li></ul></li><li><span><a href=\"#reduce\" data-toc-modified-id=\"reduce-5\"><code>reduce</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#Use-Commutative-and-associate-functions-(for-distributed-computing)\" data-toc-modified-id=\"Use-Commutative-and-associate-functions-(for-distributed-computing)-5.1\">Use <em>Commutative</em> and <em>associate</em> functions (for distributed computing)</a></span></li></ul></li><li><span><a href=\"#User-defined-Functions\" data-toc-modified-id=\"User-defined-Functions-6\">User-defined Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Using-UDF-to-create-a-fractions-function\" data-toc-modified-id=\"Using-UDF-to-create-a-fractions-function-6.1\">Using UDF to create a <code>fractions</code> function</a></span></li><li><span><a href=\"#Using-typed-Python-functions\" data-toc-modified-id=\"Using-typed-Python-functions-6.2\">Using typed Python functions</a></span></li><li><span><a href=\"#Promoting-Python-functions-to-udf\" data-toc-modified-id=\"Promoting-Python-functions-to-udf-6.3\">Promoting Python functions to <code>udf</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#Option-1:-Creating-a-UDF-explicitily-with-udf()-and-apply-it-to-dataframe\" data-toc-modified-id=\"Option-1:-Creating-a-UDF-explicitily-with-udf()-and-apply-it-to-dataframe-6.3.1\">Option 1: Creating a UDF explicitily with <code>udf()</code> and apply it to dataframe</a></span></li><li><span><a href=\"#Option-2:-Creating-a-UDF-directly-using-udf()-decorator\" data-toc-modified-id=\"Option-2:-Creating-a-UDF-directly-using-udf()-decorator-6.3.2\">Option 2: Creating a UDF directly using <code>udf()</code> decorator</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD and user-defined functions\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "- The resilient distributed dataset allows for better flexibility compared to the records and columns approach of the data frame.\n",
    "- The most low level and flexible way of running Python code within the distributed Spark environment is to use the RDD. With an RDD, you have no structure imposed on your data and need to manage type information into your program, and defensively code against potential exceptions.\n",
    "- The API for data processing on the RDD is heavily inspired by the MapReduce framework. You use higher order functions such as map(), filter() and reduce() on the objects of the RDD.\n",
    "- The data frameâ€™s most basic Python code promotion functionality, called the (PySpark) UDF, emulates the \"map\" part of the RDD. You use it as a scalar function, taking Column objects as parameters and returning a single Column.\n",
    "\n",
    "\n",
    "## Terminology\n",
    "\n",
    "### Resilient Distributed Dataset (RDD)\n",
    "\n",
    "- Bag of elements, independent, no schema\n",
    "- Flexible with what you want to do but no safeguards\n",
    "\n",
    "### User-defined functions (UDF)\n",
    "\n",
    "- Simple way to promote Python functions to be used on a data frame.\n",
    "\n",
    "RDD's Pros\n",
    "\n",
    "1. When you have unordered collection of Python objects that can be pickled\n",
    "2. Unordered `key value` pairs i.e. Python dict\n",
    "\n",
    "\n",
    "## Example: Creating an RDD from a Python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T17:44:16.031789Z",
     "start_time": "2021-05-31T17:44:16.023065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:262\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "collection = [1, \"two\", 3.0, (\"four\", 4), {\"five\": 5}]\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "collection_rdd = sc.parallelize(collection)\n",
    "\n",
    "print(collection_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating data with `map`, `filter` and `reduce`\n",
    "\n",
    "- Each take a function as their only param, ie. they are _higher-order functions_.\n",
    "\n",
    "### `map`\n",
    "\n",
    "- apply one function to every object\n",
    "- need to be careful with unsupported types on whatever function you're trying to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T18:05:24.929778Z",
     "start_time": "2021-05-31T18:05:24.821124Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Map a simple function to each element to an RDD.\n",
    "# This will raise an error because not all of the elements are integers\n",
    "\n",
    "from py4j.protocol import Py4JJavaError\n",
    "import re\n",
    "\n",
    "\n",
    "def add_one(value):\n",
    "    return value + 1\n",
    "\n",
    "\n",
    "collection_rdd = collection_rdd.map(add_one)\n",
    "\n",
    "try:\n",
    "    print(collection_rdd.collect())\n",
    "except Py4JJavaError as e:\n",
    "    pass\n",
    "\n",
    "# Stack trace galore! The important bit, you'll get one of the following:\n",
    "# TypeError: can only concatenate str (not \"int\") to str\n",
    "# TypeError: unsupported operand type(s) for +: 'dict' and 'int'\n",
    "# TypeError: can only concatenate tuple (not \"int\") to tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T18:12:03.883151Z",
     "start_time": "2021-05-31T18:12:03.824825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:  [1, 'two', 3.0, ('four', 4), {'five': 5}]\n",
      "After :  [2, 'two', 4.0, ('four', 4), {'five': 5}]\n"
     ]
    }
   ],
   "source": [
    "# Safer option with a try/except inside the function\n",
    "def safer_add_one(value):\n",
    "    try:\n",
    "        return value + 1\n",
    "    except TypeError:\n",
    "        return value\n",
    "    \n",
    "# reset rdd\n",
    "collection_rdd = sc.parallelize(collection)\n",
    "print(\"Before: \", collection)\n",
    "\n",
    "# run safe adding method\n",
    "collection_rdd = collection_rdd.map(safer_add_one)\n",
    "print(\"After : \", collection_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](notes/img/rdd_error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `filter`\n",
    "\n",
    "- Returns RDD element if True, else drops it.\n",
    "-  As a reminder, False, number 0, empty sequences and collections (list, tuple, dict, set, range) are falsey \n",
    "  - ref: https://docs.python.org/3/library/stdtypes.html#truth-value-testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T18:16:26.531609Z",
     "start_time": "2021-05-31T18:16:26.445077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3.0]\n",
      "['two']\n"
     ]
    }
   ],
   "source": [
    "# Filtering RDD with lambda function to keep only int and floats\n",
    "\n",
    "collection_rdd = sc.parallelize(collection)\n",
    "\n",
    "\n",
    "collection_rdd = collection_rdd.filter(lambda x: isinstance(x, (float, int)))\n",
    "print(collection_rdd.collect())\n",
    "\n",
    "\n",
    "# Alternative: Creating a separate function\n",
    "\n",
    "collection_rdd = sc.parallelize(collection)\n",
    "\n",
    "def is_string(elem):\n",
    "    return True if isinstance(elem, str) else False\n",
    "\n",
    "collection_rdd = collection_rdd.filter(is_string)\n",
    "print(collection_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `reduce`\n",
    "\n",
    "- Used for summarization (ie. groupby and agg with dataframe)\n",
    "- Takes 2 elements and returns 1 element. If list > 2, will taking first 2 elements, then apply result again to third and so forth.\n",
    "\n",
    "![](notes/img/reduce.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T18:23:27.679935Z",
     "start_time": "2021-05-31T18:23:27.631762Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "# Add list of numbers through reduce\n",
    "\n",
    "from operator import add\n",
    "\n",
    "collection_rdd = sc.parallelize(range(10))\n",
    "print(collection_rdd.reduce(add))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use _Commutative_ and _associate_ functions (for distributed computing)\n",
    "\n",
    "Only give `reduce` _commutative_ and _associate_ functions.\n",
    "\n",
    "- Commutative function: Function in which order of arguments doesn't matter\n",
    "- Associative function: Function in which grouping of arguments doesn't matter, \n",
    "  - `subtract` is not because `(a - b) - c != a - (b - c)`\n",
    "- `add`, `multiply`, `min` and `max` are both associative and commutative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-defined Functions\n",
    "\n",
    "- UDFs allow you to implement custom functions on PySpark data frame columns\n",
    "\n",
    "### Using UDF to create a `fractions` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T23:03:44.918577Z",
     "start_time": "2021-05-31T23:03:44.503614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|fraction|\n",
      "+--------+\n",
      "|[0, 1]  |\n",
      "|[0, 2]  |\n",
      "|[0, 3]  |\n",
      "|[0, 4]  |\n",
      "|[0, 5]  |\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "fractions = [[x,y] for x in range(100) for y in range(1, 100)]\n",
    "frac_df = spark.createDataFrame(fractions, [\"numerator\", \"denominator\"])\n",
    "\n",
    "frac_df = frac_df.select(\n",
    "    F.array(F.col(\"numerator\"), F.col(\"denominator\")).alias(\"fraction\"),\n",
    ")\n",
    "\n",
    "frac_df.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using typed Python functions\n",
    "\n",
    "This section will create a function to reduce a fraction and one to transform a fraction into a floating-point number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T23:07:46.677717Z",
     "start_time": "2021-05-31T23:07:46.673130Z"
    }
   },
   "outputs": [],
   "source": [
    "from fractions import Fraction\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "Frac = Tuple[int, int]\n",
    "\n",
    "def py_reduce_fraction(frac: Frac) -> Optional[Frac]:\n",
    "    \"\"\"Reduce a fraction represented as a 2-tuple of integers\"\"\"\n",
    "    num, denom = frac\n",
    "    if denom:\n",
    "        answer = Fraction(num, denom)\n",
    "        return answer.numerator, answer.denominator\n",
    "    return None\n",
    "\n",
    "assert py_reduce_fraction((3,6)) == (1, 2)\n",
    "assert py_reduce_fraction((1, 0)) is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T23:08:53.251760Z",
     "start_time": "2021-05-31T23:08:53.248059Z"
    }
   },
   "outputs": [],
   "source": [
    "def py_fraction_to_float(frac: Frac) -> Optional[float]:\n",
    "    \"\"\"Transforms a fraction represented as a 2-tuple of integer into a float\"\"\"\n",
    "    num, denom = frac\n",
    "    if denom:\n",
    "        return num / denom\n",
    "    return None\n",
    "\n",
    "assert py_fraction_to_float((2, 8)) == 0.25\n",
    "assert py_fraction_to_float((10, 0)) is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T23:22:00.678157Z",
     "start_time": "2021-05-31T23:22:00.675719Z"
    }
   },
   "source": [
    "### Promoting Python functions to `udf`\n",
    "\n",
    "The function takes two parameters.\n",
    "\n",
    "1. The function you want to promote.\n",
    "2. Optionally, the return type of the generated UDF.\n",
    "\n",
    "#### Option 1: Creating a UDF explicitily with `udf()` and apply it to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T23:25:34.406890Z",
     "start_time": "2021-05-31T23:25:34.185345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+\n",
      "|fraction|reduced_fraction|\n",
      "+--------+----------------+\n",
      "|[0, 1]  |[0, 1]          |\n",
      "|[0, 2]  |[0, 1]          |\n",
      "|[0, 3]  |[0, 1]          |\n",
      "|[0, 4]  |[0, 1]          |\n",
      "|[0, 5]  |[0, 1]          |\n",
      "+--------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SparkFrac = T.ArrayType(T.LongType())\n",
    "\n",
    "# Promote python func to udf, passing SparkFrac type alias\n",
    "reduce_fraction = F.udf(py_reduce_fraction, SparkFrac)\n",
    "\n",
    "# apply to existing dataframe\n",
    "frac_df = frac_df.withColumn(\n",
    "    \"reduced_fraction\", reduce_fraction(F.col(\"fraction\"))\n",
    ")\n",
    "\n",
    "frac_df.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Creating a UDF directly using `udf()` decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T23:30:01.606078Z",
     "start_time": "2021-05-31T23:30:01.119592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+\n",
      "|reduced_fraction|fraction_float     |\n",
      "+----------------+-------------------+\n",
      "|[3, 50]         |0.06               |\n",
      "|[3, 67]         |0.04477611940298507|\n",
      "|[7, 76]         |0.09210526315789473|\n",
      "|[9, 23]         |0.391304347826087  |\n",
      "|[9, 25]         |0.36               |\n",
      "+----------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@F.udf(T.DoubleType())\n",
    "def fraction_to_float(frac: Frac) -> Optional[float]:\n",
    "    num, denom = frac\n",
    "    if denom:\n",
    "        return num / denom\n",
    "    return None\n",
    "\n",
    "\n",
    "frac_df = frac_df.withColumn(\n",
    "    \"fraction_float\", fraction_to_float(F.col(\"reduced_fraction\"))\n",
    ")\n",
    "\n",
    "frac_df.select(\"reduced_fraction\", \"fraction_float\").distinct().show(5, False)\n",
    "\n",
    "assert fraction_to_float.func((1, 2)) == 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark]",
   "language": "python",
   "name": "conda-env-pyspark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
